{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Copia de Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmpQ2vgQMsR2Cq1bwapMsp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanflaso/ProyectoMineria/blob/master/ProyectoNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVS2dDP8NgRk",
        "colab_type": "code",
        "outputId": "962637b9-a055-43cc-a7c7-5ccaf03b01e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o0OMVJQNkt-",
        "colab_type": "code",
        "outputId": "fbb4dbc8-26d8-4292-9070-4f7793211fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "drive.mount(\"./drive\", force_remount=True)\n",
        "\n",
        "%cd \"./drive/My Drive/widsdatathon2020\"\n",
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at ./drive\n",
            "/content/drive/My Drive/widsdatathon2020\n",
            "total 164108\n",
            "-rw------- 1 root root       52 Jan 15 18:37  samplesubmission.csv\n",
            "-rw------- 1 root root   320363 Jan 15 18:37  solution_template.csv\n",
            "-rw------- 1 root root 11709733 Feb  4 03:19  training_resumido_modif.csv\n",
            "-rw------- 1 root root 63894166 Feb  2 21:15  training_v2.csv\n",
            "-rw------- 1 root root 64356456 Feb  2 21:16  training_v2_data_manipulada.csv\n",
            "-rw------- 1 root root 27733653 Jan 15 18:37  unlabeled.csv\n",
            "-rw------- 1 root root    29944 Jan 15 18:37 'WiDS Datathon 2020 Dictionary.csv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVDirvjNNm1k",
        "colab_type": "code",
        "outputId": "2d20132f-a064-44b8-c387-fcd3f1911bbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"training_resumido_modif.csv\")\n",
        "\n",
        "\n",
        "data = data.drop(91713)\n",
        "data = data.drop(91714)\n",
        "data.tail()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hospital_death</th>\n",
              "      <th>age</th>\n",
              "      <th>elective_surgery</th>\n",
              "      <th>apache_4a_hospital_death_prob</th>\n",
              "      <th>d1_heartrate_max</th>\n",
              "      <th>icu_type</th>\n",
              "      <th>count_na_exams</th>\n",
              "      <th>count_diseases</th>\n",
              "      <th>aids</th>\n",
              "      <th>cirrhosis</th>\n",
              "      <th>diabetes_mellitus</th>\n",
              "      <th>hepatic_failure</th>\n",
              "      <th>immunosuppression</th>\n",
              "      <th>leukemia</th>\n",
              "      <th>lymphoma</th>\n",
              "      <th>solid_tumor_with_metastasis</th>\n",
              "      <th>d1_albumin_max</th>\n",
              "      <th>d1_bilirubin_min</th>\n",
              "      <th>d1_bun_max</th>\n",
              "      <th>gcs_motor_apache</th>\n",
              "      <th>d1_hemaglobin_max</th>\n",
              "      <th>ventilated_apache</th>\n",
              "      <th>d1_arterial_pco2_max</th>\n",
              "      <th>d1_arterial_ph_max</th>\n",
              "      <th>d1_sodium_max</th>\n",
              "      <th>temp_apache</th>\n",
              "      <th>glucose_apache</th>\n",
              "      <th>wbc_apache</th>\n",
              "      <th>d1_sysbp_invasive_min</th>\n",
              "      <th>d1_sysbp_noninvasive_max</th>\n",
              "      <th>d1_calcium_min</th>\n",
              "      <th>d1_hco3_max</th>\n",
              "      <th>d1_inr_min</th>\n",
              "      <th>d1_lactate_min</th>\n",
              "      <th>d1_platelets_min</th>\n",
              "      <th>d1_potassium_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>91708</th>\n",
              "      <td>0.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>115.0</td>\n",
              "      <td>Cardiac ICU</td>\n",
              "      <td>41.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>27.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>13.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>7.34</td>\n",
              "      <td>140.0</td>\n",
              "      <td>36.6</td>\n",
              "      <td>381.0</td>\n",
              "      <td>11.84</td>\n",
              "      <td>96.04</td>\n",
              "      <td>128.0</td>\n",
              "      <td>8.20</td>\n",
              "      <td>28.00</td>\n",
              "      <td>1.10</td>\n",
              "      <td>1.74</td>\n",
              "      <td>92.00</td>\n",
              "      <td>4.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91709</th>\n",
              "      <td>0.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>100.0</td>\n",
              "      <td>Med-Surg ICU</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>34.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>7.26</td>\n",
              "      <td>136.0</td>\n",
              "      <td>37.4</td>\n",
              "      <td>177.0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>81.00</td>\n",
              "      <td>127.0</td>\n",
              "      <td>7.80</td>\n",
              "      <td>30.00</td>\n",
              "      <td>1.43</td>\n",
              "      <td>1.40</td>\n",
              "      <td>133.00</td>\n",
              "      <td>3.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91710</th>\n",
              "      <td>0.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>158.0</td>\n",
              "      <td>Med-Surg ICU</td>\n",
              "      <td>77.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.1</td>\n",
              "      <td>0.97</td>\n",
              "      <td>33.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>12.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>7.36</td>\n",
              "      <td>139.0</td>\n",
              "      <td>35.8</td>\n",
              "      <td>538.0</td>\n",
              "      <td>17.55</td>\n",
              "      <td>96.04</td>\n",
              "      <td>110.0</td>\n",
              "      <td>6.60</td>\n",
              "      <td>20.00</td>\n",
              "      <td>1.43</td>\n",
              "      <td>1.74</td>\n",
              "      <td>227.00</td>\n",
              "      <td>3.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91711</th>\n",
              "      <td>0.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>82.0</td>\n",
              "      <td>Med-Surg ICU</td>\n",
              "      <td>115.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>24.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>7.37</td>\n",
              "      <td>139.0</td>\n",
              "      <td>36.3</td>\n",
              "      <td>158.0</td>\n",
              "      <td>11.84</td>\n",
              "      <td>96.04</td>\n",
              "      <td>153.0</td>\n",
              "      <td>8.21</td>\n",
              "      <td>24.52</td>\n",
              "      <td>1.43</td>\n",
              "      <td>1.74</td>\n",
              "      <td>198.34</td>\n",
              "      <td>3.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91712</th>\n",
              "      <td>0.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.19</td>\n",
              "      <td>104.0</td>\n",
              "      <td>Med-Surg ICU</td>\n",
              "      <td>65.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.50</td>\n",
              "      <td>27.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>7.39</td>\n",
              "      <td>132.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>24.40</td>\n",
              "      <td>96.04</td>\n",
              "      <td>110.0</td>\n",
              "      <td>7.40</td>\n",
              "      <td>22.00</td>\n",
              "      <td>1.50</td>\n",
              "      <td>1.74</td>\n",
              "      <td>420.00</td>\n",
              "      <td>4.90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       hospital_death   age  ...  d1_platelets_min  d1_potassium_min\n",
              "91708             0.0  75.0  ...             92.00              4.20\n",
              "91709             0.0  56.0  ...            133.00              3.80\n",
              "91710             0.0  48.0  ...            227.00              3.20\n",
              "91711             0.0  62.0  ...            198.34              3.93\n",
              "91712             0.0  82.0  ...            420.00              4.90\n",
              "\n",
              "[5 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNmSiK8mOFgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label = data['hospital_death']\n",
        "\n",
        "x = data.copy()\n",
        "\n",
        "del x['hospital_death']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W34BGhs8OoMO",
        "colab_type": "code",
        "outputId": "aee50ed4-3509-4869-8000-f18f0ec47a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "obj_df = data.select_dtypes(include=['object']).copy()\n",
        "obj_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>icu_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CTICU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Med-Surg ICU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Med-Surg ICU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CTICU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Med-Surg ICU</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       icu_type\n",
              "0         CTICU\n",
              "1  Med-Surg ICU\n",
              "2  Med-Surg ICU\n",
              "3         CTICU\n",
              "4  Med-Surg ICU"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvveGEkoOy4C",
        "colab_type": "code",
        "outputId": "93b7571b-6551-4b3b-fb39-ae0c3497ab9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "icuTypes = pd.get_dummies(obj_df, columns=[\"icu_type\"])\n",
        "icuTypes.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>icu_type_CCU-CTICU</th>\n",
              "      <th>icu_type_CSICU</th>\n",
              "      <th>icu_type_CTICU</th>\n",
              "      <th>icu_type_Cardiac ICU</th>\n",
              "      <th>icu_type_MICU</th>\n",
              "      <th>icu_type_Med-Surg ICU</th>\n",
              "      <th>icu_type_Neuro ICU</th>\n",
              "      <th>icu_type_SICU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   icu_type_CCU-CTICU  icu_type_CSICU  ...  icu_type_Neuro ICU  icu_type_SICU\n",
              "0                   0               0  ...                   0              0\n",
              "1                   0               0  ...                   0              0\n",
              "2                   0               0  ...                   0              0\n",
              "3                   0               0  ...                   0              0\n",
              "4                   0               0  ...                   0              0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaRRZMNSP4nb",
        "colab_type": "code",
        "outputId": "9b4ccc24-7a88-4b84-a53c-a3a977e64b7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "x = pd.concat([x,icuTypes], axis=1, sort=False)\n",
        "del x['icu_type']\n",
        "x.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>elective_surgery</th>\n",
              "      <th>apache_4a_hospital_death_prob</th>\n",
              "      <th>d1_heartrate_max</th>\n",
              "      <th>count_na_exams</th>\n",
              "      <th>count_diseases</th>\n",
              "      <th>aids</th>\n",
              "      <th>cirrhosis</th>\n",
              "      <th>diabetes_mellitus</th>\n",
              "      <th>hepatic_failure</th>\n",
              "      <th>immunosuppression</th>\n",
              "      <th>leukemia</th>\n",
              "      <th>lymphoma</th>\n",
              "      <th>solid_tumor_with_metastasis</th>\n",
              "      <th>d1_albumin_max</th>\n",
              "      <th>d1_bilirubin_min</th>\n",
              "      <th>d1_bun_max</th>\n",
              "      <th>gcs_motor_apache</th>\n",
              "      <th>d1_hemaglobin_max</th>\n",
              "      <th>ventilated_apache</th>\n",
              "      <th>d1_arterial_pco2_max</th>\n",
              "      <th>d1_arterial_ph_max</th>\n",
              "      <th>d1_sodium_max</th>\n",
              "      <th>temp_apache</th>\n",
              "      <th>glucose_apache</th>\n",
              "      <th>wbc_apache</th>\n",
              "      <th>d1_sysbp_invasive_min</th>\n",
              "      <th>d1_sysbp_noninvasive_max</th>\n",
              "      <th>d1_calcium_min</th>\n",
              "      <th>d1_hco3_max</th>\n",
              "      <th>d1_inr_min</th>\n",
              "      <th>d1_lactate_min</th>\n",
              "      <th>d1_platelets_min</th>\n",
              "      <th>d1_potassium_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.10</td>\n",
              "      <td>119.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.40</td>\n",
              "      <td>31.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>7.39</td>\n",
              "      <td>136.0</td>\n",
              "      <td>39.3</td>\n",
              "      <td>168.0</td>\n",
              "      <td>14.10</td>\n",
              "      <td>64.00</td>\n",
              "      <td>131.0</td>\n",
              "      <td>7.40</td>\n",
              "      <td>19.00</td>\n",
              "      <td>1.43</td>\n",
              "      <td>1.00</td>\n",
              "      <td>233.00</td>\n",
              "      <td>3.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>77.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.47</td>\n",
              "      <td>118.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.50</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>7.45</td>\n",
              "      <td>145.0</td>\n",
              "      <td>35.1</td>\n",
              "      <td>145.0</td>\n",
              "      <td>12.70</td>\n",
              "      <td>96.04</td>\n",
              "      <td>159.0</td>\n",
              "      <td>8.00</td>\n",
              "      <td>27.00</td>\n",
              "      <td>1.30</td>\n",
              "      <td>3.50</td>\n",
              "      <td>487.00</td>\n",
              "      <td>3.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>96.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>24.5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>7.39</td>\n",
              "      <td>139.0</td>\n",
              "      <td>36.7</td>\n",
              "      <td>158.0</td>\n",
              "      <td>11.84</td>\n",
              "      <td>96.04</td>\n",
              "      <td>148.0</td>\n",
              "      <td>8.21</td>\n",
              "      <td>24.52</td>\n",
              "      <td>1.43</td>\n",
              "      <td>1.74</td>\n",
              "      <td>198.34</td>\n",
              "      <td>3.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>81.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.04</td>\n",
              "      <td>116.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>24.5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>11.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>7.44</td>\n",
              "      <td>139.0</td>\n",
              "      <td>34.8</td>\n",
              "      <td>185.0</td>\n",
              "      <td>8.00</td>\n",
              "      <td>78.00</td>\n",
              "      <td>158.0</td>\n",
              "      <td>8.21</td>\n",
              "      <td>24.52</td>\n",
              "      <td>1.10</td>\n",
              "      <td>1.74</td>\n",
              "      <td>43.00</td>\n",
              "      <td>3.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.09</td>\n",
              "      <td>89.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>24.5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>7.39</td>\n",
              "      <td>139.0</td>\n",
              "      <td>36.7</td>\n",
              "      <td>158.0</td>\n",
              "      <td>11.84</td>\n",
              "      <td>96.04</td>\n",
              "      <td>147.0</td>\n",
              "      <td>8.21</td>\n",
              "      <td>24.52</td>\n",
              "      <td>1.43</td>\n",
              "      <td>1.74</td>\n",
              "      <td>198.34</td>\n",
              "      <td>3.93</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    age  elective_surgery  ...  d1_platelets_min  d1_potassium_min\n",
              "0  68.0               0.0  ...            233.00              3.40\n",
              "1  77.0               0.0  ...            487.00              3.80\n",
              "2  25.0               0.0  ...            198.34              3.93\n",
              "3  81.0               1.0  ...             43.00              3.50\n",
              "4  19.0               0.0  ...            198.34              3.93\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AoeRaFzOhNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, label, train_size=0.8, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5awCNXJ9So1g",
        "colab_type": "code",
        "outputId": "3b2b0b9c-cfb9-41ce-c7d7-8c5646790bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hospital_death</th>\n",
              "      <th>age</th>\n",
              "      <th>elective_surgery</th>\n",
              "      <th>apache_4a_hospital_death_prob</th>\n",
              "      <th>d1_heartrate_max</th>\n",
              "      <th>icu_type</th>\n",
              "      <th>count_na_exams</th>\n",
              "      <th>count_diseases</th>\n",
              "      <th>aids</th>\n",
              "      <th>cirrhosis</th>\n",
              "      <th>diabetes_mellitus</th>\n",
              "      <th>hepatic_failure</th>\n",
              "      <th>immunosuppression</th>\n",
              "      <th>leukemia</th>\n",
              "      <th>lymphoma</th>\n",
              "      <th>solid_tumor_with_metastasis</th>\n",
              "      <th>d1_albumin_max</th>\n",
              "      <th>d1_bilirubin_min</th>\n",
              "      <th>d1_bun_max</th>\n",
              "      <th>gcs_motor_apache</th>\n",
              "      <th>d1_hemaglobin_max</th>\n",
              "      <th>ventilated_apache</th>\n",
              "      <th>d1_arterial_pco2_max</th>\n",
              "      <th>d1_arterial_ph_max</th>\n",
              "      <th>d1_sodium_max</th>\n",
              "      <th>temp_apache</th>\n",
              "      <th>glucose_apache</th>\n",
              "      <th>wbc_apache</th>\n",
              "      <th>d1_sysbp_invasive_min</th>\n",
              "      <th>d1_sysbp_noninvasive_max</th>\n",
              "      <th>d1_calcium_min</th>\n",
              "      <th>d1_hco3_max</th>\n",
              "      <th>d1_inr_min</th>\n",
              "      <th>d1_lactate_min</th>\n",
              "      <th>d1_platelets_min</th>\n",
              "      <th>d1_potassium_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>91708</th>\n",
              "      <td>0.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>115.0</td>\n",
              "      <td>Cardiac ICU</td>\n",
              "      <td>41.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>27.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>13.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>7.34</td>\n",
              "      <td>140.0</td>\n",
              "      <td>36.6</td>\n",
              "      <td>381.0</td>\n",
              "      <td>11.84</td>\n",
              "      <td>96.04</td>\n",
              "      <td>128.0</td>\n",
              "      <td>8.20</td>\n",
              "      <td>28.00</td>\n",
              "      <td>1.10</td>\n",
              "      <td>1.74</td>\n",
              "      <td>92.00</td>\n",
              "      <td>4.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91709</th>\n",
              "      <td>0.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>100.0</td>\n",
              "      <td>Med-Surg ICU</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>34.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>7.26</td>\n",
              "      <td>136.0</td>\n",
              "      <td>37.4</td>\n",
              "      <td>177.0</td>\n",
              "      <td>4.22</td>\n",
              "      <td>81.00</td>\n",
              "      <td>127.0</td>\n",
              "      <td>7.80</td>\n",
              "      <td>30.00</td>\n",
              "      <td>1.43</td>\n",
              "      <td>1.40</td>\n",
              "      <td>133.00</td>\n",
              "      <td>3.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91710</th>\n",
              "      <td>0.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>158.0</td>\n",
              "      <td>Med-Surg ICU</td>\n",
              "      <td>77.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.1</td>\n",
              "      <td>0.97</td>\n",
              "      <td>33.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>12.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>7.36</td>\n",
              "      <td>139.0</td>\n",
              "      <td>35.8</td>\n",
              "      <td>538.0</td>\n",
              "      <td>17.55</td>\n",
              "      <td>96.04</td>\n",
              "      <td>110.0</td>\n",
              "      <td>6.60</td>\n",
              "      <td>20.00</td>\n",
              "      <td>1.43</td>\n",
              "      <td>1.74</td>\n",
              "      <td>227.00</td>\n",
              "      <td>3.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91711</th>\n",
              "      <td>0.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>82.0</td>\n",
              "      <td>Med-Surg ICU</td>\n",
              "      <td>115.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>24.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>7.37</td>\n",
              "      <td>139.0</td>\n",
              "      <td>36.3</td>\n",
              "      <td>158.0</td>\n",
              "      <td>11.84</td>\n",
              "      <td>96.04</td>\n",
              "      <td>153.0</td>\n",
              "      <td>8.21</td>\n",
              "      <td>24.52</td>\n",
              "      <td>1.43</td>\n",
              "      <td>1.74</td>\n",
              "      <td>198.34</td>\n",
              "      <td>3.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91712</th>\n",
              "      <td>0.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.19</td>\n",
              "      <td>104.0</td>\n",
              "      <td>Med-Surg ICU</td>\n",
              "      <td>65.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.50</td>\n",
              "      <td>27.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>7.39</td>\n",
              "      <td>132.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>24.40</td>\n",
              "      <td>96.04</td>\n",
              "      <td>110.0</td>\n",
              "      <td>7.40</td>\n",
              "      <td>22.00</td>\n",
              "      <td>1.50</td>\n",
              "      <td>1.74</td>\n",
              "      <td>420.00</td>\n",
              "      <td>4.90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       hospital_death   age  ...  d1_platelets_min  d1_potassium_min\n",
              "91708             0.0  75.0  ...             92.00              4.20\n",
              "91709             0.0  56.0  ...            133.00              3.80\n",
              "91710             0.0  48.0  ...            227.00              3.20\n",
              "91711             0.0  62.0  ...            198.34              3.93\n",
              "91712             0.0  82.0  ...            420.00              4.90\n",
              "\n",
              "[5 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZDDDqDiRwmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "x.dtypes\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on training set only.\n",
        "scaler.fit(x_train)\n",
        "# Apply transform to both the training set and the test set.\n",
        "train_pca = scaler.transform(x_train)\n",
        "test_pca = scaler.transform(x_test)\n",
        "\n",
        "pca = PCA(.95, svd_solver='full')\n",
        "\n",
        "pca.fit(x)\n",
        "\n",
        "train_pca = pca.transform(x_train)\n",
        "test_pca = pca.transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKk_pqxsfZ9B",
        "colab_type": "code",
        "outputId": "9133d594-424a-4045-9915-2466b320b8fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "clf = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto', priors=None, n_components=None, store_covariance=True, tol=0.0001)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "print(clf.score(x_test, y_test))\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "print(precision_score(y_test, y_pred, average='binary'))\n",
        "print(recall_score(y_test, y_pred))\n",
        "\n",
        "x_test_lda = clf.transform(x_test)\n",
        "x_train_lda = clf.transform(x_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9209507714114376\n",
            "0.5857253685027153\n",
            "0.45182525433871934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmG0tTHamSbr",
        "colab_type": "code",
        "outputId": "da6cbda2-6214-4a6d-89b1-c4e6531142f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "gnb = GaussianNB(priors=None, var_smoothing=0.02)\n",
        "\n",
        "gnb.fit(train_pca, y_train)\n",
        "\n",
        "print(gnb.score(test_pca, y_test))\n",
        "\n",
        "y_pred = gnb.predict(test_pca)\n",
        "\n",
        "print(precision_score(y_test, y_pred, average='binary'))\n",
        "print(recall_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8988169874066402\n",
            "0.31683168316831684\n",
            "0.09575104727707959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "53ddbe2d-623c-49a3-edbd-7dfb9b535a6b",
        "id": "_9ZidK5whZDt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "gnb = GaussianNB(priors=None, var_smoothing=0.02)\n",
        "\n",
        "gnb.fit(x_train_lda, y_train)\n",
        "\n",
        "print(gnb.score(x_test_lda, y_test))\n",
        "\n",
        "y_pred = gnb.predict(x_test_lda)\n",
        "\n",
        "print(precision_score(y_test, y_pred, average='binary'))\n",
        "print(recall_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9212233549582948\n",
            "0.5889763779527559\n",
            "0.44763614602034707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7AG2_erfLJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "x_train_scaled = preprocessing.scale(x_train)\n",
        "\n",
        "x_test_scaled = preprocessing.scale(x_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9o7S8zLJmtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, normalize\n",
        "scaler = MinMaxScaler()\n",
        "scaled_positive_x = scaler.fit_transform(x_train)\n",
        "scaled_positive_x_test = scaler.fit_transform(x_test)\n",
        "normalized_x = normalize(scaled_positive_x, norm='l1', axis=1, copy=True)\n",
        "normalized_x_test = normalize(scaled_positive_x_test, norm='l1', axis=1, copy=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m16Fs-8Ui7ZN",
        "colab_type": "code",
        "outputId": "aa30c133-df16-45a9-b37d-fd9c90303042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.naive_bayes import ComplementNB\n",
        "#norm afecta bastante el recall y precision. el alpha no mucho hasta valores altos\n",
        "clf = ComplementNB(alpha=0.1,fit_prior=False, class_prior=None, norm=False)\n",
        "clf.fit(normalized_x, y_train)\n",
        "\n",
        "clf.score(normalized_x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7694488360682549"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc8dPiuBK8pT",
        "colab_type": "code",
        "outputId": "64c171ab-0337-46ee-ce19-3de02e7f24d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "y_pred = clf.predict(normalized_x_test)\n",
        "\n",
        "print(precision_score(y_test, y_pred, average='binary'))\n",
        "print(recall_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.24783123028391169\n",
            "0.7522441651705566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6feba131-9a49-44bd-cf08-d6e8233f4801",
        "id": "0t0icVWr998F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "prec = []\n",
        "rec = []\n",
        "\n",
        "for i in np.linspace(0,100,21):\n",
        "  clf = ComplementNB(alpha=i,fit_prior=False, class_prior=None, norm=False)\n",
        "  clf.fit(normalized_x, y_train)\n",
        "\n",
        "  clf.score(normalized_x_test, y_test)\n",
        "\n",
        "  y_pred = clf.predict(normalized_x_test)\n",
        "\n",
        "  currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "  currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "  print(i)\n",
        "  print(currPrec)\n",
        "  print(currRec)\n",
        "\n",
        "  prec.append(currPrec)\n",
        "  rec.append(currRec)\n",
        "\n",
        "# Data\n",
        "df=pd.DataFrame({'x': np.linspace(0,100,21), 'precision': prec, 'recall': rec })\n",
        " \n",
        "# multiple line plot\n",
        "plt.plot( 'x', 'precision', data=df, marker='', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4, label=\"Precision\")\n",
        "plt.plot( 'x', 'recall', data=df, marker='', color='olive', linewidth=2, label=\"Recall\")\n",
        "plt.xlabel(\"Alpha\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "0.25039808917197454\n",
            "0.7528426092160383\n",
            "5.0\n",
            "0.26028547439126787\n",
            "0.7420706163973668\n",
            "10.0\n",
            "0.2748126277538042\n",
            "0.7241172950329144\n",
            "15.0\n",
            "0.28663048840651206\n",
            "0.6953919808497906\n",
            "20.0\n",
            "0.29453494482396214\n",
            "0.6708557749850389\n",
            "25.0\n",
            "0.3057288712422008\n",
            "0.6451226810293238\n",
            "30.0\n",
            "0.3157894736842105\n",
            "0.6247755834829444\n",
            "35.0\n",
            "0.32308698495748855\n",
            "0.5912627169359664\n",
            "40.0\n",
            "0.32802216834083825\n",
            "0.5667265110712149\n",
            "45.0\n",
            "0.3355506282335551\n",
            "0.5433871932974267\n",
            "50.0\n",
            "0.3408560311284047\n",
            "0.5242369838420108\n",
            "55.0\n",
            "0.34481344813448134\n",
            "0.5032914422501497\n",
            "60.0\n",
            "0.34812580784144764\n",
            "0.48354278874925194\n",
            "65.0\n",
            "0.352994555353902\n",
            "0.4655894673847995\n",
            "70.0\n",
            "0.35608591885441526\n",
            "0.4464392579293836\n",
            "75.0\n",
            "0.3567486201705971\n",
            "0.4254937163375224\n",
            "80.0\n",
            "0.36074270557029176\n",
            "0.4069419509275883\n",
            "85.0\n",
            "0.3675404350250976\n",
            "0.3943746259724716\n",
            "90.0\n",
            "0.3693379790940767\n",
            "0.38061041292639136\n",
            "95.0\n",
            "0.37492391965916005\n",
            "0.3686415320167564\n",
            "100.0\n",
            "0.382842509603073\n",
            "0.35786953919808495\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f11e13a16a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxU5dn/8c81W/aEJWGRJexrwiJB\nwIAsgiAu2AJVpCqtrQuI9PH3tNX6VNTa2gWtreICiGhrxbUWXIqIbMoWNgkEBMIiYSds2Sczc//+\nmCEGDJCQDCeZud6vV14zZ5mZaxy8v+fc55z7iDEGpZRS4ctmdQFKKaWspUGglFJhToNAKaXCnAaB\nUkqFOQ0CpZQKcw6rC6iqxMRE06pVK6vLUEqpOmXdunXHjDFJFS2rc0HQqlUr1q5da3UZSilVp4jI\n3vMt064hpZQKcxoESikV5jQIlFIqzNW5YwRKqdBWWlpKTk4OxcXFVpdSJ0VGRtK8eXOcTmelX6NB\noJSqVXJycoiLi6NVq1aIiNXl1CnGGHJzc8nJyaF169aVfp12DSmlapXi4mIaNmyoIXAJRISGDRtW\neW9Kg0ApVetoCFy6S/lvFzZdQ1988X/k5KykbdvhtG07nMaNu+k/NqWUIoz2CLZv/4jdu7/g889/\nzSuv9ODZZ5vx4YcTyMx8i8LCY1aXp5SqRex2Oz169CAlJYWxY8dSWFhY7fdcu3YtDz744HmXHzhw\ngDFjxlT7cy6F1LUb06SlpZlLubK4qOg4u3Z9zs6d/yU7ewF5eQfKLRWuuCKNtm2H067dcJo374vN\nFjY7S0rVKlu3bqVz586W1hAbG0t+fj4A48ePp1evXjz00ENly40xGGOw2WrntnRF/w1FZJ0xJq2i\n9cOmtYuKakDXrj+ia9cfYYzh6NEtZaGwd+8yDhzI4MCBDJYvf4qIiATatLm2rBupXr1kq8tXKuz8\ncUNw99Qf7plYqfUGDBjApk2b2LNnD8OHD6dPnz6sW7eOTz75hG+++YapU6dSUlJC27Ztee2114iN\njSUjI4MpU6ZQUFBAREQEixYtYt26dUybNo2PPvqIpUuXMmXKFMDfp79s2TJyc3O58cYb2bx5M8XF\nxdx///2sXbsWh8PBs88+y+DBg5kzZw7z5s2jsLCQ7OxsfvCDH/DnP/+52v8twiYIyhMRGjVKoVGj\nFK6++n9xuwvYu3cpO3cuIDt7Abm537B16wds3foBAImJnWjbdgRXXnk3jRqlWFy9Uupy8Xg8fPrp\np4wYMQKAHTt28Prrr9O3b1+OHTvGU089xeeff05MTAx/+tOfePbZZ3n44Ye59dZbefvtt+nduzen\nT58mKirqrPedNm0a06dPJz09nfz8fCIjI89aPn36dESEzMxMtm3bxnXXXcf27dsB2LhxIxs2bCAi\nIoKOHTsyefJkWrRoUa3vGZZBcC6XK4b27UfSvv1IAE6e3FMWCrt3L+LYsW0cO7aN1aufo0OHG7n6\n6l/RsmV/PdisVIgqKiqiR48egH+P4O677+bAgQMkJyfTt29fAFatWkVWVhbp6ekAuN1u+vXrxzff\nfEPTpk3p3bs3APHx8d97//T0dB566CHGjx/PD3/4Q5o3b37W8i+//JLJkycD0KlTJ5KTk8uC4Npr\nryUhIQGALl26sHfvXg2CYKhXrxVpafeSlnYvXm8pOTmr2Lx5Lhs3zmb79o/Yvv0jmjfvR3r6r+nY\n8SZEamc/oVLq0kRFRbFx48bvzY+JiSl7boxh2LBhvPXWW2etk5mZedH3f/jhh7nhhhv45JNPSE9P\nZ8GCBd/bKzifiIiIsud2ux2Px1Op112IBsFF2O1OkpMHkJw8gEGDHmfNmudZs+YFcnJW8vbbt5CY\n2Imrr/4l3br9GLvdZXW5SoWMyvbhW6Vv375MmjSJnTt30q5dOwoKCti/fz8dO3bk4MGDZGRk0Lt3\nb/Ly8r7XNZSdnU1qaiqpqalkZGSwbdu2sj0Q8O+FvPnmmwwZMoTt27fz7bff0rFjR9avXx+U76Kb\nslUQE5PE4MFP8j//8y3Dhz9HfHwLjh3bxrx5d/O3v7VmxYpplJSctrpMpdRlkJSUxJw5cxg3bhzd\nunWjX79+bNu2DZfLxdtvv83kyZPp3r07w4YN+96Vvs899xwpKSl069YNp9PJ9ddff9byiRMn4vP5\nSE1N5dZbb2XOnDln7QnUtLA5fTQYvN5SNm+ey4oVf+bIkc0AREQk0Lv3RPr0eZDY2CYWV6hU3VMb\nTh+t66p6+qjuEVSD3e6ke/c7uO++TYwb9xEtWw6gpOQUX375NM8914r58+8lN3eH1WUqpdQFaRDU\nABGhQ4cb+MlPlvHTn66gY8dReL0lrF8/gxde6Mi7747l6NGtVpeplFIV0iCoYS1a9OO22z5k4sQs\nevT4KTabg6ys95gxoxcbNsymrnXFKaVCnwZBkCQldWbUqFeZMmU33bvficdTxLx5d/Pvf/+YkpI8\nq8tTSqkyGgRBFh/fjFtueZ1bbnkdpzOGzMx/MWNGLw4e3GB1aUopBWgQXDbdu9/JPfespVGjVI4f\n38Grr/ZlzZrp2lWklLKcBsFllJjYiZ/9bDW9et2L1+vm008f4N13x1BcfNLq0pRS5ZQfhvqmm27i\n5Mma/X90zpw5PPDAAwA8/vjjTJs2rUbfv6o0CC4zpzOKG298mdGj5+JyxbF16we88kpPcnJWW12a\nUirgzBATmzdvpkGDBkyfPt3qkoJKg8AiKSm3cu+9G2jatBcnT+7htdf6s2LFM9pVpFQt069fP/bv\n3182/Ze//IXevXvTrVs3pk6dWjb/jTfeoFu3bnTv3p077rgDgPnz59OnTx969uzJ0KFDOXz48GWv\nvzKCOtaQiIwA/gbYgVnGmD+es/yvwODAZDTQyBhTL5g11SYNGrTlpz/9is8/f5jVq59j4cL/Ze/e\nJYwaNYfo6IZWl6eU5Z54Ijgj/E6dWrkNLq/Xy6JFi7j77rsB+Oyzz9ixYwdr1qzBGMPNN9/MsmXL\naNiwIU899RQrVqwgMTGR48ePA9C/f39WrVqFiDBr1iz+/Oc/88wzzwTlO1VH0IJAROzAdGAYkANk\niMg8Y0zWmXWMMf9Tbv3JQM9g1VNbORwRjBjxV1q1GsR//jOB7ds/4pVXejB69Fu0bNnf6vKUCktn\nhqHev38/nTt3ZtiwYYA/CD777DN69vQ3Vfn5+ezYsYOvv/6asWPHkpjoHyivQYMGAOTk5HDrrbdy\n8OBB3G43rVu3tuYLXUQw9wiuAnYaY3YBiMhcYBSQdZ71xwFTz7Ms5HXqNIomTTby/vvjyMlZyZw5\ngxg8+En6939Yh7lWYauyW+417cwxgsLCQoYPH8706dN58MEHMcbwyCOPcO+99561/vPPP1/h+0ye\nPJmHHnqIm2++mSVLlvD4449fhuqrLpgtTDNgX7npnMC87xGRZKA18MV5lt8jImtFZO3Ro0drvNDa\nol69ZCZMWEp6+q8xxssXXzzKP/85gvz82tmvqFSoi46O5u9//zvPPPMMHo+H4cOHM3v27LL7Ge/f\nv58jR44wZMgQ3n33XXJzcwHKuoZOnTpFs2b+Zu/111+35ktUQm3Z1LwNeM8Y461ooTFmhjEmzRiT\nlpSUdJlLu7zsdidDh/6R8eM/JTo6kV27FvLSSyls2vRPPZCslAV69uxJt27deOutt7juuuu4/fbb\n6devH6mpqYwZM4a8vDy6du3Ko48+ysCBA+nevXvZje4ff/xxxo4dS69evcq6jWqjoA1DLSL9gMeN\nMcMD048AGGOermDdDcAkY8yKi71vbRqGOtjy8g7w73/fwe7d/h2ltm2v44YbXqJ+/TYWV6ZU8Ogw\n1NVXm4ahzgDai0hrEXHh3+qfd+5KItIJqA+sDGItdVJc3BXcccfn3HzzbCIj65Od/RkvvpjCihXT\n8Pmqf3s6pZSCIAaBMcYDPAAsALYC7xhjtojIkyJyc7lVbwPmGu33qJCI0LPnT5g0aSspKePweIpY\nuPCXzJx5FQcOrLO6PKVUCNA7lNUxO3Z8yscf38+pU3sRsdGnzy8YPPhJXK6Yi79YqTpg69atdOrU\nCZHgXEMQ6owxbNu2rdZ0DakgaN/+eiZO3Ezfvv6DUatWPcuLL3Zlx45PLa5MqZoRGRlJbm6unhxx\nCYwx5ObmEhkZWaXX6R5BHXbgwDrmz/85hw75h7ROSRnHiBHPERPTyOLKlLp0paWl5OTkfO+G76py\nIiMjad68OU6n86z5F9oj0CCo43w+D6tWPcfixY/h8RQRGVmf6657hh49JuiutVKqjHYNhTCbzcHV\nV/8vEyduoW3b6yguPsG8eT/ljTeuJTd3h9XlKaXqAA2CEFG/fmvGj/8vP/jBP4mOTmTPnsW89FIq\ny5f/Aa+31OrylFK1mAZBCBERunUbz6RJW+ne/S683hK++OJRXn21L4cPZ1pdnlKqltIgCEHR0Ync\ncssc7rhjIQkJyRw8uJ4ZM3qxbNlTuneglPoeDYIQ1qbNUO6/P5O0tPvx+UpZvPi3zJrVh8OHN1ld\nmlKqFtEgCHEREXHccMOL3HnnIhISkjl0aAMzZqSxdOnvdO9AKQVoEISN1q2HBPYOJuLzlbJkyWPM\nmtWHQ4e+tro0pZTFNAjCiH/vYDp33vkF9eq14tChDcycmcbSpU/q3oFSYUyDIAy1bj2Y++/PpHfv\nSfh8HpYsmcqsWVdx6NBGq0tTSllAgyBMuVyxjBz5QmDvoDWHDm1k5szeLFnyOF6v2+rylFKXkQZB\nmPPvHWyid+8H8Pk8LF36BDNn6t6BUuFEg0AF9g6e5667FlO/fhsOH/6amTN7s3jxVEpLi6wuTykV\nZBoEqkyrVoO4775NXHXVZHw+D8uWPcnzz7dn3bqZejBZqRCmQaDO4nLFcP31f2fChKU0btydvLz9\nfPTRPbz4Ylc2b56LMT6rS1RK1TAdhlqdlzE+tmx5l8WLf8vx4/6RTBs37s6QIb+nffuROsy1UnWI\n3o9AVYvXW8rGjXNYuvQJ8vL2A9CiRTrXXvs0yckDLK5OKVUZGgSqRng8xWRkvMjy5X+gqCgXgHbt\nRjBkyB9o2rSnxdUppS5Eg0DVqJKS06xc+VdWrnwGtzsPgC5dxjJ48O9ITOxocXVKqYpoEKigKCw8\nxvLlT5ORMR2vtwQROz16TGDgwKkkJLSwujylVDkaBCqoTp/OYenSJ9mwYTbGeLHbXaSlTWTgwMeI\niqpvdXlKKfSexSrI4uObc9NNM5g0KYuUlNvwet2sXv0cs2b14fjxbKvLU0pdhAaBqjENG3Zg9Oi3\nuPfeDTRu3I3jx3fw6qt92bdvpdWlKaUuQINA1bgmTXrwk58sp23b4RQWHuONN4aQlfWe1WUppc5D\ng0AFRUREPOPGzefKK+/B4ynm3XfHsmLFNOraMSmlwoEGgQoau93JjTe+zNChfwJg4cJf8skn/nsg\nKKVqDw0CFVQiQnr6rxg9ei52ewRr177E3LmjcLvzrS5NKRWgQaAui5SUW7nzzkVERTVkx45PeO21\nAZw+vd/qspRSBDkIRGSEiHwjIjtF5OHzrPMjEckSkS0i8q9g1qOs1bJlOnffvZIGDdpx6NBGXn21\nL4cPb7K6LKXCXtCCQETswHTgeqALME5EupyzTnvgESDdGNMV+EWw6lG1Q8OG7bn77pW0aHE1p0/n\nMHt2f7KzP7O6LKXCWjD3CK4Cdhpjdhlj3MBcYNQ56/wcmG6MOQFgjDkSxHpULREdnciddy6ia9cf\n4Xbn8eabI1m/fpbVZSkVtoIZBM2AfeWmcwLzyusAdBCRr0RklYiMqOiNROQeEVkrImuPHj0apHLV\n5eRwRDJ69Fukp/8aY7zMn/9zFi16VG98o5QFrD5Y7ADaA4OAccBMEal37krGmBnGmDRjTFpSUtJl\nLlEFi4iNoUP/yI03voKInS+//AMffDAej6fY6tKUCivBDIL9QPkhKJsH5pWXA8wzxpQaY3YD2/EH\ngwojvXrdw+23f4TLFcvmzXP5xz+GUViYa3VZSoWNYAZBBtBeRFqLiAu4DZh3zjof4t8bQEQS8XcV\n7QpiTaqWatduBD/5yZfExTXj22+/5OWXu7Nt23+sLkupsBC0IDDGeIAHgAXAVuAdY8wWEXlSRG4O\nrLYAyBWRLGAx8EtjjG4KhqkmTbrzs5+tolmzPuTl7eftt2/hnXfGkJd3wOrSlAppej8CVev4fF4y\nMl7kiy9+g9udT0REAkOH/olevX6OiNWHtZSqm/R+BKpOsdns9OkzmYkTs+jQ4UZKSk7x8cf38dpr\n13D0aJbV5SkVcjQIVK2VkNCC226bx5gx7xAT05h9+77i5Zd7sGTJ43g8JVaXp1TI0CBQtZqI0LXr\nWCZN2sqVV/4cn6+UpUuf4JVXerB373Kry1MqJGgQqDohKqo+N900gwkTltKwYUeOHdvGnDnXMH/+\nvRQXn7S6PKXqNA0CVackJ1/Dffd9zTXXPIbN5mT9+hlMn96ZrKz39KY3Sl0iDQJV5zgcEQwe/AT3\n3beRFi3Syc8/xLvvjmXu3FGcOrXv4m+glDqLBoGqs5KSuvCTnyzjhhteIiIinu3b5/Pii11Yvfrv\nehc0papAg0DVaSI20tLuY9KkrXTu/EPc7nz++98pzJzZm337VlpdnlJ1ggaBCglxcVfwox+9z623\nfkhCQjKHDm1k9uyr+c9/7qagQEesVepCNAhUSOnUaRSTJmUxYMCj2O0uNm6czQsvdCAj4yV8Pq/V\n5SlVK2kQqJDjdEYzZMhT3H9/Jm3aDKO4+CSffDKRV1/ty/79GVaXp1Sto0GgQlbDhh348Y8XMHbs\nu8THN+fAgbXMmtWH+fPv1WGulSpHg0CFNBGhS5cxTJq0lfT0X2Oz2Vm/fgYvvNCR9etn6R3RlEKD\nQIUJlyuWoUP/yH33baJVq8EUFeUyf/7PefXVqzlwYJ3V5SllKQ0CFVaSkjpz552LGD36LWJjm7J/\n/2pmzuzNxx9PoqjohNXlKWUJDQIVdkSElJTbeOCBbfTt+xAiNtaufZEXXujIhg2v6dlFKuzojWlU\n2DtyZDOffDKJvXuXAdCwYUeuueb/SEm5DZvNYXF1StWMC92YRoNAKcAYQ2bmmyxe/FtOntwDQIMG\n7Rkw4FG6dRuvgaDqPA0CpSrJ6y1l06Z/snz57zlxIhuA+vXbBgLhx9jtTosrVOrSaBAoVUU+n4dN\nm95k+fKnOH58JwD16rVmwIDf0L37ndjtLosrVKpqNAiUukQ+n4fNm+eybNnvyM3dDkBCQjIDBvyG\nHj0maCCoOkODQKlq8vm8bNnyNsuW/Y5jx7YBEB/fgv79H6Fnz5/icERYXKFSF1YjQSAi/YH2xpjX\nRCQJiDXG7K7BOitFg0BZyefzkpX1HsuW/Y6jR7cAEB/fnPT0h7nyyrtxOCItrlCpilU7CERkKpAG\ndDTGdBCRK4B3jTHpNVvqxWkQqNrAGB9ZWe+zbNmTHDmyGfAPhX3NNY/Rs+dP9aCyqnUuFASVvaDs\nB8DNQAGAMeYAEFcz5SlV94jY6Np1LPfd9zU/+tH7NG7cnby8A3z88X289FIKW7d+oPdQVnVGZYPA\nbfz/qg2AiMQErySl6g4RG507/5B7713PmDHv0KBBO3Jzt/POO6OZPTudb7/90uoSlbqoygbBOyLy\nClBPRH4OfA7MDF5ZStUtZ/YQJk7MYuTI6cTENCInZyWvvTaAt966maNHs6wuUanzqsrB4mHAdYAA\nC4wxC4NZ2PnoMQJVF5SU5LFy5TOsWDGN0tICRGx07z6BwYOfID6+udXlqTBUrYPFImIHPjfGDA5G\ncVWlQaDqkvz8Qyxd+jvWr5+Bz+fB4YikT59f0L//r4mMrGd1eSqMVOtgsTHGC/hEJKHGK1MqxMXG\nNuGGG6YzcWIWXbqMxeMp5quv/sjf/96WlSufxeMpsbpEpSp9+uh/gJ7AQgJnDgEYYx4MXmkV0z0C\nVZft37+GhQt/xd69SwH/VcqDB/+Obt3GI6KjwqvgqYnTRz8AfgssA9aV+7vYB48QkW9EZKeIPFzB\n8gkiclRENgb+flbJepSqk5o1u4q77lrM7bd/TKNGKZw6tZcPP7yTV165ku3bP9ZTTpUlqnKw2AV0\nCEx+Y4wpvcj6dmA7MAzIATKAccaYrHLrTADSjDEPVLZg3SNQocLn87Jp0z9YvPi3nD6dA0Dz5n0Z\nNOhJ2rQZiohYXKEKJdXeIxCRQcAOYDrwIrBdRK65yMuuAnYaY3YZY9zAXGBUpatWKsTZbHZ69JjA\nAw9sZ9iwaURHJ5KTs4p//vM65swZyJ49S60uUYWJynYNPQNcZ4wZaIy5BhgO/PUir2kG7Cs3nROY\nd67RIrJJRN4TkRYVvZGI3CMia0Vk7dGjRytZslJ1g9MZxdVX/z+mTNnNtdc+TVRUA779djmvvz6I\nN94Yyr59K6wuUYW4ygaB0xjzzZkJY8x2oCYGU5kPtDLGdMN/IPr1ilYyxswwxqQZY9KSkpJq4GOV\nqn1crlj693+YKVN2M2jQE0REJLB79yJmz07nzTevZ//+DKtLVCGqskGwVkRmicigwN9M4GId9fuB\n8lv4zQPzyhhjco0xZ86fmwX0qmQ9SoWsiIh4Bg58jClTdjNgwP/hcsWyc+d/mTXrKt5662YOHdpo\ndYkqxFT29NEIYBLQPzBrOfBiuUa8otc48B8svhZ/AGQAtxtjtpRbp6kx5mDg+Q+AXxtj+l6oFj1Y\nrMJNYeExVqyYxpo1z1NaWghA584/ZNCgJ2jUKMXi6lRdURPDUMcAxYGLy86cERRhjCm8yOtGAs8B\ndmC2Meb3IvIksNYYM09EnsY/qqkHOA7cb4zZdqH31CBQ4So//zBfffVn1q59EY+nGBBSUm5l4MCp\nJCZ2sro8VcvVRBCsAoYaY/ID07HAZ8aYq2u00krQIFDhLi/vAMuXP8369TPwet2I2EhJGUd6+q9o\n3Lib1eWpWqomLiiLPBMCAIHn0TVRnFKqauLirmDkyOeZPHknvXrdi4iNzMw3efnl7rz55kj27Fmq\nF6apKqlsEBSIyJVnJkQkDSgKTklKqcpISGjBjTe+zOTJO7nqqgdxOqPZufNTXn99EK++2o+tW/+N\nMT6ry1R1QGW7hnrjvyDsQGBWU+BWY8xFh5moado1pFTFCguPsWbNdNaseZ6iolwAGjbswNVX/5Ju\n3e7A4YiwuEJlpUs+RhAIgH3GmEMi4gTuBX4IZAGPGWOOB6PgC9EgUOrC3O4CNmyYzcqVz3Dq1F4A\nYmOb0rfvL+jV614iI3Ug4XBUnSBYj/8g8fHAkBJzgclAD6CzMWZMMAq+EA0CpSrH6y0lK+tdvvrq\nTxw+vAnwX6OQlnY/ffpMIS6uqcUVqsupOkHwtTGme+D5dOCoMebxwPRGY0yPINR7QRoESlWNMYbs\n7AV89dWf2LNnCQB2u4tu3e4kPf2XNGzY4cJvoEJCdc4asgcuDAP/hWFflFvmqGB9pVQtIyK0azeC\nu+5azM9+tprOnUfj9ZayYcMsXnihE++8M0avVg5zF9sjeBQYCRwDWgJXGmOMiLQDXjfGpF+eMr+j\newRKVV9u7nZWrJjG11+/jtfrBqBjx1EMHPgYTZteeZFXq7qouvcs7ov/LKHPjDEFgXkdgFhjzPqa\nLvZiNAiUqjl5eQdZseIvrF37Mh6P/4zwDh1uZODAqVxxRYVthqqjqn1lcW2iQaBUzcvPPxwIhJfK\nxjNq334kAwdOpVmzqyyuTtUEDQKlVKUUFBxhxYpnyMiYTmmp//bkbdsOZ+DAqbRo0c/i6lR1aBAo\nparEP+LpM2RkvIDb7R9dpk2boQwcOJWWLftf5NWqNtIgUEpdksLCXFat+iurV/8dtzsPgNathzBw\n4FSSky92t1pVm2gQKKWqpajoBKtWPcfq1X+jpOQUAMnJAxk4cCqtWw+2uDpVGRoESqkaUVx8klWr\n/sbq1c9RXHwSgDZthjF06J9o2rSnxdWpC9EgUErVqOLiU6xZ8zwrVkwr20NITR3P4MG/o3791hZX\npyqiQaCUCorCwlyWL/8DGRkv4PW6sdtdpKVN5JprHiU6OtHq8lQ5GgRKqaA6eXIPixc/xqZN/wQM\nERHxpKf/mr59f4HTqfewqg00CJRSl8WhQ1+zaNHD7Nz5X8B/N7VBg56gR48J2Gw6PJmVNAiUUpfV\n7t1fsHDhrzh40H/vqsTEzlx77dN07HgzImJxdeFJg0ApddkZ42PLlnf54ovfcOLELgBatEhn2LA/\n06LF1RZXF340CJRSlvF63axd+wrLlj1JYeExADp1uoVrr32axMROFlcXPjQIlFKWKyk5zYoV01i5\n8hlKSwsRsZGaOp7+/R8hKamz1eWFPA0CpVStkZ9/iCVLnmD9+pkY4wWEzp1/yIABv9F7IQSRBoFS\nqtY5cWI3K1b8hQ0bZuP1lgDQrt0IBgx4VAe2CwINAqVUrZWXd5CVK59h7dqXy4a+Tk6+hgEDHqVN\nm2F6llEN0SBQStV6hYW5rF79N9aseb5sHKMrrkijf//f0KnTKEQudot1dSEaBEqpOqOk5DQZGS+x\natWzFBQcASApqSv9+z9CSsqtemHaJdIgUErVOaWlhaxf/yorVvyF06f3AVC/fhvS039N9+534XBE\nWFxh3aJBoJSqs7xeN5s2/ZMvv/wjx4/vAPxDV1x55T106TKGpKQuehyhEiwLAhEZAfwNsAOzjDF/\nPM96o4H3gN7GmAu28hoESoUnn89LVta7LF/+B44cySyb37BhRzp3Hk2XLqNp0qSnhsJ5WBIEImIH\ntgPDgBwgAxhnjMk6Z7044GPABTygQaCUuhBjfGRnL2TLlnf45psPKSo6XrasXr3WZaHQrNlVeoC5\nHKuCoB/wuDFmeGD6EQBjzNPnrPccsBD4JfC/GgRKqcry+Tzs2bOUrKz32Lbt3xQUHC5bFhfXjM6d\nf0jnzqNp2bI/NpvdwkqtZ1UQjAFGGGN+Fpi+A+hjjHmg3DpXAo8aY0aLyBLOEwQicg9wD0DLli17\n7d27Nyg1K6XqLp/Py759K9i69X22bv2g7AAzQExMIzp2vIUuXcbQqtUg7HanhZVao1YGgfj32b4A\nJhhj9lwoCMrTPQKl1MUYYzsds9QAABJTSURBVDhwIIOsrPfZuvW9stFPASIj69Op0w9ITb2dVq0G\nhc2eQq3sGhKRBCAbyA+8pAlwHLj5QmGgQaCUqgpjDIcPfx0Ihfc5dmxr2bK4uCtISRlHaup4mjTp\nEdIHmq0KAgf+g8XXAvvxHyy+3Riz5TzrL0H3CJRSQXb0aBaZmW+xefO/ztpTSEzsTGrqeFJTb6d+\n/dYWVhgcVp4+OhJ4Dv/po7ONMb8XkSeBtcaYeeesuwQNAqXUZWKMISdnFZmZb7Jly9tl90oAaNHi\nalJTx9O164+Ijk60sEo/r89wpNjDoUIPybEuGkRWvTtLLyhTSqkL8HpL2bVrIZmZb7Jt24eUlhYC\nYLM5aNduBKmp4+nY8Waczuig1+IzhmPFXg4W+hv+g4UejhZ58Aaa6qHNYkhrFFXl99UgUEqpSnK7\n89m27UMyM98kO3th4J4J4HLFlh1kbt16CHa7q9qfZYzheIm/0T/T8B8u9OC5QLPctX4EN7WKq/Jn\naRAopdQlKCg4wubNb5OZ+Sb7968umx8REU+7dtfTseMo2rcfSWRkwkXfyxjDKbfvrEb/UKEHt69q\nbXDDSDs/71y/yt9Fg0Apparp+PGdZGb+i6ys984a4sJmc9Cq1WA6dhxFhw434YhpzvESLydKvJwo\n9n73vMR7wS39i4l32mgS7eCKGAd9GkVV+QwnDQKllKohxhgOHM0mc+uHZG+fx7GDX4HxlS2XhG7Y\nmozE1mQEEn9pA+LFOIQm0Q6aRjtpGu2gSbSDGGf1hsvQIFBKKfyNeF6pj0KPwe0zlHr9j+Wflwam\n3d7vnpeWTUNBqY+Sct05xn0c3+GF+A59iu/IYvAWfveB0S2xNxmBrcn1SIO+SAX3Uoi0S1ljf+Yx\nzmmr8WsaNAiUUmGn2OPjaLGXo0Wesx5LvMFr84y3CN/R5f5QOLQA3Ee/W+isj7PJMBq2uI5WbYbS\nskETmkY7SHDVfKNfEQ0CpVTI8vgMucVejhZ7OFr03WNeqe/iLw4Cl02oH2Gjvkuwn1pPfs7HHN7z\nEaeOby+3lnDFFWm0bTucdu2G07x536DfeU2DQClVZxljKPH6u3TyS33kBf6OBbbwjxd7udxNvtMG\n9SPs1I+w0yDweOZ5tEMq3MI/duwbtm+fT3b2AvbuXYbX6y5bFhGRQJs219K27XDath1OvXrJNV6z\nBoFSqlYq9RnyyzXw+aU+8tzes6bzS33VOtvmXC6bUC/ChssmuGyC0x54tAmu8s/PWkbZsgi7jZjz\nNPaV/t6lhezZs5SdO/9LdvYCcnO/OWt5YmKnwN7CCJKTr6mRC9k0CJRSVWaM/+BoocdHkcd/gLXQ\n46PIa8rmFXsNPuO/Gtb/CD6+e+41BlPuuQ8wgedeQ1D7621Ag0g7SZF2kqIcJEXZaRTlID4IB2Kr\n6+TJPezcuYDs7AXs3r2IkpLTZcvs9giSk6+hbdvhtG9/PUlJXS7pMzQIlFJlvMaQ5/Zx0u3llNu/\nxe1v2M1ZjX6Rp2a3xIMp3mkjKcpOUqS/wU+KctAwwo7dVrsa/MrwekvJyVlFdrY/GA4cWAf4f4g2\nbYZyxx0LL+l9LxQEwT06oZS67HzGcNrt41SgoT/l9nKyxP942u3vcqkj7XsZh0Ccy0as00ac006s\n00Y9l82/pR9pJ9IROrektNudJCcPIDl5AEOGPEVBwVF27VpIdvYCWrYcEJTP1CBQqo4wxvi7ZUq/\n66YpCPyV38LPc9edhl4g0Lj7G/nyz8vmuWxE2KrXJ1+XxcQkkZp6O6mptwftMzQIlLKQ12fI93zX\nPVNYaijwBJ57DAVn5gemL3cDbxeIdtiIcgjRDttZz6PsQqTDhl3AJmAXwQbYRLAF5pU9r2C+XSDC\nLtjCtIGvTTQIlAoCnzEUekzg7Bdv2dkv554hU2hRJ3ysw0ZChI0El504p43oihp6h/8smXDdEg8n\nGgRKXQKvzz+S5IkSLyfd/gHFzhx4PfNnZfdMjENIcNlJcNlIiPA/1nPZSXDZiXfZcNTBg6gqeDQI\nlDqPEq+PkyU+Tri9nAyMHnlm2qp++Ai7EO0QYgJb79EOG9FOIdZho16gwY932XFqQ6+qQINAhb2C\nUh+Hi/xjw+cWe8u28i9Xt02MQ8oOlJY17g4h5pzpaIduyavg0CBQYePMyJOHCj1lDf/hIn//fTBE\n2b9r4M93ZkyM06YHS5XlNAhUSDLGcNLtK7v136Ei/2NRDV7JGue0US/C3/deP8JOvQg78eUae916\nV3WFBoGq04o9Pk6Wu3jqZIl/9Mkjhd6zxoy/FDaBBJeN+i5/I18vwh4YVdJOQoT2w6vQoUGgajW3\n13zXyLu9nCop99ztq5GxauwCSVEOGkfZaRzloMGZrXuXdtuo8KBBoGqFMwdsDxd6OFLkKdvKr+kD\ntk4bNI5y0Dja4X+McpAYZceuDb4KYxoE6rIyxnCiJNDoF3k4EjhwWxCEM3Qi7EKTQKPvf/T35etW\nvlJn0yBQQVPqMxwr8p+Zc+RMw1/koSZP0rELJLjs1Ct34VT9CH8Xz+W6BaBSdZ0GgaoxhR4f3+aV\nsievlJyCUnKLvdW+6EqA+DNXxQaGRKjnCjxG2Kt9gxCllAaBqoYSr499+R725rnZm1/KkSLvJb/X\nuQdsG0b6G/o4Pc9eqaDTIFCV5vEZ9heUsjevlL35pRwo8FzSFn+kXcoO2DYq1/Brg6+UNTQI1Hn5\njOFQoYe9ge6e/QWlVb5jVYLL9r1GP64W3ipQqXCmQaDOkuf2sut0Kdmn3ezNK63yRVmNo+wkx7lI\njnXSLMYRUneOUipUBTUIRGQE8DfADswyxvzxnOX3AZMAL5AP3GOMyQpmTepsPmPYX+Bh12k32afd\nVe7nbxBhp1Wck5ZxTpJjnURpw69UnRO0IBAROzAdGAbkABkiMu+chv5fxpiXA+vfDDwLjAhWTcqv\noNTHrtNu/19eaZWuzo1z2kiOc/ob/1gn8S57ECtVSl0OwdwjuArYaYzZBSAic4FRQFkQGGNOl1s/\nBurMrVbrFBPo688OdPkcLPRU+rVRdiE5zun/i3VRP0L795UKNcEMgmbAvnLTOUCfc1cSkUnAQ4AL\nGFLRG4nIPcA9AC1btqzxQkONMYbjJV4OFnrYk1fKrtPuKg3V0CzGQZt4F23jXTSOsmvDr1SIs/xg\nsTFmOjBdRG4H/g+4q4J1ZgAzANLS0nSvoRxj/LdMPFTo4WDg73Chp0oHeaPsQpt4F23inbSOdxGt\n/fxKhZVgBsF+oEW56eaBeeczF3gpiPXUeeVvrFK+4S++hBE4G0fZaRvvom2Ci6bRDj2HX6kwFswg\nyADai0hr/AFwG3B7+RVEpL0xZkdg8gZgB6pMqc+wL3Dh1sHCUg4VXvrgbBE2oVW8k7bxLtrEu4h1\n6la/UsovaEFgjPGIyAPAAvynj842xmwRkSeBtcaYecADIjIUKAVOUEG3ULgpKPWRfdrNjlNu9uS5\nL3mAtki70CTaQdNoB63jXDSLdehQy0qpCgX1GIEx5hPgk3PmPVbu+ZRgfn5dcObA7o5TbnaecpNT\nUPkzes5w2YTG0XaaRjtpGu2gSbSDejryplKqkiw/WByOzlzEteOUmx2nSjhRUvnNfofgH18/sLXf\nNNp/Ry1t9JVSl0qD4DJxew278/xdPtmn3JW+iXqc00abeCdXxPi39hN1cDalVA3TIAgit9ew/VQJ\nWSdK2JtXSmVP7mkcZaddgov2CRF6Hr9SKug0CGqYMYacAg+ZucVsO+nGXYnz+W0CybFO2ie4aJfg\n0mEblFKXlQZBDTnl9rL5eAmZucWcdF+8zz/SLrSNd9E+wUXreCcRdj2dUyllDQ2Caij1Gb45WUJm\nbgl780svun49l61sq795rFNP51RK1QoaBFVU1vVzvJhtJy7e9RPrsNG1QQRdG0SQFKn9/Uqp2keD\noJLOdP1sPl580dM97QLtE1ykNoikdbxTz/JRStVqGgTnUeoz5OT77837bV4pByoxdHPTaAepDSLo\nXD9Cb9CilKozNAgCztyY/dt8/83ZDxR6qMwAnjEOIaVBJKkNIkiM0v+cSqm6J2xbLq8xHCzwlG3x\nV+XG7Nr1o5QKJWETBD5jOFz4XcO/r6C0ygO6NYl20E27fpRSISbkg8DtNczbk8e+gqrdmxf8p3ue\nuUVjyzinDt2slApJIR8EThscKfJUKgTinTZaxjlJjnXSMs5Jgl7hq5QKAyEfBCJCyzgnm4+XfG9Z\njENIjnOVNfw6dLNSKhyFfBCAfxyfzcdLiLLLWVv8DXX4ZqWUCo8gaJ/g4qed6umVvUopVYGwCIJI\nh41IPctHKaUqpK2jUkqFOQ0CpZQKcxoESikV5jQIlFIqzGkQKKVUmNMgUEqpMCfGVG38HauJyFFg\n7yW+PBE4VoPl1AX6ncODfufwUJ3vnGyMSapoQZ0LguoQkbXGmDSr67ic9DuHB/3O4SFY31m7hpRS\nKsxpECilVJgLtyCYYXUBFtDvHB70O4eHoHznsDpGoJRS6vvCbY9AKaXUOTQIlFIqzIVNEIjICBH5\nRkR2isjDVtcTDCLSQkQWi0iWiGwRkSmB+Q1EZKGI7Ag81re61pokInYR2SAiHwWmW4vI6sBv/baI\nuKyusSaJSD0ReU9EtonIVhHpFwa/8f8E/k1vFpG3RCQy1H5nEZktIkdEZHO5eRX+ruL398B33yQi\nV1bns8MiCETEDkwHrge6AONEpIu1VQWFB/h/xpguQF9gUuB7PgwsMsa0BxYFpkPJFGBruek/AX81\nxrQDTgB3W1JV8PwN+K8xphPQHf93D9nfWESaAQ8CacaYFMAO3Ebo/c5zgBHnzDvf73o90D7wdw/w\nUnU+OCyCALgK2GmM2WWMcQNzgVEW11TjjDEHjTHrA8/z8DcQzfB/19cDq70O3GJNhTVPRJoDNwCz\nAtMCDAHeC6wSat83AbgGeBXAGOM2xpwkhH/jAAcQJSIOIBo4SIj9zsaYZcDxc2af73cdBbxh/FYB\n9USk6aV+drgEQTNgX7npnMC8kCUirYCewGqgsTHmYGDRIaCxRWUFw3PArwBfYLohcNIY4wlMh9pv\n3Ro4CrwW6A6bJSIxhPBvbIzZD0wDvsUfAKeAdYT273zG+X7XGm3TwiUIwoqIxALvA78wxpwuv8z4\nzxcOiXOGReRG4IgxZp3VtVxGDuBK4CVjTE+ggHO6gULpNwYI9IuPwh+CVwAxfL8LJeQF83cNlyDY\nD7QoN908MC/kiIgTfwi8aYz5IDD78JndxsDjEavqq2HpwM0isgd/d98Q/P3n9QJdCBB6v3UOkGOM\nWR2Yfg9/MITqbwwwFNhtjDlqjCkFPsD/24fy73zG+X7XGm3TwiUIMoD2gbMMXPgPNM2zuKYaF+gf\nfxXYaox5ttyiecBdged3Af+53LUFgzHmEWNMc2NMK/y/6RfGmPHAYmBMYLWQ+b4AxphDwD4R6RiY\ndS2QRYj+xgHfAn1FJDrwb/zMdw7Z37mc8/2u84A7A2cP9QVOletCqjpjTFj8ASOB7UA28KjV9QTp\nO/bHv+u4CdgY+BuJv998EbAD+BxoYHWtQfjug4CPAs/bAGuAncC7QITV9dXwd+0BrA38zh8C9UP9\nNwaeALYBm4F/ABGh9jsDb+E/BlKKf8/v7vP9roDgPxMyG8jEf0bVJX+2DjGhlFJhLly6hpRSSp2H\nBoFSSoU5DQKllApzGgRKKRXmNAiUUirMaRAodR4icouIGBHpFJhuVX5kyPO85qLrKFXbaBAodX7j\ngC8Dj0qFLA0CpSoQGK+pP/6Lem6rYPkEEfmPiCwJjBU/tdxiu4jMDIyf/5mIRAVe83MRyRCRr0Xk\nfRGJvjzfRqkL0yBQqmKj8I/5vx3IFZFeFaxzFTAa6AaMFZG0wPz2wHRjTFfgZGAdgA+MMb2NMWfu\nIVDXx89XIUKDQKmKjcM/kB2Bx4q6hxYaY3KNMUX4B0LrH5i/2xizMfB8HdAq8DxFRJaLSCYwHuga\nlMqVqiLHxVdRKryISAP8I5mmiojBf0csg39sl/LOHZ/lzHRJuXleICrwfA5wizHmaxGZgH98JKUs\np3sESn3fGOAfxphkY0wrY0wLYDdnD/sLMCxwT9ko/HeO+uoi7xsHHAwMFT6+xqtW6hJpECj1feOA\nf58z733gkXPmrQnM3wS8b4xZe5H3/S3+O8Z9hX8kTaVqBR19VKlLEOjaSTPGPGB1LUpVl+4RKKVU\nmNM9AqWUCnO6R6CUUmFOg0AppcKcBoFSSoU5DQKllApzGgRKKRXm/j9oq6Ybq60nOgAAAABJRU5E\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e43d6142-b12f-4118-92fa-91593c7c402b",
        "id": "zYgBr_d0gZF0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "neigh = KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='ball_tree', leaf_size=30, p=1, metric='canberra', metric_params=None, n_jobs=None)\n",
        "neigh.fit(train_pca, y_train)\n",
        "\n",
        "neigh.score(test_pca, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9162623344054953"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZvrl1bdQW9I",
        "colab_type": "code",
        "outputId": "1012180f-10f0-49d5-cb4a-82d39a1ddf85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = neigh.predict(test_pca)\n",
        "\n",
        "print(precision_score(y_test, y_pred, average='binary'))\n",
        "print(recall_score(y_test, y_pred))\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7402135231316725\n",
            "0.12447636146020347\n",
            "[[16599    73]\n",
            " [ 1463   208]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP_bDj6eiHvC",
        "colab_type": "code",
        "outputId": "5b67d6af-1df4-400c-bca8-554d02384969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "neigh = KNeighborsClassifier(n_neighbors=100, weights='distance', algorithm='ball_tree', leaf_size=30, p=1, metric='canberra', metric_params=None, n_jobs=-1)\n",
        "neigh.fit(x_train_lda, y_train)\n",
        "\n",
        "neigh.score(x_test_lda, y_test)\n",
        "\n",
        "y_pred = neigh.predict(x_test_lda)\n",
        "\n",
        "print(precision_score(y_test, y_pred, average='binary'))\n",
        "print(recall_score(y_test, y_pred))\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5468599033816425\n",
            "0.33871932974266905\n",
            "[[16203   469]\n",
            " [ 1105   566]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20wwBaAemmmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "abclf = AdaBoostClassifier(n_estimators=350, base_estimator=None, learning_rate=0.6, algorithm='SAMME.R', random_state=None)\n",
        "#scores = cross_val_score(abclf, x_train, y_train, cv=5)\n",
        "#scores.mean()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqz_eh6Qn6BH",
        "colab_type": "code",
        "outputId": "fa36c010-4052-4f53-e273-6ecf9c7bf031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "abclf.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.6,\n",
              "                   n_estimators=350, random_state=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdRVyKPgnHvn",
        "colab_type": "code",
        "outputId": "209276b9-1809-41c5-e478-845744c2b40e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = abclf.predict(x_test)\n",
        "\n",
        "print(precision_score(y_test, y_pred, average='binary'))\n",
        "print(recall_score(y_test, y_pred))\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9149709302325582\n",
            "0.75344105326152\n",
            "[[16555   117]\n",
            " [  412  1259]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVK9BpPqijzp",
        "colab_type": "code",
        "outputId": "df722e85-f5dc-4693-cd76-8745f498a26b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "abclf = AdaBoostClassifier(n_estimators=350, base_estimator=None, learning_rate=0.6, algorithm='SAMME.R', random_state=None)\n",
        "\n",
        "abclf.fit(x_train_lda, y_train)\n",
        "\n",
        "y_pred = abclf.predict(x_test_lda)\n",
        "\n",
        "print(precision_score(y_test, y_pred, average='binary'))\n",
        "print(recall_score(y_test, y_pred))\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.720125786163522\n",
            "0.27408737283064033\n",
            "[[16494   178]\n",
            " [ 1213   458]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Il4r4gVP_lJq",
        "outputId": "ea0ddc95-e02a-4fb9-bf39-26ca90c2e6a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "abclf = AdaBoostClassifier(n_estimators=350, base_estimator=None, learning_rate=0.6, algorithm='SAMME.R', random_state=None)\n",
        "\n",
        "abclf.fit(normalized_x, y_train)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.6,\n",
              "                   n_estimators=350, random_state=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "89c87b2d-e9d0-4dd8-ad70-5a9b0b8af8d2",
        "id": "YsOXNifO_-SJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = abclf.predict(normalized_x_test)\n",
        "\n",
        "print(precision_score(y_test, y_pred, average='binary'))\n",
        "print(recall_score(y_test, y_pred))\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7558226897069872\n",
            "0.6020347097546379\n",
            "[[16347   325]\n",
            " [  665  1006]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c78fbdb3-106f-4f38-f122-69e7647c4818",
        "id": "WAm5b-QHIN6l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "prec = []\n",
        "rec = []\n",
        "\n",
        "for i in range(5, 700, 5):\n",
        "  abclf = AdaBoostClassifier(n_estimators=i, base_estimator=None, \n",
        "                             learning_rate=0.6, algorithm='SAMME.R', random_state=None)\n",
        "  abclf.fit(x_train, y_train)\n",
        "\n",
        "  y_pred = abclf.predict(x_test)\n",
        "\n",
        "  currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "  currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "  print(i)\n",
        "  print(currPrec)\n",
        "  print(currRec)\n",
        "\n",
        "  prec.append(currPrec)\n",
        "  rec.append(currRec)\n",
        "\n",
        "# Data\n",
        "df=pd.DataFrame({'x': range(5, 700, 5), 'precision': prec, 'recall': rec })\n",
        " \n",
        "# multiple line plot\n",
        "plt.plot( 'x', 'precision', data=df, marker='', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4, label=\"Precision\")\n",
        "plt.plot( 'x', 'recall', data=df, marker='', color='olive', linewidth=2, label=\"Recall\")\n",
        "plt.xlabel(\"Alpha\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "0.7164634146341463\n",
            "0.2812687013764213\n",
            "10\n",
            "0.7546948356807511\n",
            "0.38479952124476363\n",
            "15\n",
            "0.8983050847457628\n",
            "0.5709156193895871\n",
            "20\n",
            "0.7998595505617978\n",
            "0.6816277678037104\n",
            "25\n",
            "0.9059233449477352\n",
            "0.6223818073010173\n",
            "30\n",
            "0.9090909090909091\n",
            "0.6702573309395572\n",
            "35\n",
            "0.9077901430842608\n",
            "0.6834230999401556\n",
            "40\n",
            "0.8982266769468004\n",
            "0.6971873129862358\n",
            "45\n",
            "0.9000757002271007\n",
            "0.7115499700777977\n",
            "50\n",
            "0.9030075187969925\n",
            "0.7187312986235787\n",
            "55\n",
            "0.8933038999264165\n",
            "0.7265110712148414\n",
            "60\n",
            "0.8867243867243867\n",
            "0.7354877318970676\n",
            "65\n",
            "0.8892908827785818\n",
            "0.7354877318970676\n",
            "70\n",
            "0.8876889848812095\n",
            "0.7378815080789947\n",
            "75\n",
            "0.8921852387843705\n",
            "0.7378815080789947\n",
            "80\n",
            "0.894134477825465\n",
            "0.7480550568521843\n",
            "85\n",
            "0.8961318051575932\n",
            "0.748653500897666\n",
            "90\n",
            "0.8948497854077253\n",
            "0.748653500897666\n",
            "95\n",
            "0.8924501424501424\n",
            "0.7498503889886295\n",
            "100\n",
            "0.8895224518888097\n",
            "0.7468581687612208\n",
            "105\n",
            "0.8953823953823954\n",
            "0.7426690604428486\n",
            "110\n",
            "0.8895224518888097\n",
            "0.7468581687612208\n",
            "115\n",
            "0.8891257995735607\n",
            "0.748653500897666\n",
            "120\n",
            "0.8968481375358166\n",
            "0.7492519449431478\n",
            "125\n",
            "0.902315484804631\n",
            "0.7462597247157391\n",
            "130\n",
            "0.9034833091436865\n",
            "0.7450628366247756\n",
            "135\n",
            "0.9026870007262164\n",
            "0.743865948533812\n",
            "140\n",
            "0.9010115606936416\n",
            "0.7462597247157391\n",
            "145\n",
            "0.9045553145336226\n",
            "0.748653500897666\n",
            "150\n",
            "0.9025974025974026\n",
            "0.748653500897666\n",
            "155\n",
            "0.9018050541516246\n",
            "0.7474566128067026\n",
            "160\n",
            "0.9002153625269204\n",
            "0.7504488330341114\n",
            "165\n",
            "0.9047272727272727\n",
            "0.7444643925792939\n",
            "170\n",
            "0.9066374908825675\n",
            "0.743865948533812\n",
            "175\n",
            "0.9047272727272727\n",
            "0.7444643925792939\n",
            "180\n",
            "0.9067055393586005\n",
            "0.7444643925792939\n",
            "185\n",
            "0.9053857350800583\n",
            "0.7444643925792939\n",
            "190\n",
            "0.9035532994923858\n",
            "0.7456612806702573\n",
            "195\n",
            "0.9022447501810282\n",
            "0.7456612806702573\n",
            "200\n",
            "0.90625\n",
            "0.7462597247157391\n",
            "205\n",
            "0.9042089985486212\n",
            "0.7456612806702573\n",
            "210\n",
            "0.9048656499636892\n",
            "0.7456612806702573\n",
            "215\n",
            "0.9051412020275162\n",
            "0.7480550568521843\n",
            "220\n",
            "0.9073668854850474\n",
            "0.7444643925792939\n",
            "225\n",
            "0.9045553145336226\n",
            "0.748653500897666\n",
            "230\n",
            "0.9039017341040463\n",
            "0.748653500897666\n",
            "235\n",
            "0.9053468208092486\n",
            "0.7498503889886295\n",
            "240\n",
            "0.9079710144927536\n",
            "0.7498503889886295\n",
            "245\n",
            "0.9072463768115943\n",
            "0.7492519449431478\n",
            "250\n",
            "0.908303249097473\n",
            "0.7528426092160383\n",
            "255\n",
            "0.9096167751265365\n",
            "0.7528426092160383\n",
            "260\n",
            "0.9139314369073669\n",
            "0.7498503889886295\n",
            "265\n",
            "0.9094858797972484\n",
            "0.7516457211250748\n",
            "270\n",
            "0.9114015976761075\n",
            "0.7510472770795931\n",
            "275\n",
            "0.9102099927588704\n",
            "0.7522441651705566\n",
            "280\n",
            "0.9089595375722543\n",
            "0.7528426092160383\n",
            "285\n",
            "0.9090252707581228\n",
            "0.75344105326152\n",
            "290\n",
            "0.9064748201438849\n",
            "0.7540394973070018\n",
            "295\n",
            "0.9089595375722543\n",
            "0.7528426092160383\n",
            "300\n",
            "0.9065420560747663\n",
            "0.7546379413524835\n",
            "305\n",
            "0.9089595375722543\n",
            "0.7528426092160383\n",
            "310\n",
            "0.906025824964132\n",
            "0.755834829443447\n",
            "315\n",
            "0.9084354722422494\n",
            "0.7540394973070018\n",
            "320\n",
            "0.9116582186821144\n",
            "0.75344105326152\n",
            "325\n",
            "0.9077144917087239\n",
            "0.75344105326152\n",
            "330\n",
            "0.9090909090909091\n",
            "0.7540394973070018\n",
            "335\n",
            "0.9084354722422494\n",
            "0.7540394973070018\n",
            "340\n",
            "0.9108049311094997\n",
            "0.7516457211250748\n",
            "345\n",
            "0.9065420560747663\n",
            "0.7546379413524835\n",
            "350\n",
            "0.9083694083694084\n",
            "0.75344105326152\n",
            "355\n",
            "0.9109985528219972\n",
            "0.75344105326152\n",
            "360\n",
            "0.9106628242074928\n",
            "0.7564332734889287\n",
            "365\n",
            "0.9123188405797101\n",
            "0.75344105326152\n",
            "370\n",
            "0.9113832853025937\n",
            "0.7570317175344106\n",
            "375\n",
            "0.9107271418286537\n",
            "0.7570317175344106\n",
            "380\n",
            "0.914616497829233\n",
            "0.7564332734889287\n",
            "385\n",
            "0.9128242074927954\n",
            "0.758228605625374\n",
            "390\n",
            "0.9121037463976945\n",
            "0.7576301615798923\n",
            "395\n",
            "0.9126984126984127\n",
            "0.7570317175344106\n",
            "400\n",
            "0.9081779053084649\n",
            "0.7576301615798923\n",
            "405\n",
            "0.9128242074927954\n",
            "0.758228605625374\n",
            "410\n",
            "0.9134823359769286\n",
            "0.758228605625374\n",
            "415\n",
            "0.9121037463976945\n",
            "0.7576301615798923\n",
            "420\n",
            "0.9094827586206896\n",
            "0.7576301615798923\n",
            "425\n",
            "0.9102011494252874\n",
            "0.758228605625374\n",
            "430\n",
            "0.9125722543352601\n",
            "0.755834829443447\n",
            "435\n",
            "0.9091564527757751\n",
            "0.7546379413524835\n",
            "440\n",
            "0.9086330935251798\n",
            "0.755834829443447\n",
            "445\n",
            "0.9133574007220217\n",
            "0.7570317175344106\n",
            "450\n",
            "0.9092872570194385\n",
            "0.755834829443447\n",
            "455\n",
            "0.9094176851186196\n",
            "0.7570317175344106\n",
            "460\n",
            "0.9086330935251798\n",
            "0.755834829443447\n",
            "465\n",
            "0.9098774333093006\n",
            "0.7552363853979653\n",
            "470\n",
            "0.9093525179856116\n",
            "0.7564332734889287\n",
            "475\n",
            "0.909942363112392\n",
            "0.755834829443447\n",
            "480\n",
            "0.9092872570194385\n",
            "0.755834829443447\n",
            "485\n",
            "0.9096774193548387\n",
            "0.7594254937163375\n",
            "490\n",
            "0.9158200290275762\n",
            "0.7552363853979653\n",
            "495\n",
            "0.912167026637869\n",
            "0.758228605625374\n",
            "500\n",
            "0.9145546705286025\n",
            "0.755834829443447\n",
            "505\n",
            "0.913768115942029\n",
            "0.7546379413524835\n",
            "510\n",
            "0.9133574007220217\n",
            "0.7570317175344106\n",
            "515\n",
            "0.9152787834902245\n",
            "0.7564332734889287\n",
            "520\n",
            "0.9133574007220217\n",
            "0.7570317175344106\n",
            "525\n",
            "0.9152787834902245\n",
            "0.7564332734889287\n",
            "530\n",
            "0.9133574007220217\n",
            "0.7570317175344106\n",
            "535\n",
            "0.9126353790613718\n",
            "0.7564332734889287\n",
            "540\n",
            "0.9145546705286025\n",
            "0.755834829443447\n",
            "545\n",
            "0.9144927536231884\n",
            "0.7552363853979653\n",
            "550\n",
            "0.913768115942029\n",
            "0.7546379413524835\n",
            "555\n",
            "0.911849710982659\n",
            "0.7552363853979653\n",
            "560\n",
            "0.9124457308248914\n",
            "0.7546379413524835\n",
            "565\n",
            "0.9138305575669804\n",
            "0.7552363853979653\n",
            "570\n",
            "0.9131693198263386\n",
            "0.7552363853979653\n",
            "575\n",
            "0.9112554112554112\n",
            "0.755834829443447\n",
            "580\n",
            "0.911976911976912\n",
            "0.7564332734889287\n",
            "585\n",
            "0.9100719424460432\n",
            "0.7570317175344106\n",
            "590\n",
            "0.9115107913669065\n",
            "0.758228605625374\n",
            "595\n",
            "0.9107913669064748\n",
            "0.7576301615798923\n",
            "600\n",
            "0.9114470842332614\n",
            "0.7576301615798923\n",
            "605\n",
            "0.910136592379583\n",
            "0.7576301615798923\n",
            "610\n",
            "0.9094827586206896\n",
            "0.7576301615798923\n",
            "615\n",
            "0.910136592379583\n",
            "0.7576301615798923\n",
            "620\n",
            "0.911976911976912\n",
            "0.7564332734889287\n",
            "625\n",
            "0.9106628242074928\n",
            "0.7564332734889287\n",
            "630\n",
            "0.9094176851186196\n",
            "0.7570317175344106\n",
            "635\n",
            "0.9091564527757751\n",
            "0.7546379413524835\n",
            "640\n",
            "0.9105339105339105\n",
            "0.7552363853979653\n",
            "645\n",
            "0.9113193943763518\n",
            "0.7564332734889287\n",
            "650\n",
            "0.9093525179856116\n",
            "0.7564332734889287\n",
            "655\n",
            "0.9107271418286537\n",
            "0.7570317175344106\n",
            "660\n",
            "0.9108554996405464\n",
            "0.758228605625374\n",
            "665\n",
            "0.9096774193548387\n",
            "0.7594254937163375\n",
            "670\n",
            "0.9087643678160919\n",
            "0.7570317175344106\n",
            "675\n",
            "0.9100719424460432\n",
            "0.7570317175344106\n",
            "680\n",
            "0.9083094555873925\n",
            "0.7588270496708558\n",
            "685\n",
            "0.9107913669064748\n",
            "0.7576301615798923\n",
            "690\n",
            "0.9088952654232425\n",
            "0.758228605625374\n",
            "695\n",
            "0.9102011494252874\n",
            "0.758228605625374\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9ba100e438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxd9Xng/89z93u1Was3yQvBeMEY\nGwTBGEKAkkDakA0KDM02afh1Ekgz6a8NTKaUZJhf25QkJVPPJDSTkIWyBDItyZCyBkgggA3YGNt4\nwau8abO2K939+f1xjq6vZNmSLV1d2ed5v1566Wz3nOdeXZ3nfJfzPaKqGGOM8S5fqQMwxhhTWpYI\njDHG4ywRGGOMx1kiMMYYj7NEYIwxHhcodQAnqq6uTufNm1fqMIwx5pTy+uuvt6tq/UjrTrlEMG/e\nPNauXVvqMIwx5pQiIruPtc6qhowxxuMsERhjjMdZIjDGGI+zRGCMMR5nicAYYzzOEoExxnjcKdd9\n1HhXfzpH2C/4fVLqUCZEIpNjV2+aWNDHrFiAwBjf1+CIwSKl/RwSmRyvtA7QncxyZlWIxdVh4pkc\nB+IZ/CJUhnzEAj7CfqEjkWVXb4pkVmmIBpgRC7A/nqEtkSEW8DG/IkhtxI+IkFPlQH+Gw8ksAvhE\niAWEiqCfqrAP/7D3nc4pA5kc0YCP4Gny3ZhslgjMlNeRyPBMS5ydvWnCPuHiGVFW1EU50J9mIKNU\nh/3URvxjPpEeT06VXb1petM5zqwMURYcudAcT+dQoHzY+kxOaRvIkAMC4pwA9/WnSWaViqCPiqCP\nypCfjkSGlw8NkMw6J/WAQEM0QEXIR0CE/kwOv084qyrE0powGzqTvNWRoCuZpT+jBHww3T2hLpoW\nZnZZABGhN5Xlna4Ue/rSVIZ8nF8XpSbiB6BtIMNz++IcGshwRmWIS2fGqAr5T+pz6kpm+fm7PXQk\nswBs7krxm3399GVyJ7U/gLBPKA/66EvnSOZGHh4/6IM55UGqQn76Mzk6ElnaE1kGtw77hRnRAPMq\nggR9QjzjXDzMKQ9SGfJzqD9DXyZH1C/EAj6SWaXfTSLTo37Kg74hCbYvnWN3b4qORJZY0EdDJEBD\n1E8kMPTvnlOlO5WjJ5VlIKvUhP3Uu597XyZHf1rJofhFmBbyE/If/7uayirJbG5IPJmc0p7IUh70\nHfW9Gy851Z5H0NzcrMW8oaw/k+PtziSdiSyVIR8N0QBzyoOj/uHMseVUEY59BZtT5WB/hlRWKQ/6\nUKA9kaVtIENbIsu7PSmOcV7IE6DGTQiDV4XVYT+Lq0MIwrbuJMmsMqssyOyyADt6UuyLZ6iL+FlS\nE0YVNh1OsqZ1gK6UczIL+uCC+igBn3NCD/qEiF/Y05dmf38GgEXTQlw2q4yeVJYtXSk2HU6SyE7s\n/1QsIPRnjr/P8qCPnOpR2wlwRmWQaMDH5sNJCkMLCMwqC5JTJewXKkN+UlmlLZEhkXGWhfxCIqMM\nZJ0Tapl7AmxLZPNJ7HQS8gllQSEgThI51udeGfRRFXZKIImM0jqQYfimZQFBYcR9lAd8VEd8VIX8\ndCWzdCazhP1CU1mQeCbHzp40OSDqF2aWBehJOUlPgQ80lnFeffSE35uIvK6qzSOus0TgSGWV5/fH\neasjcdQftDzg4+azqqgOn9zVUyl1JbOkc0qdW+weTlXZ3ZtmR2+avX1psqrMLgtSFfKxuzdNZzJL\nRdBJiDNjAeZWOFdWAL3pLK8eGqA9kWVWLMC8yhDgVOGkckoiq+zqTbG7N01WYW55kHPrIkyP+gn5\nhLZEll29aTZ1Jsd1JTleQR9kc1C6CIwZu3Nrw1wzp+KEX3e8RFDUqiERuRq4F/ADP1DVvxu2fi7w\nQ6Ae6AT+RFVbihnTsTzV0sfbnckR1/Vlcvzrzh4+edY0/AJ7+tJsPpwilVMunhGlLjL5NWw5Vd45\nnGKHW2ztS+eIBoTKoJ/6qJ/6aIC3OxPs6EkDzlXl3PIg1WE/lSE/DVE/PhGe3NvHAffqdlDrQHbI\nfHcqR0v8yDbTQj5qI3729KVJu2fPXb1pXj40cNyYd/el2d2XnoB3P7HSJcwAYZ8Q9gs9pQxinBqi\n/iHfmelRP2G/j950lkRGSWaVYL56xsee3jQ9qRzTwj6ayoN0JrPsLfguAUQDQmOZU72TySnxTI7D\nbrXYSGIB58r81P0Ux274/+dEKNoZTET8wGrgKqAFWCMij6vqpoLN7gF+oqo/FpErgL8FPlmsmArl\nVDnUn6Ei5NQL7uo9/gnq0ECWe9Z3UBP205k88ofY3p3is4umTWppIZ7O8fiu3qNOqr1p50uyvefo\n1/RnlM1dqQk5flcql68+mSwVQR+Xzow5JYjDTsKuDDoJqSOZpWcC4/GJU0VwslU8Ub8wLewnndP8\nCa0q5Kc37cTZm86RySmN5UEubIgSdRNBdzJHXzpHRp3XrWtP8G7Pkb/x/IogV84uY1rY7zTK9md4\n53CSbd2pIVU+M2MB5lcE2dqdoj0x9KQRDQjLayOs70iMWt00mqAPLmyIcsmMGF2pHIf6M0yPBY76\nX1DVURu2VZWBjNKbzuEX8g3Hw7dpT2Tdkqtz8i8P+pgeDRAJ+FBVulI5dvWm2O82WMeCQmciy+6+\nNKmsUh8JUBvxM5B1qn0ifiHqF7pTOdoSmaMuCgSYEQvQWBag360C6khkR0w4scCR+v/98Qyp3JH2\nn2lhP36BVE7pSuYYyyfvFxj+FawK+Ypyrila1ZCIrATuUtUPuvN3AKjq3xZssxG4WlX3ivNX71bV\nyuPtdyKqhlSVB7Z10xLPEPIJf/yeSh7c3j3kQ794epS98TR7+zLH3pFrRizAJxdUjak3S1cyy85e\n5x806vfREPUzqyx4VONPVzLLwf4Mcyuc+t1BB/rT/GJHL72n8BXkSEI+oSHqp89thK0J+6mL+KmL\nBqiP+JkeC+R7i/SksuTU+acYPFkksznaE1kOJ7OoOv9wW7tS+WTZVO6coLZ1pxjIKJUhH/Mqguzs\nSec/y9qwn8XVYc6tCxP0CWtaB9gXz1DlthU59b05ygM+3lMVYkdPihcP9Dv7C/qYUxFkQVWIMytD\nE9KzSVXZ0ZNmV2+KpnJn3yOdUJPZHN2pHBG/UBY80qtGVdkXz9CVyjKQUWIB4YzKENGAj0zOaZfJ\nuO03A1mlN5XDJ1AXcUqNyaySyilRv9M2ksw6V+YiQtgnTAv7CPtPnx7oqk51Zl86R1ahLOg0KA/v\npZTNKR3JLP2ZHJmcc+HQEA0M+R/Oug27fp/zXfYV7COrSncyR2cyS0/KafytjwboTmXZH8/gE5hf\nEaI+6udAf4auZJbKkFPSj4zj8y5JG4GIXIdzkv9Td/6TwHtV9daCbf4FeFVV7xWRjwOPAXWq2jFs\nX7cAtwDMmTPn/N27jzmI3pjs6EnxyLtHLpvrI37aCq6cfAJ/eW4tyazyoy1ddI/havPChihXzC47\narmqsqcvzdbuFDt70kNKE4UaywIsqQ6zpDrMpsNJnmmJk8PpBfHReRXMrwzRNpDhgW3dJ3ylGhCO\navcYbkFViLOrwwR8wu7eFAm3m19TeZDedJYD8Qy7+9IciGeGXA1F/cL59VE6k07jbtDnnIzCfiHo\nEyqDPs6sCqHAWx0J9sczxDM5ElmlOuRnRixAU3mABVXhojTIZwavynxHTo5Zda62Brsq7o9niASE\n2vDI7SjHk805VR/RgJS8O6cxx1OyNoIx+H+BfxKRzwAvAvuAo86UqnofcB84JYLxHnRb99AqkrZh\nxeeI3/mnjgSck/BPt3UP6bUicFTR7rXWAaJ+YUFViJZ4Br9ATcTPywf7hxTvj6UlnqElnuHZffEh\nJZNkVnnk3R7eUxVie/fRVTtN5QEunh5jWtjpTnc4maWlL8PB/gyxoHBhQ5Q55cF8n+3eVI6OZJZ9\n8Qx96RxVIR9Xzi7jrGnh/D7PrAoNOcYMnBM1OH222wYytLn1lAunhY7qSncsf9BYPqbtJtLwLqUi\nQqBgkU+ExvLgSe/f7xNi1nfdnOKKmQj2AU0F843usjxV3Q98HEBEyoFPqGpXEWMCnKvB44kWFL9m\nlgX5kwVVvNGeIORzGrzmVjgnjh+90zWkke+FA/28cKB/XLGNdLGvMGISaK6PcMXssnyxszrsZ3ZZ\nkKU1R++jsTw45ISnqiRzSth3YleyQZ8wqyzIrLKTP3kaY6aWYiaCNcACEZmPkwBuBP5D4QYiUgd0\nqmoOuAOnB1HRDa/zGy4SGLr+WCe+a+dV8ND27lGrXQr5gNnlzr0JAxllXzzNoZPoBXBeXYQrZ5ed\ndHWEiFPva4wxRUsEqpoRkVuBJ3G6j/5QVTeKyDeAtar6OPB+4G9FRHGqhr5YrHgKjXb+G+sJsrE8\nyA1nVvHouz3HvBMyv21ZgAsbosytCB7VwNaVzLLxcJJ17Yl8w2VlyMd7G6K8sL8/3/tg0OJpIa5q\nPPkkYIwxhTx5Q9lv9sV5tfXYfd7Prg7z4Xljv2HjYH+Gx3b05E/ic8uDiDjLQz5h5Ywoy2sjo564\nc6rs6U3Tn1XOrAwR8gsDmRxbulLEMzlSWaU24mdpTXhILwRjjBnNVG4sLonhV9jDhU+wymRGLMAt\nS6o52J+hOuw/6XFAfCL5u3MHRQM+ltdFTmp/xhgzFt5MBKN0vxzeRjAWQZ/QNI7eJ8YYUyqnz90g\nJ2C0EsF4btowxphTjSfPeKOWCKw3jTHGQ7yZCEYtEVgiMMZ4hycTQXq0RDDGO2WNMeZ04MkznlUN\nGWPMEZ5MBKPd/BW1RGCM8RDPJQJVJT1q91HPfSzGGA/z3Bkvq8d/JKFf4CRuIzDGmFOW5xLBWHoM\n2Rg+xhgv8V4iGLWh2HMfiTHG4zx31hu1RGD1QsYYj/FeIrCuo8YYM4T3EoGNM2SMMUN47qw3Wong\nRIegNsaYU533EoG1ERhjzBCWCIaxqiFjjNd47qw36l3FVjVkjPEYzyWC0UoEUasaMsZ4jOcSQdJu\nKDPGmCE8d9Yb9VkEVjVkjPGYoiYCEblaRLaIyHYRuX2E9XNE5Dci8qaIvCUiHypmPGBPJzPGmOGK\nlghExA+sBq4BlgA3iciSYZv9V+ARVV0B3Aj8z2LFM2jUO4ttCGpjjMcU86x3IbBdVXeoagp4CPjI\nsG0UqHSnq4D9RYwHOH6JwIagNsZ4UTETwWxgb8F8i7us0F3An4hIC/AEcNtIOxKRW0RkrYisbWtr\nG1dQxysRlAd9NgS1McZzSl0PchNwv6o2Ah8CfioiR8WkqveparOqNtfX14/rgMNLBI1lgfz0+fXR\nce3bGGNORYHRNzlp+4CmgvlGd1mhzwFXA6jq70UkAtQBrcUKangi+Oj8SjqTWUI+YUasmB+HMcZM\nTcUsEawBFojIfBEJ4TQGPz5smz3AlQAishiIAOOr+xnF8KqhsF+YUx60JGCM8ayiJQJVzQC3Ak8C\nm3F6B20UkW+IyLXuZn8BfF5E1gMPAp9R1eN36xmHrCqFeUCwxmFjjCnqZbCqPoHTCFy47M6C6U3A\nqmLGUGj4OEMhnz2f2BhjSt1YPKmSw9oHQnbzmDHGeCsRDC8RBH2WCIwxxlOJYHiPISsRGGOM1xOB\nlQiMMcZjiWCExmJjjPE6byUCqxoyZkKoKkXs6X3aU82VOoQhPHUXlZUIzGTK5TKA4PP5ARgYOExL\nyyuo5hDxEY3WEIlUcfDgenbvfpFMpp9YrIGysgbKyuoJhcrp7+8gHm8lHm9lYKCdbDYNQChURixW\nTzhchYiQzaaIx9tIpXpoalrF4sWfoKxs6HAs2Wyazs7t9PUdoLr6PVRVzSEeP0R7+xYikSrq6hYR\nCETy26vm6O/vIBKpwu8P5fexfv1P+O1v/zuqOT7wgXs466wPs2nTo+zZ81uamlaxaNFHCIcr8/tp\nb3+H3t79xGL1BINR+vvbSSS6iUariUZrSaX66O9vw+cL5t97NFrDCKPNnBDVHAMDh/OfX1fXLtra\nNtLbu49otJZYrA6f7/inQL8/RG3tWdTVLSYYjAFKItFFPN5KefkM6uoWD+mCrqr097e5f6fBbds4\nfPhdWlvfpq1tI62tG4nHW5k//3IWL76OyspGVLMcOPAmu3e/QDodp75+CRUVs+nv7yCR6Mwn3aam\ni7nooi+P63MZiZxqWb25uVnXrl17Uq995VA/z+/vz89f2BDlitllExWamQJUlUTiMPF4G/F4K/39\nbQQCEerrl+D3h9i16wU6OrYwY8YKZsw4l82bf8H69T8hEIgwb977mTWrmbKyBiKRakSEXC5Df387\n8XgbqlkA6uoWM3Pmeajm2Lbt/3LgwJvU1JzpnkjDJBLdrFv3IzZseIBwuJLzzrsFER+vvPIdUqne\nSfkcRPxMn34O9fVno5qltXUj7e3vkMul89v4fMEh8yI+KipmU1bWAChtbZvJZAYACIer8PuDZDIJ\nUqm+IccKBstIp+P5eb8/xPTpy6irW8TBg+tobX37pOKPxeooK6unrKyBWKyeWKyOdDpOPN6Gzxcg\nFqvH5wvQ399GMtkN4P69Oty/fXv+b1YsdXWLmTPnUhKJw/T0tNDWtikfSzGcffYNXHfdQyf1WhF5\nXVWbR1znpUTwbEsfa9oS+flLZ8ZYNSM2UaGNSy6XRWTyRj9VVdraNlJTs4BAIDwpxxxJJpMkm00C\nEAhE8fuDtLa+zfPP38WWLY+jmsXnC7JgwYe44IIvoJpj164XSCZ7KCurRzVHW9tGOjq25U/8zpV4\ncVVXn0Eul6W7e/cJvW727PcSi9WSy2UZGOigv7+d2tqzmDv3/ZSV1RcksFZSqT6i0Tr3KrmBWKyO\nQCCMquavopPJHgD3xOhc4W7d+it27Hh6xM9h2rR5VFTMorNzO/F4K+FwJXV1i0kmu+no2HbUiTMS\nmUYy2TtkeW3tQi677E6SyV6ee+6/MDDQyfTpy1i06GPs2vU8u3e/iDPC/OA+qmloWMrAQAfpdD+x\nWB3hcBWJRBf9/e2EwxXEYvXkcun81Xsi0XVCn+uxRCLTiMWcZFJZOZv6+rOpqpqTP/ZoVTSpVB/t\n7Zvp6NiWT5rhcBWxWB1tbZsYGOg46jXhcBWhUJk7Xekeu4n6+rNpaDibhoalhMNVbNnyb2zf/mvS\naefidNq0M5g//3Ki0Vra2jYRjx8iFnNKR4OlysrKJpqaVp7UZ2GJwPXgtm529x25AvrIvAoWV0/e\nSbCv7xCvv/59enr2MTDQgWoO1RxdXTtpb3+HSKSa88+/hebmP6OiYlb+dV1du9m581l2736R7u7d\nxOOtVFTM4rzzbmHRoo/i9wePOpaqsm/fq2zc+HO6unYCEAxGqa1dRDAY4803/zft7ZuZMWMFN930\nSyorh48QPj7xeCvbtj1BLFbHggUfAoS3336Id975Bblclmw2SXv7Fg4f3sHwk4ZzEjj572U4XDXk\nSjKV6qW1dSOpVB9z5lxCff3Z7N+/hgMH3qCpaSUXXHArgUCYnTt/Q2fnVuLxtvyJyOfz56sR/P4Q\nuVyG3btfoK/vIADV1e9h4cJr6e7eTWfn9nxCnzfv/VxwwReJx1t5/fXvkckkueiiLzNnziXj+FTH\nLpnszVdDiPhoaDib+volhELl+W1SqT6CwbL8xUcmk6Sv7wDxeCu5XJa6ukVEo9Wo5kgkutz3JkSj\ntfnXJJM99PUdoqbmzPyygYHDtLa+TXv7ZiormzjjjCvzVUtjlc2m8iWxweTQ39+erxJzrvydpB+L\n1ROJTHMvpHzEYrXuyb/+hI97YjGm2bnzOTo7txGN1lJePoP6+sWUlU2fkiMWWCLAOTF+d0MnAwXt\nBLcsrqYm4p/I8I4pk0nygx9cyKFDb426rd8f4uKL/5Llyz/LCy98nbfe+ukxt41Ga5kx41z3amMp\nVVVz2LHjWTZteoTu7j3HPY6ID9UcFRWz+IM/+HsqK5vw+fzE421HFW9zuQydne/S1rYxfxWaTvcT\nj7eSTseH1LlmMgPs3782f7U1ffoywuFK9uz53Qgx+PN1r+l0P6o5/P4Q55///7Bq1VcpL59BPN7K\nG2/8M2+99TMikSrmzbuciopZbnVNjvr6JdTVLaKiYiaxWH3RSzi5XJa9e19GNcvcue8bd122MZPB\nEgHQm8qyeuPh/HzQB/95WS2+ScrcTz/9VV5++ZtUV5/BypV/QTRam7+Sr6xspK5uMQcOvMFrr32X\nzZt/MeS1fn+IBQs+xLx5l1NffzaxWB179vyONWv+ifb2d455zIqK2SxZcj1z5qxCxEci0e0WOQ9y\n1lkfZu7c9/Hooze4RfmJ5fMFOeOMKzl0aAO9vc7o47FYPZde+jWqqpoQ8VNTcya1tQvyV225XJZE\n4jB+f5hwuGLCYzLGyywRANu7Uzy6oyc/PysW4FMLp01kaEOo5ti3bw09PS3E44d44olbERE++9nf\njVrHt3fvy/z617dx4MAbnHXWh/ngB79DTc17RjiG0t29m9bWjfkeCYcPv8uMGeexdOkNNDVdPOrV\najab4qWXvsmBA28Qj7eimnUbS6fhjM/qEBGqquZSX392vjdKIBChrKyBYDBGf39HvroLhOnTlxGN\nVpPJJHjzzR8Rjx/ioou+7O7XGDPZLBEALx/s58UDR3oMLa+NcPWc8uO84sT097fz6qvfZWCgk0wm\nwbvvPkVPz94h27zvfX/N5Zd/Y0z7U83R07OPqqqm0Tc2xphRHC8ReOY+gtaBoT0oGqIT1zaQzaZ4\n6KGPsnfvS0OWV1Y2MWtWs3s1PY/3ve+vx7xPEZ8lAWPMpPBQIhjaLW76BD6R7Mkn/4K9e1+iomI2\nq1Z9FREfM2euoLHxImtINMZMeZ5IBKms0pkcmgjqIxPz1tetu581a/4Jvz/EH//xYzQ2vndC9muM\nMZPFE5erbYmh1ULVYd+EjDO0bduv+eUvPw/ANdf8D0sCxphTkicSwdHtA+MvDbS0vMrPf34duVyG\nVau+yvnn3zLufRpjTCl4JBEMax8YZyI4cOANHnjgatLpfpYv/yxXXvm349qfMcaUkkcSwcSVCA4e\nXMdPfvIHJBJdLFr0MT784fum5O3kxhgzVqd9IlBV2oaVCE6262gi0cXPfvZBEonDLFx4Lddd99Co\nw9gaY8xUV9SzmIhcDdwL+IEfqOrfDVv/HeBydzYGNKjqhN56KiJ8cWk1rQNZWgcydCSyVARPLv+9\n+ur/IB5vpbFxJddd90hRB7QyxpjJUrREICJ+YDVwFdACrBGRx1V10+A2qvqfC7a/DVhRjFjCfh9N\n5T6ayo8epXOskskeXnnlOwBceeX/V9Khm40xZiIVs2roQmC7qu5Q1RTwEPCR42x/E/BgEeMZl9de\nW00icZg5cy5l7tzLSh2OMcZMmGImgtlA4WA7Le6yo4jIXGA+8Nwx1t8iImtFZG1bW9uEBzqaVKqP\n3//+WwBcdtmd1jhsjDmtTJXG4huBR/UYz5VT1ftUtVlVm+vr60fapKheeeVeBgY6aGxcyfz5V076\n8Y0xppiKmQj2AYWjpjW6y0ZyI1O0WmhgoJOXX/4HAK644m4rDRhjTjvFTARrgAUiMl9EQjgn+8eH\nbyQii4Bq4PdFjOWkvfTSN0kmu5k//0rmz7+i1OEYY8yEK1oiUNUMcCvwJLAZeERVN4rIN0Tk2oJN\nbwQe0in4YITe3gO8+up3AaenkDHGnI6Keh+Bqj4BPDFs2Z3D5u8qZgyjUVV+9as/o6bmTFat+ssh\n61588W4ymQEWLfoYs2dfWKIIjTGmuKZKY3HJ9PS08MYb9/HCC1+nsFBy+PAO3njjPkC4/PL/VroA\njTGmyDyfCBIJ54H26XQ8Pw3w/PN3kctlOPfcT9LQcHapwjPGmKKzRJDoyk93de0GoLX1bd5662f4\nfEEuu+yuEkVmjDGTw/OJYGDgSCmgu3sPAL///bcB5fzzb6G6en6JIjPGmMnh+URQWCLo7nZKBAcP\nrgPgnHNuLklMxhgzmSwRDEkEe1BVOjq2AlBXt7BUYRljzKSxRJAorBraTV/fAdLpONFoLdFoTQkj\nM8aYyWGJYFhj8WBpoLb2rFKFZIwxk8oSwbCqIUsExhivsURQUDUUjx/i0KG3AKipWVCqkIwxZlJZ\nIigoEQDs3PksYCUCY4x3WCJwE0Ek4jwqub39HcASgTHGOzyfCAZvKJs+fdmQ5TU1Z5YiHGOMmXSe\nTwSDJYLp08/NL6usbCQUKitVSMYYM6k8nQhyuQypVC8gNDQszS+3aiFjjJeMORGIyCUi8ll3ul5E\nTvlBeBKJbsBpH5g2bV5+eU2NJQJjjHeMKRGIyN8AXwXucBcFgZ8VK6jJUthQXFU1N7+8tta6jhpj\nvGOsJYKPAdcCcQBV3Q9UFCuoyTJ4D4GTCJryy61qyBjjJWNNBCn3mcIKICKnRUvqYIkgGq0mGIxR\nXj4DgNpaG2zOGOMdY31m8SMi8n1gmoh8HviPwD8XL6zJMfwegmuu+Sc6O7db1ZAxxlPGlAhU9R4R\nuQroARYCd6rq00WNbBIM3kMQDjuJYMmST5QyHGOMKYlRE4GI+IFnVPVy4JQ/+RcqrBoyxhivGrWN\nQFWzQE5Eqk505yJytYhsEZHtInL7Mbb5YxHZJCIbReRfTvQY4zG8asgYY7xorG0EfcAGEXkat+cQ\ngKp+6VgvcEsSq4GrgBZgjYg8rqqbCrZZgNMldZWqHhaRhpN4DyetsNeQMcZ41VgTwS/cnxNxIbBd\nVXcAiMhDwEeATQXbfB5YraqHAVS19QSPMS5HSgRWNWSM8a6xNhb/WERCwGAH+y2qmh7lZbOBvQXz\nLcB7h21zFoCIvAT4gbtU9d/HEtNEsKohY4wZYyIQkfcDPwZ2AQI0icinVfXFCTj+AuD9QCPwooic\no6pDHhIgIrcAtwDMmTNnnIc8YrBqyBqLjTFeNtYbyr4FfEBVL1PV9wEfBL4zymv2AU0F843uskIt\nwOOqmlbVncBWnMQwhKrep6rNqtpcX18/xpBHZyUCY4wZeyIIquqWwRlV3Yoz3tDxrAEWiMh8t1rp\nRuDxYdv8K05pABGpw6kq2jHGmMbNEoExxoy9sXitiPyAIwPN3QysPd4LVDUjIrcCT+LU//9QVTeK\nyDeAtar6uLvuAyKyCcgCf3P2lGsAABPPSURBVKmqHSfzRk7G4A1l1lhsjPGysSaC/wR8ERjsLvpb\n4H+O9iJVfQJ4YtiyOwumFfiK+zOpMpkE2WwSvz9EIBCZ7MMbY8yUMdZEEADuVdVvQ/4egXDRopoE\nhdVCIlLiaIwxpnTG2kbwLBAtmI8Cz0x8OJPHqoWMMcYx1kQQUdW+wRl3OlackCaHNRQbY4xjrIkg\nLiLnDc6ISDMwUJyQJseRRHDCQygZY8xpZaxtBF8Gfi4i+935mcANxQlpcqTTzpBJodAp/6A1Y4wZ\nl+OWCETkAhGZoaprgEXAw0Aa+Hdg5yTEVzTpdD8AweApXcNljDHjNlrV0PeBlDu9EvgvOCOKHgbu\nK2JcRTeYCAKB6ChbGmPM6W20qiG/qna60zcA96nqY8BjIrKuuKEVVzrtNHFYicAY43WjlQj8IjKY\nLK4EnitYN9b2hSnJqoaMMcYx2sn8QeAFEWnH6SX0WwARORPoLnJsRWWJwBhjHMdNBKr630XkWZxe\nQk+5Q0KAU5K4rdjBFdORRGBtBMYYbxu1ekdVXxlh2dbihDN5MhlrIzDGGBj7DWWnHasaMsYYh+cT\ngXUfNcZ4nWcTgVUNGWOMw7OJwKqGjDHGYYnAEoExxuMsEVj3UWOMx3k4EVgbgTHGgKcTgVUNGWMM\nWCKwRGCM8TzPJwK7j8AY43WeTAS5XIZcLo2ID78/VOpwjDGmpIqaCETkahHZIiLbReT2EdZ/RkTa\nRGSd+/OnxYxnUGFDsYhMxiGNMWbKKtozBUTEj/M0s6uAFmCNiDyuqpuGbfqwqt5arDhGYu0Dxhhz\nRDFLBBcC21V1h6qmgIeAjxTxeGNm7QPGGHNEMRPBbGBvwXyLu2y4T4jIWyLyqIg0jbQjEblFRNaK\nyNq2trZxB2bjDBljzBGlbiz+JTBPVZcBTwM/HmkjVb1PVZtVtbm+vn7cB7WqIWOMOaKYiWAfUHiF\n3+guy1PVDlVNurM/AM4vYjx5lgiMMeaIYiaCNcACEZkvIiHgRuDxwg1EZGbB7LXA5iLGk2fjDBlj\nzBFF6zWkqhkRuRV4EvADP1TVjSLyDWCtqj4OfElErgUyQCfwmWLFU8jGGTLGmCOKlggAVPUJ4Ilh\ny+4smL4DuKOYMYzEqoaMMeaIUjcWl8SR7qOWCIwxxtOJwNoIjDHGo4nA7iMwxpgjPJkIrI3AGGOO\nsERgjDEe5+lEYGMNGWOMRxOBtREYY8wRnkwEVjVkjDFHWCIwxhiP83gisDYCY4zxaCKwNgJjjBnk\n0URgVUPGGDPIEoExxnicpxOB3UdgjDEeTQR2H4ExxhzhyURgVUPGGHOE5xKBao5MJgFAIBApcTTG\nGFN6nksEg11HA4EoIlLiaIwxpvQ8lwisfcAYY4byXCKw9gFjjBnKEoExxnichxOB3UNgjDFQ5EQg\nIleLyBYR2S4itx9nu0+IiIpIczHjARtnyBhjhitaIhARP7AauAZYAtwkIktG2K4C+HPg1WLFUsiq\nhowxZqhilgguBLar6g5VTQEPAR8ZYbv/Bvw9kChiLHmWCIwxZqhiJoLZwN6C+RZ3WZ6InAc0qer/\nPd6OROQWEVkrImvb2trGFZSNM2SMMUOVrLFYRHzAt4G/GG1bVb1PVZtVtbm+vn5cx7X7CIwxZqhi\nJoJ9QFPBfKO7bFAFsBR4XkR2ARcBjxe7wdiqhowxZqhiJoI1wAIRmS8iIeBG4PHBlararap1qjpP\nVecBrwDXquraIsZkVUPGGDNM0RKBqmaAW4Engc3AI6q6UUS+ISLXFuu4o7ESgTHGDBUo5s5V9Qng\niWHL7jzGtu8vZiyDEokuACKRaZNxOGOMmfI8d2fxwEAnANFodYkjMcaYqcFziSCROAxANFpT4kiM\nMWZq8FwiGCwRRCJWIjDGGChyG8FUNDBgJQJjprJ0Ok1LSwuJxKQMNnDaiUQiNDY2EgwGx/waDyYC\nayMwZipraWmhoqKCefPm2VMET5Cq0tHRQUtLC/Pnzx/z6zxVNaSq+TYCqxoyZmpKJBLU1tZaEjgJ\nIkJtbe0Jl6Y8lQjS6X6y2RSBQMSeR2DMFGZJ4OSdzGfnqURgPYaMMeZonkoE1mPIGDMWfr+f5cuX\ns3TpUq6//nr6+/vHvc+1a9fypS996Zjr9+/fz3XXXTfu45wMTzUWW48hY04df/dme1H3f/uKumOu\ni0ajrFu3DoCbb76Z733ve3zlK1/Jr1dVVBWfb+zX0s3NzTQ3H3tMzVmzZvHoo4+OeX8TyZMlAusx\nZIwZq0svvZTt27eza9cuFi5cyKc+9SmWLl3K3r17eeqpp1i5ciXnnXce119/PX19fQCsWbOGiy++\nmHPPPZcLL7yQ3t5enn/+ef7oj/4IgBdeeIHly5ezfPlyVqxYQW9vL7t27WLp0qWA02D+2c9+lnPO\nOYcVK1bwm9/8BoD777+fj3/841x99dUsWLCAv/qrv5qQ9+ipRGBtBMaYE5HJZPj1r3/NOeecA8C2\nbdv4whe+wMaNGykrK+Puu+/mmWee4Y033qC5uZlvf/vbpFIpbrjhBu69917Wr1/PM888QzQ6tHPK\nPffcw+rVq1m3bh2//e1vj1q/evVqRIQNGzbw4IMP8ulPfzrfE2jdunU8/PDDbNiwgYcffpi9e/cy\nXh6rGrI2AmPM6AYGBli+fDnglAg+97nPsX//fubOnctFF10EwCuvvMKmTZtYtWoVAKlUipUrV7Jl\nyxZmzpzJBRdcAEBlZeVR+1+1ahVf+cpXuPnmm/n4xz9OY2PjkPW/+93vuO222wBYtGgRc+fOZevW\nrQBceeWVVFVVAbBkyRJ2795NU1MT4+GxRGAlAmNOFcerwy+2wjaCQmVlZflpVeWqq67iwQcfHLLN\nhg0bRt3/7bffzh/+4R/yxBNPsGrVKp588kkikciYYguHw/lpv99PJpMZ0+uOx1NVQ1YiMMZMlIsu\nuoiXXnqJ7du3AxCPx9m6dSsLFy7kwIEDrFmzBoDe3t6jTtbvvvsu55xzDl/96le54IILeOedd4as\nv/TSS3nggQcA2Lp1K3v27GHhwoVFey+eSgTWRmCMmSj19fXcf//93HTTTSxbtoyVK1fyzjvvEAqF\nePjhh7nttts499xzueqqq4660/cf//EfWbp0KcuWLSMYDHLNNdcMWf+FL3yBXC7HOeecww033MD9\n998/pCQw0URVi7bzYmhubta1a0/uaZY//ekH2LHjaW6++deceebVExyZMWYibN68mcWLF5c6jFPa\nSJ+hiLyuqiP2X7USgTHGeJynEoG1ERhjzNE8mQisRGCMMUd4JhHkclkSiW7AHlxvjDGFPJMIkslu\nQAmHq/D5/KUOxxhjpoyiJgIRuVpEtojIdhG5fYT1fyYiG0RknYj8TkSWFCuWIzeTWfuAMcYUKloi\nEBE/sBq4BlgC3DTCif5fVPUcVV0OfBP4drHisfYBY8xYFQ5D/eEPf5iurq4J3f/999/PrbfeCsBd\nd93FPffcM6H7P1HFLBFcCGxX1R2qmgIeAj5SuIGq9hTMlgFFu6nBHlFpjBmrwSEm3n77bWpqali9\nenWpQyqqYo41NBsoHBavBXjv8I1E5IvAV4AQcMVIOxKRW4BbAObMmXNSwViJwJhTz9e/XpxHVv7N\n34z9mnPlypW89dZb+fl/+Id/4JFHHiGZTPKxj32Mr3/96wD85Cc/4Z577kFEWLZsGT/96U/55S9/\nyd13300qlaK2tpYHHniA6dOnT/j7Ga+SNxar6mpVfQ/wVeC/HmOb+1S1WVWb6+vrT+o4g20EViIw\nxoxVNpvl2Wef5dprrwXgqaeeYtu2bbz22musW7eO119/nRdffJGNGzdy991389xzz7F+/Xruvfde\nAC655BJeeeUV3nzzTW688Ua++c1vlvLtHFMxSwT7gMKxURvdZcfyEPC/ihWMlQiMOfWcyJX7RBoc\nhnrfvn0sXryYq666CnASwVNPPcWKFSsA6OvrY9u2baxfv57rr7+eujpnxNSaGuc809LSwg033MCB\nAwdIpVLMnz+/JO9nNMUsEawBFojIfBEJATcCjxduICILCmb/ENhWrGCODC9hJQJjzPENthHs3r0b\nVc23Eagqd9xxB+vWrWPdunVs376dz33uc8fcz2233catt97Khg0b+P73v3/U4HNTRdESgapmgFuB\nJ4HNwCOqulFEviEi17qb3SoiG0VkHU47waeLFY+VCIwxJyoWi/Hd736Xb33rW2QyGT74wQ/ywx/+\nMP9Iyn379tHa2soVV1zBz3/+czo6OgDo7HTON93d3cyePRuAH//4x6V5E2NQ1AfTqOoTwBPDlt1Z\nMP3nxTx+Ies1ZIw5GStWrGDZsmU8+OCDfPKTn2Tz5s2sXLkSgPLycn72s59x9tln87WvfY3LLrsM\nv9/PihUruP/++7nrrru4/vrrqa6u5oorrmDnzp0lfjcj88ww1I89dhPbt/87N9zwf5g37/0TH5gx\nZkLYMNTjd6LDUHvmUZWf+MSDo29kjDEeVPLuo8YYY0rLEoExZso51aqsp5KT+ewsERhjppRIJEJH\nR4clg5OgqnR0dBCJRE7odZ5pIzDGnBoaGxtpaWmhra2t1KGckiKRCI2NjSf0GksExpgpJRgMTtk7\ncE9XVjVkjDEeZ4nAGGM8zhKBMcZ43Cl3Z7GItAG7T/BldUB7EcIpFou3uE61eOHUi9niLa6TiXeu\nqo44jv8plwhOhoisPdat1VORxVtcp1q8cOrFbPEW10THa1VDxhjjcZYIjDHG47ySCO4rdQAnyOIt\nrlMtXjj1YrZ4i2tC4/VEG4Exxphj80qJwBhjzDFYIjDGGI877ROBiFwtIltEZLuI3F7qeABE5Ici\n0ioibxcsqxGRp0Vkm/u72l0uIvJdN/63ROS8EsTbJCK/EZFN7jOm/3wqxywiERF5TUTWu/F+3V0+\nX0RedeN6WERC7vKwO7/dXT9vMuMtiNsvIm+KyK+merwisktENojIOhFZ6y6bkt8HN4ZpIvKoiLwj\nIptFZOVUjVdEFrqf6+BPj4h8uajxqupp+wP4gXeBM4AQsB5YMgXieh9wHvB2wbJvAre707cDf+9O\nfwj4NSDARcCrJYh3JnCeO10BbAWWTNWY3eOWu9NB4FU3jkeAG93l3wP+kzv9BeB77vSNwMMl+l58\nBfgX4Ffu/JSNF9gF1A1bNiW/D24MPwb+1J0OAdOmcrwFcfuBg8DcYsZbkjc3iR/iSuDJgvk7gDtK\nHZcby7xhiWALMNOdnglscae/D9w00nYljP3fgKtOhZiBGPAG8F6cOzEDw78bwJPASnc64G4nkxxn\nI/AscAXwK/efeirHO1IimJLfB6AK2Dn8M5qq8Q6L8QPAS8WO93SvGpoN7C2Yb3GXTUXTVfWAO30Q\nmO5OT6n34FZDrMC5yp6yMbvVLOuAVuBpnJJhl6pmRogpH6+7vhuoncx4gX8E/grIufO1TO14FXhK\nRF4XkVvcZVP1+zAfaAN+5Fa9/UBEypi68Ra6ERh84HrR4j3dE8EpSZ20PuX69YpIOfAY8GVV7Slc\nN9ViVtWsqi7HudK+EFhU4pCOSUT+CGhV1ddLHcsJuERVzwOuAb4oIu8rXDnFvg8BnKrY/6WqK4A4\nTtVK3hSLFwC3Teha4OfD1010vKd7ItgHNBXMN7rLpqJDIjITwP3d6i6fEu9BRII4SeABVf2Fu3hK\nxwygql3Ab3CqVqaJyODDmApjysfrrq8COiYxzFXAtSKyC3gIp3ro3ikcL6q6z/3dCvwfnGQ7Vb8P\nLUCLqr7qzj+KkximaryDrgHeUNVD7nzR4j3dE8EaYIHb+yKEU8x6vMQxHcvjwKfd6U/j1MMPLv+U\n2zPgIqC7oHg4KUREgP8NbFbVbxesmpIxi0i9iExzp6M47RmbcRLCdceId/B9XAc8515xTQpVvUNV\nG1V1Hs539DlVvXmqxisiZSJSMTiNU4/9NlP0+6CqB4G9IrLQXXQlsGmqxlvgJo5UCw3GVZx4S9EA\nMsmNLR/C6eXyLvC1UsfjxvQgcABI41ytfA6njvdZYBvwDFDjbivAajf+DUBzCeK9BKcY+hawzv35\n0FSNGVgGvOnG+zZwp7v8DOA1YDtOcTvsLo+489vd9WeU8Lvxfo70GpqS8bpxrXd/Ng7+X03V74Mb\nw3Jgrfud+FegeorHW4ZTyqsqWFa0eG2ICWOM8bjTvWrIGGPMKCwRGGOMx1kiMMYYj7NEYIwxHmeJ\nwBhjPM4SgTHHICIfFREVkUXu/DwpGDH2GK8ZdRtjphpLBMYc203A79zfxpy2LBEYMwJ3XKVLcG72\nu3GE9Z8RkX8Tkefd8eH/pmC1X0T+WZxnITzl3t2MiHxeRNaI85yEx0QkNjnvxpjjs0RgzMg+Avy7\nqm4FOkTk/BG2uRD4BM6dzNeLSLO7fAGwWlXPBrrcbQB+oaoXqOq5OENefK6o78CYMbJEYMzIbsIZ\nAA7390jVQ0+raoeqDgC/wClBAOxU1XXu9Os4z54AWCoivxWRDcDNwNlFidyYExQYfRNjvEVEanBG\nAD1HRBTnKVGKM55LoeHjswzOJwuWZYGoO30/8FFVXS8in8EZV8iYkrMSgTFHuw74qarOVdV5qtqE\n84SrpmHbXeU+RzYKfBR4aZT9VgAH3CG9b57wqI05SZYIjDnaTThj7Bd6DOdRp4Vec5e/BTymqmtH\n2e9f4zzZ7SXgnQmI05gJYaOPGnMS3KqdZlW9tdSxGDNeViIwxhiPsxKBMcZ4nJUIjDHG4ywRGGOM\nx1kiMMYYj7NEYIwxHmeJwBhjPO7/B2t09Q2S8hx5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fiBWFhVTwU3",
        "colab_type": "code",
        "outputId": "f9fefb9f-8d62-457f-96b6-115321008c37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "prec = []\n",
        "rec = []\n",
        "\n",
        "for estimators in range(10, 300, 10):\n",
        "  abclf = GradientBoostingClassifier(loss='deviance', learning_rate=0.01, n_estimators=estimators, subsample=0.7, max_depth=20, min_impurity_decrease=0. , verbose=1, max_features='sqrt', max_leaf_nodes=None)\n",
        "  abclf.fit(x_train, y_train)\n",
        "\n",
        "  y_pred = abclf.predict(x_test)\n",
        "\n",
        "  currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "  currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "  print(i)\n",
        "  print(currPrec)\n",
        "  print(currRec)\n",
        "\n",
        "  prec.append(currPrec)\n",
        "  rec.append(currRec)\n",
        "\n",
        "# Data\n",
        "df=pd.DataFrame({'x': range(5, 700, 5), 'precision': prec, 'recall': rec })\n",
        " \n",
        "# multiple line plot\n",
        "plt.plot( 'x', 'precision', data=df, marker='', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4, label=\"Precision\")\n",
        "plt.plot( 'x', 'recall', data=df, marker='', color='olive', linewidth=2, label=\"Recall\")\n",
        "plt.xlabel(\"Alpha\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5681           0.0105            3.84s\n",
            "         2           0.5532           0.0094            3.73s\n",
            "         3           0.5293           0.0111            3.37s\n",
            "         4           0.5189           0.0081            3.08s\n",
            "         5           0.5128           0.0072            2.71s\n",
            "         6           0.5034           0.0072            2.21s\n",
            "         7           0.4889           0.0074            1.70s\n",
            "         8           0.4818           0.0071            1.19s\n",
            "         9           0.4700           0.0072            0.61s\n",
            "        10           0.4623           0.0059            0.00s\n",
            "695\n",
            "0.0\n",
            "0.0\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "         1           0.5673           0.0120            6.94s\n",
            "         2           0.5456           0.0102            7.50s\n",
            "         3           0.5345           0.0092            7.84s\n",
            "         4           0.5245           0.0100            7.71s\n",
            "         5           0.5069           0.0095            7.66s\n",
            "         6           0.4954           0.0075            7.56s\n",
            "         7           0.4812           0.0079            7.27s\n",
            "         8           0.4769           0.0064            7.01s\n",
            "         9           0.4662           0.0070            6.58s\n",
            "        10           0.4589           0.0055            6.15s\n",
            "        20           0.3837           0.0044            0.00s\n",
            "695\n",
            "0.0\n",
            "0.0\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "         1           0.5604           0.0120           10.75s\n",
            "         2           0.5499           0.0108           11.27s\n",
            "         3           0.5278           0.0107           12.09s\n",
            "         4           0.5190           0.0089           12.33s\n",
            "         5           0.5061           0.0076           12.61s\n",
            "         6           0.5008           0.0085           12.67s\n",
            "         7           0.4840           0.0082           12.54s\n",
            "         8           0.4776           0.0066           12.42s\n",
            "         9           0.4732           0.0056           12.45s\n",
            "        10           0.4594           0.0063           12.10s\n",
            "        20           0.3895           0.0038            6.88s\n",
            "        30           0.3340           0.0033            0.00s\n",
            "695\n",
            "0.0\n",
            "0.0\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "         1           0.5650           0.0131           13.44s\n",
            "         2           0.5527           0.0097           15.62s\n",
            "         3           0.5310           0.0102           16.57s\n",
            "         4           0.5218           0.0088           17.66s\n",
            "         5           0.5072           0.0074           18.02s\n",
            "         6           0.4927           0.0084           18.34s\n",
            "         7           0.4860           0.0067           18.55s\n",
            "         8           0.4771           0.0069           18.54s\n",
            "         9           0.4675           0.0064           18.45s\n",
            "        10           0.4596           0.0067           18.34s\n",
            "        20           0.3837           0.0047           13.68s\n",
            "        30           0.3323           0.0030            7.17s\n",
            "        40           0.2948           0.0024            0.00s\n",
            "695\n",
            "0.0\n",
            "0.0\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "         1           0.5709           0.0102           19.09s\n",
            "         2           0.5511           0.0114           20.83s\n",
            "         3           0.5325           0.0109           22.50s\n",
            "         4           0.5233           0.0094           23.22s\n",
            "         5           0.5087           0.0088           23.57s\n",
            "         6           0.4977           0.0076           24.47s\n",
            "         7           0.4864           0.0083           24.46s\n",
            "         8           0.4787           0.0074           24.48s\n",
            "         9           0.4653           0.0064           24.60s\n",
            "        10           0.4578           0.0062           24.57s\n",
            "        20           0.3852           0.0039           20.74s\n",
            "        30           0.3336           0.0030           14.56s\n",
            "        40           0.2897           0.0025            7.46s\n",
            "        50           0.2602           0.0019            0.00s\n",
            "695\n",
            "0.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5654           0.0120           19.67s\n",
            "         2           0.5467           0.0122           20.55s\n",
            "         3           0.5317           0.0110           22.38s\n",
            "         4           0.5228           0.0093           24.10s\n",
            "         5           0.5092           0.0090           25.35s\n",
            "         6           0.4967           0.0083           26.31s\n",
            "         7           0.4860           0.0072           27.46s\n",
            "         8           0.4748           0.0073           27.84s\n",
            "         9           0.4636           0.0065           28.20s\n",
            "        10           0.4566           0.0069           28.27s\n",
            "        20           0.3850           0.0042           26.09s\n",
            "        30           0.3316           0.0030           20.80s\n",
            "        40           0.2928           0.0025           14.54s\n",
            "        50           0.2583           0.0022            7.48s\n",
            "        60           0.2318           0.0019            0.00s\n",
            "695\n",
            "1.0\n",
            "0.0706163973668462\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5627           0.0132           23.04s\n",
            "         2           0.5475           0.0116           26.55s\n",
            "         3           0.5330           0.0097           29.66s\n",
            "         4           0.5196           0.0095           30.59s\n",
            "         5           0.5066           0.0085           31.44s\n",
            "         6           0.4971           0.0076           32.64s\n",
            "         7           0.4878           0.0080           33.73s\n",
            "         8           0.4734           0.0074           34.22s\n",
            "         9           0.4685           0.0063           34.58s\n",
            "        10           0.4566           0.0069           34.65s\n",
            "        20           0.3840           0.0043           33.11s\n",
            "        30           0.3317           0.0032           28.20s\n",
            "        40           0.2910           0.0024           22.04s\n",
            "        50           0.2583           0.0023           15.12s\n",
            "        60           0.2303           0.0017            7.65s\n",
            "        70           0.2066           0.0015            0.00s\n",
            "695\n",
            "0.9900332225913622\n",
            "0.17833632555356074\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5652           0.0111           30.99s\n",
            "         2           0.5500           0.0104           35.31s\n",
            "         3           0.5315           0.0110           36.28s\n",
            "         4           0.5192           0.0074           37.34s\n",
            "         5           0.5050           0.0087           38.76s\n",
            "         6           0.4979           0.0081           39.88s\n",
            "         7           0.4833           0.0071           40.23s\n",
            "         8           0.4788           0.0079           40.41s\n",
            "         9           0.4662           0.0062           40.92s\n",
            "        10           0.4547           0.0073           40.95s\n",
            "        20           0.3845           0.0041           40.08s\n",
            "        30           0.3331           0.0030           35.05s\n",
            "        40           0.2903           0.0022           28.97s\n",
            "        50           0.2587           0.0021           22.29s\n",
            "        60           0.2301           0.0018           15.11s\n",
            "        70           0.2064           0.0016            7.66s\n",
            "        80           0.1848           0.0012            0.00s\n",
            "695\n",
            "0.9973684210526316\n",
            "0.22681029323758228\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5623           0.0135           24.54s\n",
            "         2           0.5482           0.0103           32.48s\n",
            "         3           0.5310           0.0112           35.48s\n",
            "         4           0.5230           0.0103           38.21s\n",
            "         5           0.5066           0.0089           40.70s\n",
            "         6           0.4994           0.0069           42.67s\n",
            "         7           0.4872           0.0074           44.14s\n",
            "         8           0.4744           0.0066           45.11s\n",
            "         9           0.4690           0.0069           45.53s\n",
            "        10           0.4584           0.0060           46.09s\n",
            "        20           0.3862           0.0037           46.72s\n",
            "        30           0.3327           0.0028           42.66s\n",
            "        40           0.2904           0.0024           36.97s\n",
            "        50           0.2595           0.0021           30.25s\n",
            "        60           0.2306           0.0016           23.05s\n",
            "        70           0.2083           0.0014           15.45s\n",
            "        80           0.1864           0.0013            7.79s\n",
            "        90           0.1685           0.0012            0.00s\n",
            "695\n",
            "0.9961759082217974\n",
            "0.3117893476959904\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5642           0.0098           40.43s\n",
            "         2           0.5439           0.0127           42.69s\n",
            "         3           0.5328           0.0107           46.67s\n",
            "         4           0.5165           0.0105           46.97s\n",
            "         5           0.5105           0.0095           47.51s\n",
            "         6           0.4927           0.0079           49.44s\n",
            "         7           0.4841           0.0064           50.60s\n",
            "         8           0.4720           0.0076           51.78s\n",
            "         9           0.4702           0.0068           52.34s\n",
            "        10           0.4542           0.0059           52.91s\n",
            "        20           0.3860           0.0041           53.71s\n",
            "        30           0.3344           0.0029           48.99s\n",
            "        40           0.2913           0.0024           43.91s\n",
            "        50           0.2600           0.0019           37.28s\n",
            "        60           0.2314           0.0017           30.35s\n",
            "        70           0.2081           0.0012           23.15s\n",
            "        80           0.1849           0.0014           15.63s\n",
            "        90           0.1683           0.0009            7.89s\n",
            "       100           0.1519           0.0009            0.00s\n",
            "695\n",
            "0.9939668174962293\n",
            "0.3943746259724716\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5608           0.0134           35.28s\n",
            "         2           0.5445           0.0099           43.16s\n",
            "         3           0.5334           0.0102           46.42s\n",
            "         4           0.5175           0.0093           50.03s\n",
            "         5           0.5114           0.0079           52.93s\n",
            "         6           0.4940           0.0078           54.22s\n",
            "         7           0.4853           0.0084           56.47s\n",
            "         8           0.4732           0.0065           57.98s\n",
            "         9           0.4697           0.0060           59.20s\n",
            "        10           0.4558           0.0057           59.48s\n",
            "        20           0.3813           0.0040            1.00m\n",
            "        30           0.3329           0.0033           56.64s\n",
            "        40           0.2917           0.0026           51.64s\n",
            "        50           0.2584           0.0018           45.18s\n",
            "        60           0.2301           0.0015           38.22s\n",
            "        70           0.2067           0.0014           30.88s\n",
            "        80           0.1852           0.0014           23.43s\n",
            "        90           0.1679           0.0010           15.70s\n",
            "       100           0.1528           0.0009            7.90s\n",
            "695\n",
            "0.9920739762219286\n",
            "0.44943147815679235\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5604           0.0116           41.66s\n",
            "         2           0.5475           0.0094           48.89s\n",
            "         3           0.5340           0.0102           54.43s\n",
            "         4           0.5244           0.0096           57.65s\n",
            "         5           0.5106           0.0093           59.97s\n",
            "         6           0.4940           0.0078            1.06m\n",
            "         7           0.4863           0.0077            1.08m\n",
            "         8           0.4774           0.0076            1.11m\n",
            "         9           0.4649           0.0073            1.13m\n",
            "        10           0.4605           0.0060            1.14m\n",
            "        20           0.3853           0.0041            1.15m\n",
            "        30           0.3311           0.0034            1.08m\n",
            "        40           0.2901           0.0021           59.60s\n",
            "        50           0.2605           0.0020           53.34s\n",
            "        60           0.2301           0.0018           46.01s\n",
            "        70           0.2069           0.0015           38.76s\n",
            "        80           0.1845           0.0011           31.07s\n",
            "        90           0.1676           0.0012           23.52s\n",
            "       100           0.1523           0.0011           15.77s\n",
            "695\n",
            "0.9905325443786982\n",
            "0.5008976660682226\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5628           0.0130           45.82s\n",
            "         2           0.5526           0.0106           51.84s\n",
            "         3           0.5395           0.0091           56.15s\n",
            "         4           0.5252           0.0077            1.02m\n",
            "         5           0.5094           0.0095            1.06m\n",
            "         6           0.4981           0.0076            1.10m\n",
            "         7           0.4860           0.0081            1.12m\n",
            "         8           0.4748           0.0074            1.15m\n",
            "         9           0.4679           0.0067            1.17m\n",
            "        10           0.4586           0.0058            1.19m\n",
            "        20           0.3866           0.0043            1.24m\n",
            "        30           0.3326           0.0031            1.21m\n",
            "        40           0.2938           0.0022            1.13m\n",
            "        50           0.2583           0.0019            1.02m\n",
            "        60           0.2325           0.0017           54.51s\n",
            "        70           0.2072           0.0015           47.17s\n",
            "        80           0.1868           0.0014           39.76s\n",
            "        90           0.1682           0.0012           31.92s\n",
            "       100           0.1514           0.0010           24.04s\n",
            "695\n",
            "0.9918319719953326\n",
            "0.5086774386594853\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5646           0.0107           50.42s\n",
            "         2           0.5519           0.0110           59.59s\n",
            "         3           0.5339           0.0111            1.06m\n",
            "         4           0.5184           0.0093            1.12m\n",
            "         5           0.5105           0.0088            1.16m\n",
            "         6           0.4982           0.0085            1.20m\n",
            "         7           0.4881           0.0082            1.22m\n",
            "         8           0.4745           0.0069            1.23m\n",
            "         9           0.4677           0.0064            1.26m\n",
            "        10           0.4587           0.0059            1.29m\n",
            "        20           0.3827           0.0046            1.38m\n",
            "        30           0.3319           0.0030            1.33m\n",
            "        40           0.2892           0.0025            1.25m\n",
            "        50           0.2581           0.0022            1.14m\n",
            "        60           0.2316           0.0018            1.03m\n",
            "        70           0.2056           0.0014           54.53s\n",
            "        80           0.1863           0.0013           46.90s\n",
            "        90           0.1668           0.0012           39.48s\n",
            "       100           0.1521           0.0009           31.81s\n",
            "695\n",
            "0.9901960784313726\n",
            "0.5439856373429084\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5611           0.0118           56.78s\n",
            "         2           0.5434           0.0105            1.05m\n",
            "         3           0.5322           0.0101            1.16m\n",
            "         4           0.5229           0.0092            1.22m\n",
            "         5           0.5100           0.0074            1.26m\n",
            "         6           0.4992           0.0088            1.29m\n",
            "         7           0.4854           0.0075            1.33m\n",
            "         8           0.4749           0.0067            1.37m\n",
            "         9           0.4666           0.0068            1.39m\n",
            "        10           0.4603           0.0066            1.41m\n",
            "        20           0.3866           0.0043            1.49m\n",
            "        30           0.3309           0.0032            1.45m\n",
            "        40           0.2910           0.0026            1.37m\n",
            "        50           0.2567           0.0020            1.27m\n",
            "        60           0.2303           0.0016            1.16m\n",
            "        70           0.2061           0.0015            1.04m\n",
            "        80           0.1846           0.0012           55.05s\n",
            "        90           0.1668           0.0011           47.64s\n",
            "       100           0.1523           0.0009           40.08s\n",
            "695\n",
            "0.9857433808553971\n",
            "0.5792938360263316\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5607           0.0115           59.79s\n",
            "         2           0.5482           0.0093            1.14m\n",
            "         3           0.5345           0.0110            1.23m\n",
            "         4           0.5241           0.0091            1.26m\n",
            "         5           0.5064           0.0094            1.33m\n",
            "         6           0.4981           0.0071            1.39m\n",
            "         7           0.4845           0.0074            1.41m\n",
            "         8           0.4769           0.0076            1.46m\n",
            "         9           0.4684           0.0062            1.49m\n",
            "        10           0.4611           0.0059            1.51m\n",
            "        20           0.3858           0.0044            1.56m\n",
            "        30           0.3299           0.0034            1.56m\n",
            "        40           0.2906           0.0022            1.49m\n",
            "        50           0.2587           0.0017            1.37m\n",
            "        60           0.2326           0.0017            1.27m\n",
            "        70           0.2082           0.0015            1.16m\n",
            "        80           0.1862           0.0013            1.03m\n",
            "        90           0.1668           0.0011           54.83s\n",
            "       100           0.1519           0.0010           47.32s\n",
            "695\n",
            "0.9898063200815495\n",
            "0.5810891681627768\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5574           0.0121            1.04m\n",
            "         2           0.5468           0.0110            1.18m\n",
            "         3           0.5342           0.0110            1.25m\n",
            "         4           0.5215           0.0086            1.33m\n",
            "         5           0.5099           0.0078            1.42m\n",
            "         6           0.4982           0.0080            1.48m\n",
            "         7           0.4883           0.0078            1.52m\n",
            "         8           0.4747           0.0077            1.55m\n",
            "         9           0.4705           0.0062            1.59m\n",
            "        10           0.4588           0.0058            1.61m\n",
            "        20           0.3841           0.0037            1.71m\n",
            "        30           0.3335           0.0031            1.70m\n",
            "        40           0.2925           0.0027            1.62m\n",
            "        50           0.2587           0.0020            1.53m\n",
            "        60           0.2315           0.0019            1.42m\n",
            "        70           0.2069           0.0012            1.30m\n",
            "        80           0.1857           0.0010            1.18m\n",
            "        90           0.1674           0.0010            1.06m\n",
            "       100           0.1541           0.0010           56.20s\n",
            "695\n",
            "0.9878172588832488\n",
            "0.5822860562537403\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5636           0.0126           58.12s\n",
            "         2           0.5442           0.0122            1.09m\n",
            "         3           0.5366           0.0104            1.15m\n",
            "         4           0.5196           0.0088            1.27m\n",
            "         5           0.5106           0.0083            1.36m\n",
            "         6           0.4991           0.0087            1.40m\n",
            "         7           0.4821           0.0074            1.45m\n",
            "         8           0.4763           0.0072            1.50m\n",
            "         9           0.4662           0.0070            1.55m\n",
            "        10           0.4601           0.0063            1.57m\n",
            "        20           0.3814           0.0039            1.71m\n",
            "        30           0.3312           0.0032            1.71m\n",
            "        40           0.2923           0.0024            1.66m\n",
            "        50           0.2573           0.0021            1.58m\n",
            "        60           0.2299           0.0017            1.49m\n",
            "        70           0.2039           0.0015            1.38m\n",
            "        80           0.1857           0.0011            1.27m\n",
            "        90           0.1667           0.0011            1.15m\n",
            "       100           0.1513           0.0009            1.03m\n",
            "695\n",
            "0.9837008628954937\n",
            "0.6140035906642729\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5587           0.0133            1.09m\n",
            "         2           0.5521           0.0096            1.28m\n",
            "         3           0.5322           0.0086            1.50m\n",
            "         4           0.5201           0.0096            1.58m\n",
            "         5           0.5098           0.0083            1.65m\n",
            "         6           0.4972           0.0081            1.72m\n",
            "         7           0.4894           0.0074            1.76m\n",
            "         8           0.4719           0.0077            1.78m\n",
            "         9           0.4633           0.0074            1.80m\n",
            "        10           0.4599           0.0057            1.84m\n",
            "        20           0.3828           0.0040            1.91m\n",
            "        30           0.3343           0.0031            1.89m\n",
            "        40           0.2917           0.0026            1.84m\n",
            "        50           0.2589           0.0020            1.75m\n",
            "        60           0.2304           0.0018            1.65m\n",
            "        70           0.2063           0.0014            1.54m\n",
            "        80           0.1860           0.0014            1.44m\n",
            "        90           0.1662           0.0008            1.31m\n",
            "       100           0.1516           0.0009            1.19m\n",
            "695\n",
            "0.9856733524355301\n",
            "0.6175942549371634\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5628           0.0135           53.99s\n",
            "         2           0.5458           0.0118            1.14m\n",
            "         3           0.5312           0.0094            1.29m\n",
            "         4           0.5189           0.0093            1.40m\n",
            "         5           0.5078           0.0095            1.49m\n",
            "         6           0.4980           0.0070            1.53m\n",
            "         7           0.4870           0.0070            1.63m\n",
            "         8           0.4795           0.0079            1.68m\n",
            "         9           0.4651           0.0064            1.73m\n",
            "        10           0.4557           0.0056            1.79m\n",
            "        20           0.3883           0.0039            1.98m\n",
            "        30           0.3325           0.0028            1.99m\n",
            "        40           0.2902           0.0027            1.95m\n",
            "        50           0.2582           0.0022            1.85m\n",
            "        60           0.2302           0.0015            1.76m\n",
            "        70           0.2064           0.0014            1.66m\n",
            "        80           0.1860           0.0013            1.56m\n",
            "        90           0.1674           0.0010            1.44m\n",
            "       100           0.1518           0.0009            1.32m\n",
            "       200           0.0613           0.0003            0.00s\n",
            "695\n",
            "0.9848771266540642\n",
            "0.6235786953919809\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5589           0.0125            1.24m\n",
            "         2           0.5479           0.0106            1.37m\n",
            "         3           0.5322           0.0086            1.51m\n",
            "         4           0.5211           0.0097            1.61m\n",
            "         5           0.5047           0.0090            1.73m\n",
            "         6           0.5003           0.0072            1.81m\n",
            "         7           0.4896           0.0078            1.86m\n",
            "         8           0.4768           0.0066            1.90m\n",
            "         9           0.4676           0.0062            1.94m\n",
            "        10           0.4575           0.0060            2.01m\n",
            "        20           0.3815           0.0039            2.10m\n",
            "        30           0.3295           0.0035            2.10m\n",
            "        40           0.2917           0.0022            2.05m\n",
            "        50           0.2577           0.0020            2.00m\n",
            "        60           0.2297           0.0015            1.89m\n",
            "        70           0.2066           0.0016            1.78m\n",
            "        80           0.1843           0.0014            1.67m\n",
            "        90           0.1677           0.0012            1.56m\n",
            "       100           0.1510           0.0010            1.44m\n",
            "       200           0.0607           0.0003            8.07s\n",
            "695\n",
            "0.9829222011385199\n",
            "0.6199880311190904\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5652           0.0122            1.16m\n",
            "         2           0.5458           0.0104            1.42m\n",
            "         3           0.5338           0.0107            1.56m\n",
            "         4           0.5243           0.0103            1.66m\n",
            "         5           0.5047           0.0096            1.73m\n",
            "         6           0.4957           0.0080            1.78m\n",
            "         7           0.4851           0.0074            1.85m\n",
            "         8           0.4759           0.0071            1.90m\n",
            "         9           0.4700           0.0057            1.96m\n",
            "        10           0.4584           0.0055            1.99m\n",
            "        20           0.3858           0.0044            2.22m\n",
            "        30           0.3339           0.0028            2.24m\n",
            "        40           0.2918           0.0027            2.20m\n",
            "        50           0.2591           0.0023            2.11m\n",
            "        60           0.2321           0.0016            2.02m\n",
            "        70           0.2077           0.0015            1.92m\n",
            "        80           0.1863           0.0012            1.80m\n",
            "        90           0.1684           0.0009            1.69m\n",
            "       100           0.1534           0.0011            1.57m\n",
            "       200           0.0611           0.0003           16.31s\n",
            "695\n",
            "0.978644382544104\n",
            "0.6307600239377619\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5706           0.0124            1.26m\n",
            "         2           0.5498           0.0103            1.48m\n",
            "         3           0.5386           0.0093            1.67m\n",
            "         4           0.5206           0.0088            1.78m\n",
            "         5           0.5059           0.0095            1.89m\n",
            "         6           0.4984           0.0079            1.98m\n",
            "         7           0.4877           0.0078            2.04m\n",
            "         8           0.4714           0.0077            2.09m\n",
            "         9           0.4663           0.0076            2.14m\n",
            "        10           0.4562           0.0067            2.17m\n",
            "        20           0.3845           0.0038            2.39m\n",
            "        30           0.3328           0.0030            2.43m\n",
            "        40           0.2910           0.0023            2.38m\n",
            "        50           0.2584           0.0022            2.29m\n",
            "        60           0.2293           0.0018            2.21m\n",
            "        70           0.2069           0.0015            2.10m\n",
            "        80           0.1856           0.0011            1.98m\n",
            "        90           0.1688           0.0011            1.88m\n",
            "       100           0.1513           0.0009            1.76m\n",
            "       200           0.0612           0.0003           24.77s\n",
            "695\n",
            "0.9735159817351599\n",
            "0.6379413524835428\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5615           0.0114            1.61m\n",
            "         2           0.5513           0.0110            1.64m\n",
            "         3           0.5352           0.0093            1.83m\n",
            "         4           0.5197           0.0090            1.95m\n",
            "         5           0.5104           0.0077            2.06m\n",
            "         6           0.5015           0.0074            2.14m\n",
            "         7           0.4892           0.0083            2.25m\n",
            "         8           0.4740           0.0075            2.31m\n",
            "         9           0.4595           0.0079            2.33m\n",
            "        10           0.4590           0.0069            2.36m\n",
            "        20           0.3811           0.0045            2.52m\n",
            "        30           0.3308           0.0034            2.54m\n",
            "        40           0.2909           0.0023            2.48m\n",
            "        50           0.2593           0.0019            2.40m\n",
            "        60           0.2273           0.0016            2.31m\n",
            "        70           0.2063           0.0016            2.21m\n",
            "        80           0.1858           0.0012            2.09m\n",
            "        90           0.1669           0.0010            1.98m\n",
            "       100           0.1509           0.0010            1.84m\n",
            "       200           0.0611           0.0003           32.49s\n",
            "695\n",
            "0.9797047970479705\n",
            "0.6355475763016158\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5612           0.0126            1.46m\n",
            "         2           0.5482           0.0110            1.65m\n",
            "         3           0.5361           0.0099            1.86m\n",
            "         4           0.5248           0.0099            1.97m\n",
            "         5           0.5053           0.0078            2.09m\n",
            "         6           0.4965           0.0074            2.23m\n",
            "         7           0.4848           0.0068            2.30m\n",
            "         8           0.4738           0.0076            2.34m\n",
            "         9           0.4729           0.0069            2.41m\n",
            "        10           0.4548           0.0071            2.45m\n",
            "        20           0.3850           0.0039            2.63m\n",
            "        30           0.3284           0.0027            2.64m\n",
            "        40           0.2916           0.0025            2.60m\n",
            "        50           0.2578           0.0019            2.53m\n",
            "        60           0.2311           0.0016            2.44m\n",
            "        70           0.2067           0.0015            2.34m\n",
            "        80           0.1862           0.0010            2.23m\n",
            "        90           0.1678           0.0012            2.12m\n",
            "       100           0.1526           0.0009            2.00m\n",
            "       200           0.0612           0.0004           40.95s\n",
            "695\n",
            "0.9746606334841629\n",
            "0.644524236983842\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5642           0.0131            1.38m\n",
            "         2           0.5489           0.0113            1.49m\n",
            "         3           0.5346           0.0096            1.73m\n",
            "         4           0.5237           0.0091            1.85m\n",
            "         5           0.5060           0.0082            1.98m\n",
            "         6           0.4955           0.0073            2.14m\n",
            "         7           0.4880           0.0079            2.21m\n",
            "         8           0.4761           0.0067            2.29m\n",
            "         9           0.4657           0.0071            2.36m\n",
            "        10           0.4576           0.0062            2.40m\n",
            "        20           0.3883           0.0037            2.69m\n",
            "        30           0.3300           0.0034            2.74m\n",
            "        40           0.2899           0.0024            2.74m\n",
            "        50           0.2588           0.0021            2.65m\n",
            "        60           0.2306           0.0016            2.55m\n",
            "        70           0.2056           0.0014            2.46m\n",
            "        80           0.1849           0.0014            2.35m\n",
            "        90           0.1671           0.0010            2.23m\n",
            "       100           0.1503           0.0009            2.11m\n",
            "       200           0.0614           0.0003           48.87s\n",
            "695\n",
            "0.9772313296903461\n",
            "0.642130460801915\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5737           0.0094            1.79m\n",
            "         2           0.5543           0.0110            2.01m\n",
            "         3           0.5338           0.0110            2.14m\n",
            "         4           0.5217           0.0093            2.21m\n",
            "         5           0.5055           0.0098            2.28m\n",
            "         6           0.4986           0.0075            2.35m\n",
            "         7           0.4923           0.0067            2.46m\n",
            "         8           0.4774           0.0075            2.51m\n",
            "         9           0.4663           0.0071            2.58m\n",
            "        10           0.4586           0.0057            2.64m\n",
            "        20           0.3851           0.0048            2.85m\n",
            "        30           0.3317           0.0029            2.86m\n",
            "        40           0.2924           0.0024            2.86m\n",
            "        50           0.2578           0.0018            2.79m\n",
            "        60           0.2290           0.0018            2.68m\n",
            "        70           0.2064           0.0015            2.58m\n",
            "        80           0.1845           0.0013            2.48m\n",
            "        90           0.1688           0.0010            2.36m\n",
            "       100           0.1521           0.0010            2.25m\n",
            "       200           0.0615           0.0003           56.79s\n",
            "695\n",
            "0.9740840035746202\n",
            "0.6523040095751047\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5633           0.0117            1.80m\n",
            "         2           0.5478           0.0101            2.09m\n",
            "         3           0.5277           0.0102            2.25m\n",
            "         4           0.5253           0.0096            2.37m\n",
            "         5           0.5092           0.0083            2.52m\n",
            "         6           0.4925           0.0080            2.60m\n",
            "         7           0.4845           0.0064            2.68m\n",
            "         8           0.4789           0.0064            2.76m\n",
            "         9           0.4668           0.0069            2.77m\n",
            "        10           0.4610           0.0068            2.83m\n",
            "        20           0.3864           0.0040            2.99m\n",
            "        30           0.3346           0.0031            3.02m\n",
            "        40           0.2919           0.0022            2.98m\n",
            "        50           0.2573           0.0020            2.91m\n",
            "        60           0.2307           0.0016            2.80m\n",
            "        70           0.2075           0.0014            2.70m\n",
            "        80           0.1850           0.0013            2.60m\n",
            "        90           0.1678           0.0010            2.48m\n",
            "       100           0.1522           0.0010            2.37m\n",
            "       200           0.0615           0.0003            1.09m\n",
            "695\n",
            "0.9750889679715302\n",
            "0.6558946738479952\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5654           0.0101            1.80m\n",
            "         2           0.5457           0.0111            2.11m\n",
            "         3           0.5394           0.0103            2.22m\n",
            "         4           0.5262           0.0086            2.38m\n",
            "         5           0.5108           0.0087            2.46m\n",
            "         6           0.4994           0.0088            2.53m\n",
            "         7           0.4872           0.0082            2.58m\n",
            "         8           0.4742           0.0063            2.66m\n",
            "         9           0.4701           0.0057            2.70m\n",
            "        10           0.4610           0.0064            2.77m\n",
            "        20           0.3891           0.0040            3.02m\n",
            "        30           0.3321           0.0036            3.10m\n",
            "        40           0.2944           0.0026            3.07m\n",
            "        50           0.2593           0.0022            3.01m\n",
            "        60           0.2296           0.0017            2.92m\n",
            "        70           0.2069           0.0014            2.82m\n",
            "        80           0.1865           0.0012            2.73m\n",
            "        90           0.1672           0.0011            2.62m\n",
            "       100           0.1531           0.0010            2.51m\n",
            "       200           0.0610           0.0003            1.22m\n",
            "695\n",
            "0.971756398940865\n",
            "0.6588868940754039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-47ab29770d0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'precision'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recall'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# multiple line plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    409\u001b[0m             )\n\u001b[1;32m    410\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         ]\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrays must all be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPzL-xy0NJxL",
        "colab_type": "code",
        "outputId": "459e31d5-6e20-4bf0-c157-bdf8b5c32c57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "# Data\n",
        "df=pd.DataFrame({'x': range(10, 300, 10), 'precision': prec, 'recall': rec })\n",
        " \n",
        "# multiple line plot\n",
        "plt.plot( 'x', 'precision', data=df, marker='', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4, label=\"Precision\")\n",
        "plt.plot( 'x', 'recall', data=df, marker='', color='olive', linewidth=2, label=\"Recall\")\n",
        "plt.xlabel(\"estimators\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9ba0e17e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEGCAYAAACO8lkDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEo4Xl8NlTF0",
        "colab_type": "code",
        "outputId": "85929262-3fa0-4594-e774-3677fc95995d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "abclf = GradientBoostingClassifier(loss='deviance', learning_rate=0.01, n_estimators=200, subsample=0.7, max_depth=20, min_impurity_decrease=0. , verbose=1, max_features='sqrt', max_leaf_nodes=None)\n",
        "abclf.fit(x_train, y_train)\n",
        "\n",
        "y_pred = abclf.predict(x_test)\n",
        "\n",
        "currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "print(currPrec)\n",
        "print(currRec)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5637           0.0130            1.22m\n",
            "         2           0.5460           0.0105            1.38m\n",
            "         3           0.5363           0.0098            1.48m\n",
            "         4           0.5187           0.0084            1.62m\n",
            "         5           0.5068           0.0093            1.70m\n",
            "         6           0.4979           0.0073            1.79m\n",
            "         7           0.4898           0.0074            1.84m\n",
            "         8           0.4789           0.0076            1.92m\n",
            "         9           0.4671           0.0068            1.94m\n",
            "        10           0.4570           0.0055            1.96m\n",
            "        20           0.3864           0.0039            2.10m\n",
            "        30           0.3323           0.0030            2.09m\n",
            "        40           0.2939           0.0026            2.04m\n",
            "        50           0.2586           0.0022            1.94m\n",
            "        60           0.2306           0.0017            1.84m\n",
            "        70           0.2065           0.0012            1.72m\n",
            "        80           0.1844           0.0011            1.61m\n",
            "        90           0.1680           0.0010            1.48m\n",
            "       100           0.1526           0.0009            1.35m\n",
            "       200           0.0612           0.0003            0.00s\n",
            "0.9849340866290018\n",
            "0.6259724715739078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN-_Grw2shMG",
        "colab_type": "code",
        "outputId": "0b222e46-5756-45b4-fb8b-957939ae0455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "abclf = GradientBoostingClassifier(loss='deviance', learning_rate=0.032, n_estimators=300, subsample=0.5, max_depth=100, min_impurity_decrease=0.1 , verbose=1, max_features='sqrt', max_leaf_nodes=None, ccp_alpha=0)\n",
        "abclf.fit(x_train, y_train)\n",
        "\n",
        "y_pred = abclf.predict(x_test)\n",
        "\n",
        "currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "print(currPrec)\n",
        "print(currRec)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5201           0.0348            1.27m\n",
            "         2           0.4884           0.0328            1.11m\n",
            "         3           0.4600           0.0241            1.06m\n",
            "         4           0.4369           0.0172            1.11m\n",
            "         5           0.4134           0.0137            1.16m\n",
            "         6           0.3984           0.0127            1.18m\n",
            "         7           0.3823           0.0118            1.17m\n",
            "         8           0.3635           0.0129            1.15m\n",
            "         9           0.3514           0.0096            1.14m\n",
            "        10           0.3366           0.0109            1.12m\n",
            "        20           0.2452           0.0054            1.02m\n",
            "        30           0.1868           0.0030           57.79s\n",
            "        40           0.1490           0.0021           53.08s\n",
            "        50           0.1226           0.0019           48.28s\n",
            "        60           0.1052           0.0011           43.87s\n",
            "        70           0.0925           0.0010           39.56s\n",
            "        80           0.0832           0.0004           35.54s\n",
            "        90           0.0748           0.0005           32.17s\n",
            "       100           0.0701           0.0003           29.01s\n",
            "       200           0.0556           0.0000            9.80s\n",
            "       300           0.0532          -0.0000            0.00s\n",
            "0.9025454545454545\n",
            "0.7426690604428486\n",
            "[[16538   134]\n",
            " [  430  1241]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "495eb66d-ffaa-4840-c6f7-2e4126585442",
        "id": "6ve4usKkjr5z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "abclf = GradientBoostingClassifier(loss='deviance', learning_rate=0.032, n_estimators=300, subsample=0.5, max_depth=100, min_impurity_decrease=0.1 , verbose=1, max_features='sqrt', max_leaf_nodes=None, ccp_alpha=0)\n",
        "abclf.fit(x_train_lda, y_train)\n",
        "\n",
        "y_pred = abclf.predict(x_test_lda)\n",
        "\n",
        "currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "print(currPrec)\n",
        "print(currRec)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5414           0.0190            1.13m\n",
            "         2           0.5093           0.0139            1.09m\n",
            "         3           0.4864           0.0121            1.06m\n",
            "         4           0.4684           0.0098            1.05m\n",
            "         5           0.4587           0.0085            1.02m\n",
            "         6           0.4408           0.0069            1.02m\n",
            "         7           0.4320           0.0060            1.01m\n",
            "         8           0.4223           0.0054            1.01m\n",
            "         9           0.4109           0.0049           59.58s\n",
            "        10           0.4008           0.0046           58.90s\n",
            "        20           0.3365           0.0019           53.27s\n",
            "        30           0.3035           0.0012           47.40s\n",
            "        40           0.2823           0.0007           40.86s\n",
            "        50           0.2662           0.0004           35.42s\n",
            "        60           0.2567           0.0002           31.06s\n",
            "        70           0.2487           0.0002           27.80s\n",
            "        80           0.2515          -0.0000           25.07s\n",
            "        90           0.2447           0.0000           22.77s\n",
            "       100           0.2457          -0.0000           20.82s\n",
            "       200           0.2407          -0.0000            8.05s\n",
            "       300           0.2356          -0.0001            0.00s\n",
            "0.5439367311072056\n",
            "0.37043686415320165\n",
            "[[16153   519]\n",
            " [ 1052   619]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdL3bngusBrl",
        "colab_type": "code",
        "outputId": "46ce8e4b-b4af-43f8-96ea-7f1b71fbf67f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "prec = []\n",
        "rec = []\n",
        "\n",
        "valor = 0.001\n",
        "lista = []\n",
        "while valor < 10:\n",
        "  lista.append(valor)\n",
        "  valor += valor\n",
        "\n",
        "for rate in lista:\n",
        "  abclf = GradientBoostingClassifier(loss='deviance', learning_rate=rate, \n",
        "                                     n_estimators=300, subsample=0.5, max_depth=100, \n",
        "                                     min_impurity_decrease=0.1 , verbose=1, \n",
        "                                     max_features='sqrt', max_leaf_nodes=None, ccp_alpha=0)\n",
        "  abclf.fit(x_train, y_train)\n",
        "\n",
        "  y_pred = abclf.predict(x_test)\n",
        "\n",
        "  currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "  currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "  print(rate)\n",
        "  print(currPrec)\n",
        "  print(currRec)\n",
        "\n",
        "  prec.append(currPrec)\n",
        "  rec.append(currRec)\n",
        "\n",
        "# Data\n",
        "df=pd.DataFrame({'x': lista, 'precision': prec, 'recall': rec })\n",
        " \n",
        "# multiple line plot\n",
        "plt.plot( 'x', 'precision', data=df, marker='', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4, label=\"Precision\")\n",
        "plt.plot( 'x', 'recall', data=df, marker='', color='olive', linewidth=2, label=\"Recall\")\n",
        "plt.xlabel(\"Alpha\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5813           0.0012            1.14m\n",
            "         2           0.5737           0.0011            1.22m\n",
            "         3           0.5839           0.0012            1.20m\n",
            "         4           0.5731           0.0012            1.16m\n",
            "         5           0.5744           0.0011            1.16m\n",
            "         6           0.5715           0.0012            1.15m\n",
            "         7           0.5762           0.0010            1.16m\n",
            "         8           0.5708           0.0012            1.15m\n",
            "         9           0.5658           0.0010            1.15m\n",
            "        10           0.5685           0.0010            1.16m\n",
            "        20           0.5476           0.0010            1.11m\n",
            "        30           0.5481           0.0010            1.06m\n",
            "        40           0.5294           0.0008            1.01m\n",
            "        50           0.5155           0.0009           58.27s\n",
            "        60           0.5037           0.0008           55.71s\n",
            "        70           0.4912           0.0007           53.15s\n",
            "        80           0.4890           0.0008           50.57s\n",
            "        90           0.4803           0.0007           48.14s\n",
            "       100           0.4700           0.0005           45.40s\n",
            "       200           0.4007           0.0004           22.36s\n",
            "       300           0.3578           0.0003            0.00s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.001\n",
            "0.0\n",
            "0.0\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5778           0.0021            1.30m\n",
            "         2           0.5733           0.0021            1.32m\n",
            "         3           0.5744           0.0023            1.27m\n",
            "         4           0.5609           0.0020            1.29m\n",
            "         5           0.5690           0.0024            1.24m\n",
            "         6           0.5629           0.0023            1.21m\n",
            "         7           0.5611           0.0024            1.18m\n",
            "         8           0.5592           0.0023            1.14m\n",
            "         9           0.5541           0.0020            1.14m\n",
            "        10           0.5497           0.0022            1.12m\n",
            "        20           0.5311           0.0015            1.11m\n",
            "        30           0.5163           0.0017            1.05m\n",
            "        40           0.4866           0.0014            1.01m\n",
            "        50           0.4726           0.0011           58.32s\n",
            "        60           0.4595           0.0012           55.18s\n",
            "        70           0.4432           0.0010           52.20s\n",
            "        80           0.4262           0.0011           49.56s\n",
            "        90           0.4133           0.0008           47.06s\n",
            "       100           0.4055           0.0009           44.55s\n",
            "       200           0.3169           0.0004           21.88s\n",
            "       300           0.2582           0.0004            0.00s\n",
            "0.002\n",
            "1.0\n",
            "0.05685218432076601\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5740           0.0052           58.15s\n",
            "         2           0.5676           0.0050            1.01m\n",
            "         3           0.5616           0.0049            1.01m\n",
            "         4           0.5581           0.0037            1.10m\n",
            "         5           0.5511           0.0044            1.09m\n",
            "         6           0.5513           0.0044            1.08m\n",
            "         7           0.5430           0.0041            1.07m\n",
            "         8           0.5416           0.0033            1.09m\n",
            "         9           0.5288           0.0037            1.11m\n",
            "        10           0.5194           0.0038            1.10m\n",
            "        20           0.4846           0.0029            1.07m\n",
            "        30           0.4528           0.0022            1.03m\n",
            "        40           0.4306           0.0018           59.98s\n",
            "        50           0.4010           0.0019           56.98s\n",
            "        60           0.3799           0.0012           54.20s\n",
            "        70           0.3601           0.0013           51.86s\n",
            "        80           0.3487           0.0011           49.38s\n",
            "        90           0.3298           0.0010           46.82s\n",
            "       100           0.3160           0.0008           44.28s\n",
            "       200           0.2180           0.0004           20.92s\n",
            "       300           0.1584           0.0003            0.00s\n",
            "0.004\n",
            "0.9904306220095693\n",
            "0.4955116696588869\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5789           0.0087            1.17m\n",
            "         2           0.5483           0.0076            1.24m\n",
            "         3           0.5446           0.0089            1.18m\n",
            "         4           0.5348           0.0073            1.21m\n",
            "         5           0.5257           0.0070            1.20m\n",
            "         6           0.5163           0.0063            1.18m\n",
            "         7           0.5083           0.0065            1.17m\n",
            "         8           0.4949           0.0057            1.16m\n",
            "         9           0.4931           0.0059            1.15m\n",
            "        10           0.4863           0.0057            1.14m\n",
            "        20           0.4263           0.0035            1.07m\n",
            "        30           0.3819           0.0027            1.02m\n",
            "        40           0.3467           0.0025           57.56s\n",
            "        50           0.3151           0.0018           55.00s\n",
            "        60           0.2894           0.0015           51.95s\n",
            "        70           0.2686           0.0013           48.96s\n",
            "        80           0.2473           0.0012           46.44s\n",
            "        90           0.2300           0.0013           43.97s\n",
            "       100           0.2129           0.0010           41.25s\n",
            "       200           0.1243           0.0006           18.50s\n",
            "       300           0.0883           0.0001            0.00s\n",
            "0.008\n",
            "0.9558198810535259\n",
            "0.6732495511669659\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5539           0.0200            1.04m\n",
            "         2           0.5312           0.0153            1.11m\n",
            "         3           0.5132           0.0110            1.21m\n",
            "         4           0.4964           0.0149            1.15m\n",
            "         5           0.4829           0.0116            1.15m\n",
            "         6           0.4711           0.0082            1.21m\n",
            "         7           0.4518           0.0114            1.16m\n",
            "         8           0.4430           0.0092            1.15m\n",
            "         9           0.4313           0.0097            1.11m\n",
            "        10           0.4197           0.0090            1.07m\n",
            "        20           0.3351           0.0049            1.04m\n",
            "        30           0.2826           0.0028           59.48s\n",
            "        40           0.2429           0.0028           56.04s\n",
            "        50           0.2133           0.0020           52.93s\n",
            "        60           0.1886           0.0013           50.03s\n",
            "        70           0.1670           0.0011           46.70s\n",
            "        80           0.1495           0.0013           43.82s\n",
            "        90           0.1338           0.0009           40.64s\n",
            "       100           0.1254           0.0008           37.82s\n",
            "       200           0.0713           0.0004           14.29s\n",
            "       300           0.0589           0.0000            0.00s\n",
            "0.016\n",
            "0.9232488822652757\n",
            "0.7414721723518851\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5196           0.0360            1.20m\n",
            "         2           0.4899           0.0302            1.14m\n",
            "         3           0.4599           0.0243            1.09m\n",
            "         4           0.4369           0.0181            1.10m\n",
            "         5           0.4150           0.0180            1.07m\n",
            "         6           0.3919           0.0139            1.07m\n",
            "         7           0.3804           0.0135            1.06m\n",
            "         8           0.3649           0.0112            1.06m\n",
            "         9           0.3492           0.0103            1.07m\n",
            "        10           0.3325           0.0095            1.07m\n",
            "        20           0.2425           0.0050            1.03m\n",
            "        30           0.1886           0.0030           56.54s\n",
            "        40           0.1495           0.0019           51.92s\n",
            "        50           0.1257           0.0019           47.58s\n",
            "        60           0.1091           0.0008           43.06s\n",
            "        70           0.0952           0.0008           38.83s\n",
            "        80           0.0835           0.0004           35.25s\n",
            "        90           0.0775           0.0003           31.91s\n",
            "       100           0.0743           0.0001           28.88s\n",
            "       200           0.0568           0.0000            9.74s\n",
            "       300           0.0544           0.0000            0.00s\n",
            "0.032\n",
            "0.9120639534883721\n",
            "0.7510472770795931\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.4616           0.0702            1.24m\n",
            "         2           0.4215           0.0361            1.19m\n",
            "         3           0.3844           0.0285            1.17m\n",
            "         4           0.3601           0.0208            1.21m\n",
            "         5           0.3297           0.0211            1.16m\n",
            "         6           0.3112           0.0159            1.17m\n",
            "         7           0.2886           0.0134            1.17m\n",
            "         8           0.2717           0.0115            1.16m\n",
            "         9           0.2562           0.0129            1.13m\n",
            "        10           0.2409           0.0092            1.13m\n",
            "        20           0.1503           0.0039           58.79s\n",
            "        30           0.1069           0.0024           50.29s\n",
            "        40           0.0825           0.0007           42.95s\n",
            "        50           0.0723           0.0004           37.09s\n",
            "        60           0.0659           0.0007           32.29s\n",
            "        70           0.0618           0.0001           28.39s\n",
            "        80           0.0572           0.0000           25.30s\n",
            "        90           0.0566           0.0000           22.50s\n",
            "       100           0.0564          -0.0000           19.93s\n",
            "       200           0.0505           0.0001            6.53s\n",
            "       300           0.0496           0.0000            0.00s\n",
            "0.064\n",
            "0.9078171091445427\n",
            "0.7366846199880311\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.3650           0.1193            1.38m\n",
            "         2           0.3314           0.0445            1.20m\n",
            "         3           0.2875           0.0315            1.13m\n",
            "         4           0.2534           0.0215            1.14m\n",
            "         5           0.2279           0.0191            1.13m\n",
            "         6           0.2052           0.0172            1.10m\n",
            "         7           0.1837           0.0113            1.07m\n",
            "         8           0.1724           0.0078            1.05m\n",
            "         9           0.1588           0.0088            1.04m\n",
            "        10           0.1458           0.0089            1.02m\n",
            "        20           0.0839           0.0014           46.29s\n",
            "        30           0.0671           0.0004           36.65s\n",
            "        40           0.0587           0.0004           30.22s\n",
            "        50           0.0550           0.0006           25.50s\n",
            "        60           0.0537           0.0002           21.84s\n",
            "        70           0.0533          -0.0000           18.98s\n",
            "        80           0.0511          -0.0000           16.83s\n",
            "        90           0.0514           0.0005           14.99s\n",
            "       100           0.0497          -0.0000           13.38s\n",
            "       200           0.0446          -0.0000            4.83s\n",
            "       300           0.0438           0.0002            0.00s\n",
            "0.128\n",
            "0.8565601631543168\n",
            "0.7540394973070018\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.2122           0.2248           55.07s\n",
            "         2           0.1849           0.0181            1.04m\n",
            "         3           0.1639           0.0037           59.64s\n",
            "         4           0.1389     -172625.3603           59.49s\n",
            "         5      172625.4830           0.0007           56.41s\n",
            "         6       40394.2200           0.0013           52.88s\n",
            "         7      159160.7612           0.0015           49.27s\n",
            "         8      132231.3562  -153305365.6796           46.13s\n",
            "         9   153437597.0271       -4513.9621           44.26s\n",
            "        10      135616.8238       -5251.2486           42.57s\n",
            "        20      147727.2795          -0.0002           29.50s\n",
            "        30  2029607888.7759           0.0000           23.08s\n",
            "        40  2029761010.9857           0.0000           18.79s\n",
            "        50   153444437.5177          -0.0000           16.30s\n",
            "        60  2029742680.0661           0.0000           14.22s\n",
            "        70  2029615234.0226          -0.0000           12.84s\n",
            "        80  1876469489.7005          -0.0000           11.60s\n",
            "        90  2029748354.3884          -0.0000           10.53s\n",
            "       100   153509536.6271           0.0012            9.61s\n",
            "       200  1876466470.5863           0.0000            3.83s\n",
            "       300  3906179062.0400          -0.0048            0.00s\n",
            "0.256\n",
            "0.7923920051579626\n",
            "0.7354877318970676\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.1437          -0.0004            1.30m\n",
            "         2           0.1029          -0.1952            1.36m\n",
            "         3           0.1185     -811980.9539            1.29m\n",
            "         4      493003.2475     -943236.0082            1.24m\n",
            "         5      727986.8908      -18162.1965            1.06m\n",
            "         6      286693.4863 -209590385893.1895           58.70s\n",
            "         7    39855384.2536 -251435422704.6778           54.31s\n",
            "         8 209556966806.9861           0.0008           48.61s\n",
            "         9 335236329200.2048 -126695559846.7549           46.26s\n",
            "        10 84826427767.4087           0.0001           42.59s\n",
            "        20 91645044360.7438           0.0001           28.46s\n",
            "        30 385004547910.3751          -0.0001           23.58s\n",
            "        40 378199792802.2689          -0.0000           21.02s\n",
            "        50 461074481983.3181          -0.0005           19.08s\n",
            "        60 391151153641.9291           0.0000           17.15s\n",
            "        70 471206595287.3148           0.0000           15.47s\n",
            "        80 464896455482.0703          -0.0002           14.14s\n",
            "        90 471218564563.3311          -0.0062           13.17s\n",
            "       100 479116971919.8514           0.0000           12.34s\n",
            "       200 2585096846827708126846729060352.0000           0.0000            5.59s\n",
            "       300 239982153260011068369621008278211889882844072079211548232558785567377419903433172688532866344715733017687887426020035491185657226686282296459264.0000 -110917964253765149465728115761542550092720241047387710027917715741565353114995944605660267013450485260385755404142064153806880774278076640002048.0000            0.00s\n",
            "0.512\n",
            "0.30534612748457846\n",
            "0.533213644524237\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.0812          -0.2803            1.10m\n",
            "         2           0.1900        -115.1363           59.28s\n",
            "         3          52.4039 -24326998004.6474           58.70s\n",
            "         4 12342964466.5650 -1038948752.3133           52.11s\n",
            "         5 11036796353.2379 -12128524833.7249           53.28s\n",
            "         6 17876432614.2621 -2507384452.4288           56.22s\n",
            "         7 21846922103.2044 -320029140548.8246           56.40s\n",
            "         8 71266296187.9864 -1444518621.5711           55.22s\n",
            "         9 23421673784.5955     -104552.2291           50.82s\n",
            "        10 92791622504.7499   -36619799.3049           50.81s\n",
            "        20 409072631433055998836736.0000 -136357543809969066868736.0000           52.65s\n",
            "        30 1870205322744069208881197651685253853568907548453869782619776776588539141847625131549884440020791728219900619501076480.0000           0.0000            1.20m\n",
            "        40 564278371877632773998587278731817811413119758980148232217087330669047349111899306148836095983819261768800391991179522910608651026061038977024.0000           0.0000            1.40m\n",
            "        50 284019964906883812007940831568316198824230359918273665513048593382378532309267783039914589072056444708221612432586795387343894096231777959936.0000           0.0000            1.50m\n",
            "        60 519108294844572117013694465360762934116688107850817089200901772549394551046762316416106213774628578116613802161095369288989525376628007370752.0000 -34410998744541124397013578978200219107683355782595009443124924901300721505243362691901127672241519748905142724748084706997857943552.0000            1.55m\n",
            "        70 381911128117688697022130646636994065926756462804165306310004355774387593487223382572144614104852784523663865894359806177479842480663419682816.0000           0.0000            1.57m\n",
            "        80 334708140476188105802213671800089736605633966023641725607650642479711997440070212317430100762554210067491291103304191764790584118475734646784.0000           0.0000            1.55m\n",
            "        90 346422196385462100456183185178601179984972445345446104118766120277469432189482181869712781944848388126425715593786287027508582953494289317888.0000           0.0000            1.52m\n",
            "       100 189752309832170131198107297391121919235702581348190197139217727627686531882258517322144164061657185995956553868657476103067906402325490565120.0000           0.0000            1.47m\n",
            "       200 248226815865530873543513780816678503286050338528248816981441405108588685710750348255717129766450110899185392538442358214162544317379244982272.0000           0.0000           48.44s\n",
            "       300 526742503114037917581454515606372779740554061950612963710596153054320022176367519827860248921936132836209397733140144401138557493014322741248.0000           0.0000            0.00s\n",
            "1.024\n",
            "0.38154269972451793\n",
            "0.49730700179533216\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.0723          -0.9920            1.16m\n",
            "         2           0.2585   -42381786.6702            1.25m\n",
            "         3    21219330.5980 -20917967587.1988            1.21m\n",
            "         4  2386779552.6426 -38503547922.2884            1.22m\n",
            "         5 18707634265.5841 -1547405546058.8613            1.23m\n",
            "         6 302218958942.1547 -9039539686517.7988            1.28m\n",
            "         7 5674506070094.0244 -1622231322079.2197            1.30m\n",
            "         8 125465028941735.0000 -120929854114834.2500            1.34m\n",
            "         9 1888905881361326088098826048169020541389165993078263022790088490041922281729560327914856050486449910384164864.0000 -1349218486686661230758556492163652772916927592941211830262108817932961548380851288292662237783222973085253632.0000            1.38m\n",
            "        10 1619062184023993529058388784759227135972522734809535119232121459367521874342217049423355959281287245997277184.0000           0.0000            1.43m\n",
            "        20 3676592007321052599876886322643670966759647477879677046473330455481097415532545196987232971340263996985072351108125489955878082000647040417136640.0000 -337749986119632941304684945427205598250406145899829720287816799632989479989034479644991442305169658566355612972052414698774658061172736.0000            1.97m\n",
            "        30 5615007477278882786767429036909902853156008899081339180952849549278318628915157370647004391248735086012197918562588036928123352934970883588489216.0000 -313800735757496830547122700581255370077019922961740855354502639383373987341516275262278932920320963140333822063982191358238569585258031742976.0000            2.22m\n",
            "        40 4558026518045049413236820722004614630528457061871936514442408012963487197273141402598889783952970633017737384024933777143946410227866435758063616.0000           0.0000            2.30m\n",
            "        50 8277301330498758695056947670339890079395816496745217381729034520884844024450171928850650887055416299533108537600466886622328671491049617025400832.0000           0.0000            2.33m\n",
            "        60 19156552485880350834549903795734253205793398570968350755147032754647049421691495638037073919357914807073838203071666515719104644170118492319645696.0000           0.0000            2.30m\n",
            "        70 12930316142581574003254172276776411425411124385926967162853009380500834857391614396059938835640992665372610358522607288935312249204499804397240320.0000           0.0000            2.26m\n",
            "        80 14699570810671683596324708772127912635813751323978741936936775102130027820144093140589585986330578078124510987928930188886437351449333939623165952.0000           0.0000            2.19m\n",
            "        90 10085825029387304341456204265051637409860901633757604521530282044675930023970853421854327347523748939378989686578446039017776846415836824310120448.0000           0.0000            2.11m\n",
            "       100 19158061073718907793330395252839029604040161037117586770717725426641663534174392035821905393211279965104395805697843052044197417351757368305647616.0000           0.0000            2.03m\n",
            "       200 15012790646315600063535470354647338224850086107332468919708198224141303043224457669674576905561277374952459467391651152115640069170907363703521280.0000           0.0000            1.06m\n",
            "       300 25813689986084839575071945171635799919642068723002948686208011455153669128066696231307535960777331383327317328510850956281337927075152711540277248.0000           0.0000            0.00s\n",
            "2.048\n",
            "0.2537974683544304\n",
            "0.4799521244763615\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.0757          -2.6868            1.20m\n",
            "         2           5.6256         -65.0712            1.25m\n",
            "         3         715.0497 -26002869715294.0156            1.36m\n",
            "         4 13834315494094.9531 -1211500095105.1133            1.35m\n",
            "         5 15499708963329.4434 -4549627557209.0156            1.37m\n",
            "         6 161783742402734038593958028011432416865846525480177165318881756598741075284086639395078144.0000 -80891871201367019296979014005716208432923262740088582659440878299370537642043319697539072.0000            1.42m\n",
            "         7 121337806802050536012856780122111630982574897081806937299096904951531638949489784716787712.0000           0.0000            1.46m\n",
            "         8 991562961322792880720628865915221944903021593369983256997280527673969784040691821491053548028030211222754491099961412355121062231456557876379648.0000 -1031173743826278233575654517512852502371986613138749326764446087650704584852363642840249174530924432602310976623179165559466742651828790060122112.0000            1.58m\n",
            "         9 1022132181146425346758504751158888959249523813877902522401477186616465447636267435119925602935367897087387872918394238672445565524737263312830464.0000           0.0000            1.76m\n",
            "        10 1011368341557895206215134490044774157373455544698749704824040225998660230074220945311650752280014694063772879117216451531031289616571900297740288.0000 2599377838238263930854763297634140971971659758164828039222324606826207476067774021599976005722630854466304260276315848589663272960.0000            1.89m\n",
            "        20 989854663435205892797083888652155390101894842296948693165413460647700274096249802960178738115737916245699474607371809882601550392704872040366080.0000           0.0000            2.51m\n",
            "        30 1010228199937947062677256491152325097612025128660083856880295323040661606575051312183182151647201906415732706248872882194159296651404271719284736.0000           0.0000            2.65m\n",
            "        40 208708220667248538921122063264342930111825704857544415293382895289297284109242333412535497361620717712070853510053945235871085449423160950602596352.0000 -181723802197501848541090841694467482340962958849840068586118199056290230129587315616312129508197112130927466964612580181987797621817146115757178880.0000            2.66m\n",
            "        50 1357824121070011540274505712337138547098620707391820042004512503720211610048749403160831378319947006343089631869048284338365431827886063128714149888.0000           0.0000            2.65m\n",
            "        60 1300791782319004822419028855618094362277183235644183064865234433014633313060281942728317539643167840777427490854367040816808947292772485700561928192.0000           0.0000            2.60m\n",
            "        70 1152876964901881272174924381838002870291016181663004562319429325319387609549883791495291136656015928651283234775562507245784980831424132227633512448.0000           0.0000            2.53m\n",
            "        80 1207360011522941387300400157203347552751793666716368982659032893678900852135239882152605244466246260713030016139973500992522869382242321896893841408.0000           0.0000            2.44m\n",
            "        90 1552379445534463145906456350551521599284705732597558275295929418062773888987230548951824697043031479957601257819612393236372633460358328931095412736.0000           0.0000            2.34m\n",
            "       100 1414888049914119802083610530875272358961442502689867427052565573824266104317670050499435375650267438980199369330338632853391902664277123468282036224.0000           0.0000            2.24m\n",
            "       200 2178586198077388248575322435372091522801147788393118267794904946459878091160723917684020083285876334682818735566689865143837688569423767902097506304.0000           0.0000            1.15m\n",
            "       300 2073816065755172433193499888196313277440182221039990814091312596848715572079858012005197734653352412201669320804450407315146542435379586693580455936.0000           0.0000            0.00s\n",
            "4.096\n",
            "0.15869218500797447\n",
            "0.476361460203471\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.1114          -5.1841            1.06m\n",
            "         2         620.5302       -7870.9248            1.06m\n",
            "         3 1928296985507018428263694336.0000 -1945826958102546050992570368.0000            1.42m\n",
            "         4 6117254975054464185662305206272.0000 -46057470945951262133766718488576.0000            1.77m\n",
            "         5 583521213647163901306860457923969024.0000 -269304353764619447884453780643643392.0000            2.00m\n",
            "         6 9044890689239101910507600576060642557952.0000 -7591166272039643703607086398202168999936.0000            2.22m\n",
            "         7 4596289032637392583427217077781714392055808.0000 -2293892461071864873264671238503776886194176.0000            2.36m\n",
            "         8 6889820134250034814492830932475832138989568.0000           0.0000            2.54m\n",
            "         9 818750835375194728175009831582977353180531684016128.0000 -10557380090532007385730039035952323726093594918912.0000            2.68m\n",
            "        10 829308215468569985397504956880036691391542911303680.0000 -805935001264597749746525348404391776895369216.0000            2.79m\n",
            "        20 5872647628231234187273780683908334753084793077864801603972704353529496667684864.0000 -52426676932751036523968419079389228516826695218340221648051798542385152.0000            3.21m\n",
            "        30 7175168902781856424624986402164447225946424646727414770970014591978563817337353819484467987307009526464512.0000 -233789277946816687070203576522065608609386243535559642040596130795188679216655465882428622503936.0000            3.21m\n",
            "        40 9049068795169783370902757000364540747364594106803115656537836813567530443466489548111114955135274484407400624720289294450688.0000 7174863705889049020156249339389545217413703245607460027150474216568542537733203633773207344990259223986176.0000            3.13m\n",
            "        50 2034540820283159250675264390600724445637278043399739940373158338173609823277582835579631537790928660602109062627391514369739430400612106240.0000 -1104019732364196272437761534405199140912098355854392060376892585824341288546551965122820155537603197184597109644407773186474559033911017472.0000            3.04m\n",
            "        60 556490721349744753272740924621374621832818127171857784095523476222511255680356092157348772804879450804274334596255216327967763012399833633020444672.0000 -292811664785200422386581839696288154818589748356968635564083734782219943680644021797008753120796205940384335181274370167999376500824473600.0000            2.93m\n",
            "        70 479332421777527566316854307825746369096057721281646941828043891516463151937976940335709858795682590730971624237822026667804103790369273489574068224.0000           0.0000            2.82m\n",
            "        80 536907421320261469691830021234300117892779322470015652718401362216205996248793356613733810031518362451885116689848409945520632423286098101499068416.0000 -4859183758010209356477353235567019188819050473881848607582021591205085282026669255735558509797943395832926799426291700347888781623824482304.0000            2.70m\n",
            "        90 1380133638891403955787369771896542335316662841493909982374116460178768374425281657303976646511968640330046298411656394327781587682402252032204341248.0000 -863425029263607502937722520802356574655844144357855360217056645190802128577928142399026402544420890536429728377704939761394126377672297461648982016.0000            2.58m\n",
            "       100 1613276361758319262260877797896785281965271949875725459260393060841223047645782628386457108189124126796146418308113825990747983694179160670108385280.0000 -9197829484972965043349807780498666329492975213826599888675593798082250076792863145312701772574328107401136001021032567676055111900856310470541312.0000            2.46m\n",
            "       200 1272029597450211311097958953715423036569506055659896411458003572213513151142415508815491603826232446082259196380700477655066123390506862296229216256.0000           0.0000            1.24m\n",
            "       300 1373247811773936478427816641456580631315631148733709132101688108203609050240026094023038540913485601174482348099757273938501203968510705563747745792.0000           0.0000            0.00s\n",
            "8.192\n",
            "0.24775928297055058\n",
            "0.4631956912028725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9ba0d57400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrXCdM-20Cd-",
        "colab_type": "code",
        "outputId": "6a4323c6-5934-47d0-cbc4-aa6fcef6f5a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "abclf = GradientBoostingClassifier(loss='deviance', learning_rate=0.016, \n",
        "                                     n_estimators=500, subsample=0.4, max_depth=100, \n",
        "                                     min_impurity_decrease=0.1 , verbose=1, \n",
        "                                     max_features=\"sqrt\", max_leaf_nodes=None, ccp_alpha=0, \n",
        "                                   n_iter_no_change=10, validation_fraction=0.5)\n",
        "abclf.fit(x_train, y_train)\n",
        "\n",
        "y_pred = abclf.predict(x_test)\n",
        "\n",
        "currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "print(currPrec)\n",
        "print(currRec)\n",
        "print(abclf.score(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5412           0.0153           33.36s\n",
            "         2           0.5316           0.0168           33.07s\n",
            "         3           0.5174           0.0133           34.08s\n",
            "         4           0.4990           0.0126           33.64s\n",
            "         5           0.5018           0.0095           33.21s\n",
            "         6           0.4728           0.0119           32.40s\n",
            "         7           0.4672           0.0076           33.03s\n",
            "         8           0.4538           0.0074           33.78s\n",
            "         9           0.4414           0.0084           33.69s\n",
            "        10           0.4336           0.0064           33.75s\n",
            "        20           0.3582           0.0050           31.63s\n",
            "        30           0.3010           0.0041           29.75s\n",
            "        40           0.2568           0.0026           28.85s\n",
            "        50           0.2228           0.0017           27.43s\n",
            "        60           0.2007           0.0019           26.07s\n",
            "        70           0.1791           0.0014           24.99s\n",
            "        80           0.1649           0.0007           24.09s\n",
            "        90           0.1500           0.0010           23.16s\n",
            "       100           0.1339           0.0009           22.17s\n",
            "       200           0.0778           0.0001           13.70s\n",
            "       300           0.0662           0.0000            7.52s\n",
            "0.9230769230769231\n",
            "0.7324955116696589\n",
            "0.9700703265550892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "22d5f4f1-3b50-41e0-f76e-46542f280c4f",
        "id": "Z9dygJxOcxbR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "prec = []\n",
        "rec = []\n",
        "\n",
        "lista = np.linspace(0.1 ,1,10)\n",
        "\n",
        "for subS in lista:\n",
        "  abclf = GradientBoostingClassifier(loss='deviance', learning_rate=0.032, n_estimators=300, subsample=subS, max_depth=100, min_impurity_decrease=0.1 , verbose=1, max_features='sqrt', max_leaf_nodes=None, ccp_alpha=0)\n",
        "  abclf.fit(x_train, y_train)\n",
        "\n",
        "  y_pred = abclf.predict(x_test)\n",
        "\n",
        "  currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "  currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "  print(subS)\n",
        "  print(currPrec)\n",
        "  print(currRec)\n",
        "\n",
        "  prec.append(currPrec)\n",
        "  rec.append(currRec)\n",
        "\n",
        "# Data\n",
        "df=pd.DataFrame({'x': lista, 'precision': prec, 'recall': rec })\n",
        " \n",
        "# multiple line plot\n",
        "plt.plot( 'x', 'precision', data=df, marker='', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4, label=\"Precision\")\n",
        "plt.plot( 'x', 'recall', data=df, marker='', color='olive', linewidth=2, label=\"Recall\")\n",
        "plt.xlabel(\"Alpha\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5061           0.0297           23.12s\n",
            "         2           0.5053           0.0266           20.47s\n",
            "         3           0.4808           0.0223           19.88s\n",
            "         4           0.4542           0.0139           20.52s\n",
            "         5           0.4616           0.0154           20.17s\n",
            "         6           0.4375           0.0114           20.58s\n",
            "         7           0.4224           0.0118           20.32s\n",
            "         8           0.3886           0.0112           20.27s\n",
            "         9           0.4028           0.0091           20.42s\n",
            "        10           0.4019           0.0096           20.09s\n",
            "        20           0.2951           0.0079           17.83s\n",
            "        30           0.2546           0.0032           16.59s\n",
            "        40           0.2230           0.0019           15.82s\n",
            "        50           0.1923           0.0020           14.99s\n",
            "        60           0.1704           0.0012           14.19s\n",
            "        70           0.1631           0.0006           13.48s\n",
            "        80           0.1529           0.0002           12.77s\n",
            "        90           0.1380           0.0005           12.01s\n",
            "       100           0.1327           0.0005           11.28s\n",
            "       200           0.0869          -0.0000            4.84s\n",
            "       300           0.0823          -0.0000            0.00s\n",
            "0.1\n",
            "0.8756793478260869\n",
            "0.7713943746259725\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5053           0.0324           32.62s\n",
            "         2           0.4947           0.0304           31.48s\n",
            "         3           0.4772           0.0231           30.61s\n",
            "         4           0.4654           0.0148           31.88s\n",
            "         5           0.4443           0.0154           32.08s\n",
            "         6           0.4131           0.0166           31.74s\n",
            "         7           0.4015           0.0104           31.86s\n",
            "         8           0.4011           0.0103           32.54s\n",
            "         9           0.3804           0.0093           32.29s\n",
            "        10           0.3649           0.0118           31.31s\n",
            "        20           0.2784           0.0060           27.96s\n",
            "        30           0.2227           0.0035           26.02s\n",
            "        40           0.1852           0.0016           24.85s\n",
            "        50           0.1635           0.0014           23.27s\n",
            "        60           0.1402           0.0018           21.90s\n",
            "        70           0.1266           0.0014           20.48s\n",
            "        80           0.1136           0.0009           19.07s\n",
            "        90           0.1077           0.0003           17.72s\n",
            "       100           0.0958           0.0003           16.44s\n",
            "       200           0.0744           0.0001            6.29s\n",
            "       300           0.0687          -0.0000            0.00s\n",
            "0.2\n",
            "0.9018759018759018\n",
            "0.7480550568521843\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5245           0.0392           44.21s\n",
            "         2           0.4902           0.0251           48.06s\n",
            "         3           0.4680           0.0239           46.67s\n",
            "         4           0.4366           0.0179           45.32s\n",
            "         5           0.4169           0.0174           45.12s\n",
            "         6           0.4141           0.0135           46.44s\n",
            "         7           0.3913           0.0115           46.67s\n",
            "         8           0.3825           0.0134           45.06s\n",
            "         9           0.3560           0.0103           45.18s\n",
            "        10           0.3504           0.0099           44.81s\n",
            "        20           0.2677           0.0063           39.65s\n",
            "        30           0.2163           0.0032           37.70s\n",
            "        40           0.1762           0.0021           35.44s\n",
            "        50           0.1509           0.0014           32.83s\n",
            "        60           0.1278           0.0013           30.67s\n",
            "        70           0.1111           0.0005           28.44s\n",
            "        80           0.1014           0.0006           26.05s\n",
            "        90           0.0939           0.0007           23.93s\n",
            "       100           0.0856           0.0003           21.99s\n",
            "       200           0.0675           0.0000            7.65s\n",
            "       300           0.0602          -0.0000            0.00s\n",
            "0.30000000000000004\n",
            "0.9024566473988439\n",
            "0.7474566128067026\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5207           0.0351            1.01m\n",
            "         2           0.4961           0.0243            1.03m\n",
            "         3           0.4552           0.0234            1.00m\n",
            "         4           0.4382           0.0169            1.01m\n",
            "         5           0.4213           0.0172           58.82s\n",
            "         6           0.4012           0.0138           58.90s\n",
            "         7           0.3814           0.0120           58.20s\n",
            "         8           0.3692           0.0134           57.21s\n",
            "         9           0.3574           0.0098           57.15s\n",
            "        10           0.3429           0.0108           56.42s\n",
            "        20           0.2567           0.0060           52.14s\n",
            "        30           0.2010           0.0031           49.17s\n",
            "        40           0.1618           0.0021           45.51s\n",
            "        50           0.1320           0.0014           41.67s\n",
            "        60           0.1159           0.0013           38.10s\n",
            "        70           0.1022           0.0005           34.81s\n",
            "        80           0.0919           0.0008           31.57s\n",
            "        90           0.0839           0.0006           28.70s\n",
            "       100           0.0783           0.0001           26.15s\n",
            "       200           0.0603          -0.0000            8.90s\n",
            "       300           0.0560          -0.0000            0.00s\n",
            "0.4\n",
            "0.9036402569593148\n",
            "0.7576301615798923\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5243           0.0383            1.10m\n",
            "         2           0.4867           0.0254            1.13m\n",
            "         3           0.4615           0.0179            1.15m\n",
            "         4           0.4427           0.0175            1.15m\n",
            "         5           0.4158           0.0170            1.12m\n",
            "         6           0.3977           0.0143            1.12m\n",
            "         7           0.3818           0.0133            1.11m\n",
            "         8           0.3667           0.0104            1.11m\n",
            "         9           0.3511           0.0118            1.09m\n",
            "        10           0.3353           0.0096            1.09m\n",
            "        20           0.2409           0.0048           58.53s\n",
            "        30           0.1890           0.0029           54.70s\n",
            "        40           0.1507           0.0022           50.68s\n",
            "        50           0.1231           0.0014           46.53s\n",
            "        60           0.1048           0.0008           42.19s\n",
            "        70           0.0946           0.0010           38.10s\n",
            "        80           0.0838           0.0003           34.47s\n",
            "        90           0.0776           0.0002           31.26s\n",
            "       100           0.0742           0.0003           28.36s\n",
            "       200           0.0557           0.0000            9.68s\n",
            "       300           0.0536           0.0001            0.00s\n",
            "0.5\n",
            "0.9095588235294118\n",
            "0.7402752842609216\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5198           0.0367            1.52m\n",
            "         2           0.4869           0.0248            1.50m\n",
            "         3           0.4601           0.0217            1.43m\n",
            "         4           0.4398           0.0171            1.40m\n",
            "         5           0.4154           0.0165            1.37m\n",
            "         6           0.3902           0.0163            1.35m\n",
            "         7           0.3785           0.0138            1.32m\n",
            "         8           0.3594           0.0121            1.30m\n",
            "         9           0.3385           0.0117            1.26m\n",
            "        10           0.3301           0.0095            1.24m\n",
            "        20           0.2343           0.0062            1.14m\n",
            "        30           0.1781           0.0034            1.07m\n",
            "        40           0.1411           0.0027           57.93s\n",
            "        50           0.1151           0.0016           52.28s\n",
            "        60           0.0991           0.0010           47.21s\n",
            "        70           0.0864           0.0009           42.65s\n",
            "        80           0.0798           0.0004           38.42s\n",
            "        90           0.0733           0.0004           34.67s\n",
            "       100           0.0674           0.0004           31.45s\n",
            "       200           0.0544          -0.0000           10.50s\n",
            "       300           0.0510          -0.0000            0.00s\n",
            "0.6\n",
            "0.909288824383164\n",
            "0.7498503889886295\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5248           0.0332            1.80m\n",
            "         2           0.4878           0.0304            1.55m\n",
            "         3           0.4530           0.0222            1.53m\n",
            "         4           0.4298           0.0175            1.53m\n",
            "         5           0.4051           0.0157            1.51m\n",
            "         6           0.3846           0.0142            1.49m\n",
            "         7           0.3674           0.0134            1.46m\n",
            "         8           0.3499           0.0128            1.42m\n",
            "         9           0.3364           0.0105            1.41m\n",
            "        10           0.3228           0.0098            1.40m\n",
            "        20           0.2268           0.0039            1.28m\n",
            "        30           0.1699           0.0035            1.16m\n",
            "        40           0.1350           0.0021            1.06m\n",
            "        50           0.1091           0.0014           56.67s\n",
            "        60           0.0928           0.0012           50.93s\n",
            "        70           0.0824           0.0005           45.57s\n",
            "        80           0.0766           0.0010           40.75s\n",
            "        90           0.0681           0.0004           36.94s\n",
            "       100           0.0648           0.0002           33.30s\n",
            "       200           0.0503          -0.0000           11.04s\n",
            "       300           0.0485          -0.0000            0.00s\n",
            "0.7000000000000001\n",
            "0.9137426900584795\n",
            "0.7480550568521843\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5226           0.0412            1.56m\n",
            "         2           0.4825           0.0282            1.54m\n",
            "         3           0.4485           0.0199            1.66m\n",
            "         4           0.4265           0.0170            1.68m\n",
            "         5           0.4008           0.0172            1.69m\n",
            "         6           0.3807           0.0126            1.70m\n",
            "         7           0.3634           0.0138            1.64m\n",
            "         8           0.3473           0.0104            1.68m\n",
            "         9           0.3324           0.0101            1.70m\n",
            "        10           0.3187           0.0081            1.70m\n",
            "        20           0.2226           0.0048            1.48m\n",
            "        30           0.1679           0.0032            1.36m\n",
            "        40           0.1313           0.0028            1.22m\n",
            "        50           0.1064           0.0019            1.08m\n",
            "        60           0.0896           0.0013           57.60s\n",
            "        70           0.0786           0.0008           51.23s\n",
            "        80           0.0705           0.0009           45.93s\n",
            "        90           0.0670           0.0002           41.29s\n",
            "       100           0.0621           0.0002           37.19s\n",
            "       200           0.0490           0.0000           12.34s\n",
            "       300           0.0459          -0.0000            0.00s\n",
            "0.8\n",
            "0.9121771217712177\n",
            "0.7396768402154399\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5226           0.0373            1.78m\n",
            "         2           0.4818           0.0275            1.84m\n",
            "         3           0.4479           0.0248            1.76m\n",
            "         4           0.4221           0.0153            1.89m\n",
            "         5           0.3983           0.0170            1.79m\n",
            "         6           0.3765           0.0139            1.76m\n",
            "         7           0.3573           0.0116            1.79m\n",
            "         8           0.3419           0.0107            1.80m\n",
            "         9           0.3272           0.0086            1.80m\n",
            "        10           0.3141           0.0081            1.80m\n",
            "        20           0.2170           0.0057            1.67m\n",
            "        30           0.1621           0.0029            1.48m\n",
            "        40           0.1267           0.0015            1.32m\n",
            "        50           0.1042           0.0014            1.17m\n",
            "        60           0.0889           0.0018            1.03m\n",
            "        70           0.0791           0.0005           55.09s\n",
            "        80           0.0707           0.0002           49.21s\n",
            "        90           0.0661           0.0003           44.06s\n",
            "       100           0.0615           0.0003           39.67s\n",
            "       200           0.0479           0.0000           13.17s\n",
            "       300           0.0452          -0.0000            0.00s\n",
            "0.9\n",
            "0.9174041297935103\n",
            "0.7444643925792939\n",
            "      Iter       Train Loss   Remaining Time \n",
            "         1           0.5215            2.14m\n",
            "         2           0.4782            2.13m\n",
            "         3           0.4450            2.12m\n",
            "         4           0.4175            2.13m\n",
            "         5           0.3936            2.12m\n",
            "         6           0.3724            2.05m\n",
            "         7           0.3537            2.05m\n",
            "         8           0.3365            1.99m\n",
            "         9           0.3216            1.99m\n",
            "        10           0.3075            1.98m\n",
            "        20           0.2110            1.79m\n",
            "        30           0.1552            1.59m\n",
            "        40           0.1221            1.40m\n",
            "        50           0.1009            1.24m\n",
            "        60           0.0863            1.08m\n",
            "        70           0.0767           57.94s\n",
            "        80           0.0684           51.61s\n",
            "        90           0.0620           46.25s\n",
            "       100           0.0576           41.81s\n",
            "       200           0.0464           13.41s\n",
            "       300           0.0446            0.00s\n",
            "1.0\n",
            "0.9209932279909706\n",
            "0.7324955116696589\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9ba0cf4c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC4qhnb5niBN",
        "colab_type": "code",
        "outputId": "b592e055-63c7-4dc2-8cc4-830df5a95e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "abclf = GradientBoostingClassifier(loss='deviance', learning_rate=0.032, n_estimators=200, subsample=0.5, max_depth=10, min_impurity_decrease=0. , verbose=1, max_features='sqrt', max_leaf_nodes=None)\n",
        "abclf.fit(normalized_x, y_train)\n",
        "\n",
        "y_pred = abclf.predict(normalized_x_test)\n",
        "\n",
        "currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "print(currPrec)\n",
        "print(currRec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5729           0.0108            4.97m\n",
            "         2           0.5574           0.0100            5.01m\n",
            "         3           0.5467           0.0093            4.98m\n",
            "         4           0.5392           0.0084            4.95m\n",
            "         5           0.5301           0.0082            4.93m\n",
            "         6           0.5189           0.0077            4.94m\n",
            "         7           0.5104           0.0071            4.94m\n",
            "         8           0.5032           0.0067            4.94m\n",
            "         9           0.4945           0.0067            4.92m\n",
            "        10           0.4909           0.0063            4.90m\n",
            "        20           0.4279           0.0042            4.61m\n",
            "        30           0.3905           0.0031            4.38m\n",
            "        40           0.3548           0.0027            4.17m\n",
            "        50           0.3274           0.0020            3.94m\n",
            "        60           0.3033           0.0016            3.70m\n",
            "        70           0.2870           0.0015            3.45m\n",
            "        80           0.2678           0.0013            3.19m\n",
            "        90           0.2527           0.0011            2.93m\n",
            "       100           0.2421           0.0009            2.66m\n",
            "       200           0.1669           0.0003            0.00s\n",
            "0.8746690203000883\n",
            "0.5930580490724118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "44f01c3f-4af2-4bf4-d3ef-262afd3b9f83",
        "id": "LXpgWxvWpDFn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "clf = LogisticRegression(penalty='elasticnet', solver='saga' , l1_ratio=0.5, random_state=0, tol=0.0001, C=1e20, fit_intercept=True, class_weight='balanced', max_iter=5000, verbose=1, n_jobs=1)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "print(currPrec)\n",
        "print(currRec)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 4000 epochs took 249 seconds\n",
            "0.3447733822373592\n",
            "0.7875523638539796\n",
            "[[14171  2501]\n",
            " [  355  1316]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.1min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNJxQl5seQWY",
        "colab_type": "code",
        "outputId": "0db2e4b1-9508-4c6d-fb04-87a3b0e244ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "clf = LogisticRegression(penalty='elasticnet', solver='saga' , l1_ratio=0.5, random_state=0, tol=0.0001, C=1e20, fit_intercept=True, class_weight='balanced', max_iter=100, verbose=1, n_jobs=1)\n",
        "clf.fit(normalized_x, y_train)\n",
        "\n",
        "y_pred = clf.predict(normalized_x_test)\n",
        "\n",
        "currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "print(currPrec)\n",
        "print(currRec)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 6 seconds\n",
            "0.6093344857389801\n",
            "0.42190305206463197\n",
            "[[16220   452]\n",
            " [  966   705]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.2s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBFkkw5aagG2",
        "colab_type": "code",
        "outputId": "e3438a1b-b497-45e1-d111-46cde857ff6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "x_res, y_res = rus.fit_resample(x_train, y_train)\n",
        "clf = LogisticRegression(penalty='elasticnet', solver='saga' , l1_ratio=0.5, random_state=0, tol=0.0001, C=1e20, fit_intercept=True, class_weight=None, max_iter=5000, verbose=1, n_jobs=-1)\n",
        "clf.fit(x_res, y_res)\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "print(currPrec)\n",
        "print(currRec)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 47 seconds\n",
            "0.30994989262705797\n",
            "0.77737881508079\n",
            "[[13780  2892]\n",
            " [  372  1299]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   47.1s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a6d380bf-29b0-4412-ad7a-7105e0bcbc89",
        "id": "6X65uy2GjPy1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "clf = LogisticRegression(penalty='elasticnet', solver='saga' , l1_ratio=0.5, random_state=0, tol=0.0001, C=1e20, fit_intercept=True, class_weight=None, max_iter=5000, verbose=1, n_jobs=-1)\n",
        "clf.fit(x_train_lda, y_train)\n",
        "\n",
        "y_pred = clf.predict(x_test_lda)\n",
        "\n",
        "currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "print(currPrec)\n",
        "print(currRec)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 90 epochs took 0 seconds\n",
            "0.6868811881188119\n",
            "0.33213644524236985\n",
            "[[16419   253]\n",
            " [ 1116   555]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.7s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JlkaXeabhCZ",
        "colab_type": "code",
        "outputId": "157209d6-b365-4fb0-8198-05f13f4039b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "prec = []\n",
        "rec = []\n",
        "\n",
        "valor = 0.001\n",
        "lista = []\n",
        "while valor < 12:\n",
        "  lista.append(valor)\n",
        "  valor += valor\n",
        "\n",
        "for cVal in lista:\n",
        "  clf = LogisticRegression(penalty='elasticnet', solver='saga' , l1_ratio=0.5, random_state=0, tol=0.0001, C=cVal, fit_intercept=True, class_weight='balanced', max_iter=5000, verbose=1, n_jobs=-1)\n",
        "  clf.fit(x_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(x_test)\n",
        "\n",
        "  currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "  currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "  print(cVal)\n",
        "  print(currPrec)\n",
        "  print(currRec)\n",
        "\n",
        "  prec.append(currPrec)\n",
        "  rec.append(currRec)\n",
        "\n",
        "# Data\n",
        "df=pd.DataFrame({'x': lista, 'precision': prec, 'recall': rec })\n",
        " \n",
        "# multiple line plot\n",
        "plt.plot( 'x', 'precision', data=df, marker='', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4, label=\"Precision\")\n",
        "plt.plot( 'x', 'recall', data=df, marker='', color='olive', linewidth=2, label=\"Recall\")\n",
        "plt.xlabel(\"Alpha\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 1512 epochs took 103 seconds\n",
            "0.001\n",
            "0.29490124023886083\n",
            "0.7684021543985637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.7min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 2100 epochs took 131 seconds\n",
            "0.002\n",
            "0.3088589311506981\n",
            "0.767803710353082\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.2min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 2683 epochs took 181 seconds\n",
            "0.004\n",
            "0.3213572854291417\n",
            "0.7707959305804907\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.0min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 3166 epochs took 198 seconds\n",
            "0.008\n",
            "0.3326551373346897\n",
            "0.7827648114901257\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.3min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 3512 epochs took 232 seconds\n",
            "0.016\n",
            "0.33694810224632066\n",
            "0.7809694793536804\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.9min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 3733 epochs took 232 seconds\n",
            "0.032\n",
            "0.34097421203438394\n",
            "0.7833632555356074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.9min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 3859 epochs took 238 seconds\n",
            "0.064\n",
            "0.34329140461215935\n",
            "0.7839616995810892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  4.0min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 3928 epochs took 245 seconds\n",
            "0.128\n",
            "0.3439056356487549\n",
            "0.7851585876720527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  4.1min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 3964 epochs took 250 seconds\n",
            "0.256\n",
            "0.3440775681341719\n",
            "0.7857570317175344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  4.2min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 3982 epochs took 248 seconds\n",
            "0.512\n",
            "0.3442579968536969\n",
            "0.7857570317175344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  4.1min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 3991 epochs took 253 seconds\n",
            "1.024\n",
            "0.3443396226415094\n",
            "0.7863554757630161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  4.2min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 3996 epochs took 268 seconds\n",
            "2.048\n",
            "0.3444211629125196\n",
            "0.7869539198084979\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  4.5min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 3998 epochs took 256 seconds\n",
            "4.096\n",
            "0.34468308014667365\n",
            "0.7875523638539796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  4.3min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 3999 epochs took 255 seconds\n",
            "8.192\n",
            "0.34468308014667365\n",
            "0.7875523638539796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  4.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6a93f08ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f8259e10-9fa0-4bed-da59-bfaee32cb740",
        "id": "7dw6T8SWf_NU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "prec = []\n",
        "rec = []\n",
        "\n",
        "valor = 10\n",
        "lista = []\n",
        "while valor < 6000:\n",
        "  lista.append(valor)\n",
        "  valor += valor\n",
        "\n",
        "for maxIter in lista:\n",
        "  clf = LogisticRegression(penalty='elasticnet', solver='saga' , \n",
        "                           l1_ratio=0.5, random_state=0, tol=0.0001, C=8, \n",
        "                           fit_intercept=True, class_weight='balanced', \n",
        "                           max_iter=maxIter, verbose=1, n_jobs=-1)\n",
        "  \n",
        "  clf.fit(x_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(x_test)\n",
        "\n",
        "  currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "  currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "  print(maxIter)\n",
        "  print(currPrec)\n",
        "  print(currRec)\n",
        "\n",
        "  prec.append(currPrec)\n",
        "  rec.append(currRec)\n",
        "\n",
        "# Data\n",
        "df=pd.DataFrame({'x': lista, 'precision': prec, 'recall': rec })\n",
        " \n",
        "# multiple line plot\n",
        "plt.plot( 'x', 'precision', data=df, marker='', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4, label=\"Precision\")\n",
        "plt.plot( 'x', 'recall', data=df, marker='', color='olive', linewidth=2, label=\"Recall\")\n",
        "plt.xlabel(\"Alpha\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 1 seconds\n",
            "10\n",
            "0.25742574257425743\n",
            "0.7312986235786954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 1 seconds\n",
            "20\n",
            "0.26730310262529833\n",
            "0.7372830640335128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 3 seconds\n",
            "40\n",
            "0.27503308337009263\n",
            "0.7462597247157391\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.7s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 5 seconds\n",
            "80\n",
            "0.28327338129496404\n",
            "0.7540394973070018\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    5.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 11 seconds\n",
            "160\n",
            "0.2909132524605173\n",
            "0.760622381807301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   10.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 20 seconds\n",
            "320\n",
            "0.29769927957239134\n",
            "0.7666068222621185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   20.2s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 40 seconds\n",
            "640\n",
            "0.3076923076923077\n",
            "0.7779772591262717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   40.3s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 83 seconds\n",
            "1280\n",
            "0.32206710751898115\n",
            "0.7869539198084979\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.4min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 163 seconds\n",
            "2560\n",
            "0.3346094946401225\n",
            "0.7845601436265709\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.7min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "convergence after 3999 epochs took 255 seconds\n",
            "5120\n",
            "0.34468308014667365\n",
            "0.7875523638539796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  4.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6a946bbb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3df5RU5Z3n8fe3qn83qAgtIo2CBhEM\nRkyrsJjE4KAYE4hJXCFu1kyceNYIccY5u4ObWX8te3Zi0E08y6wxjgGN8Uc0s5IsGRRjoib+oFEE\nFYEGURqJNM3vhu6mqr77R91uqovqHzR9u7r7fl7n1Kl7n3vr8jxl+3ye+9yqW+buiIhIdMXyXQER\nEckvBYGISMQpCEREIk5BICIScQoCEZGIK8h3BY7VsGHDfPTo0fmuhohIv7Jq1aqd7l6Ra1u/C4LR\no0dTXV2d72qIiPQrZvZhe9tCnRoysxlmtt7Masxsfo7tp5vZi2b2lpmtMbMvhVkfERE5WmhBYGZx\nYBFwJTABmGNmE7J2+0fgKXefBMwG/jms+oiISG5hnhFcBNS4+2Z3bwaeAGZl7ePACcHyicDHIdZH\nRERyCPMawUhga8Z6LXBx1j53As+Z2TygHPirEOsjIiI55Pvjo3OAxe5eCXwJeNTMjqqTmd1oZtVm\nVl1XV9frlRQRGcjCDIJtwKiM9cqgLNMNwFMA7v4qUAIMyz6Quz/o7lXuXlVRkfPTTyIi0k1hBsFK\nYKyZjTGzItIXg5dm7fMRcBmAmY0nHQQa8ouI9KLQrhG4e8LM5gLLgTjwsLu/a2Z3A9XuvhT4e+Bn\nZvZ3pC8cf9t1X+xe5e40Ne3lwIFPaGjYQUNDy/MOEokmYrE4ZvHjfI71wDG6/mxm+X5b+y13xz2F\newo4snz0o6Ntnb32eF8f3r8dZr174tijR0/j0kvv6PH/7qF+oczdlwHLsspuz1h+D5gaZh2iKJk8\nzMGDO1s79aM7+SOdfUPDDpLJ5nxXuYdZr4VOduD1pY6nO6+Vvm3QoBGhHLfffbM4qpqbGzrt2FvK\nDx2qP6ZjFxUNorx8OOXlpzBo0HDKyk6hvPwUCgtLSaWSuCfbfXZP5Shrf/+wn1s6uFQqASRIJkP5\nzzGgpc/gjjzAjipr++h4ez5fD327fl17/ZFjlJWFc41UQZBH7s7u3ZvZu/ejDjr59PLhwweP4chG\nWdkwBg1Kd+4tnXzm8pFtp1BYWBZaG3vbkVFwfkIoPTXVlzqRY329ptWiSEHQi5qbD7Bt20pqa18N\nHq9x8ODOLr02Hi/O0bFnd+rp57KyYcRi8ZBb0zelO744ECcezbdA5JgpCELi7uzaVUNt7ats3Zru\n+HfsWHvUPGx5+SkMHTqudUom1yh+0KDhFBUN1mhNREKhIOghXRntm8UZMeKzVFZOYdSoKVRWTuak\nk8aogxeRvFIQdEN6bn9T60i/tvZVPvlkTc7RfmXllNaO/7TTqgbUfLyIDAwKgi5IJpvZvv0tPvro\nFbZu/RMfffQKBw+2/d5berR/QZuOX6N9EekPFAQ5NDXtY+vWV4OO/xVqa18nkTjUZp/s0f6IEZ+l\nqKg8TzUWEek+BUGgubmBN97437z77hM5p3mGDTuHUaMu4fTTL+H006cyZMhZGu2LyIAQ+SBIJBqp\nrn6AV175nzQ07AAgFitk5MiLWjv+UaP+HeXlutmdiAxMkQ2CVCrBm28+xEsvLWD//vRNUUeOvIjP\nf/52xoz5oi7qikhkRDII3J1nn/0Oa9Y8CsDw4Z9h2rQFjB17laZ7RCRyIhkEf/7zQtaseZTCwnJm\nzXqYCRO+EXzdXkQkeiIXBBs3LmPFin8A4OqrH2X8+KvzXCMRkfyK1DA4mTzMs89+B3AuvfRuhYCI\nCBELgk2bnqOh4ROGDTuHz3/+H/NdHRGRPiFSQbB27WMATJz4H3RRWEQkEJkgaG5uYP36ZwGYOPGb\nea6NiEjfEZkg+PDDlzh8+CCnnVbFkCFj8l0dEZE+IzJBsHnzCgDOPPPyPNdERKRviUwQbNnyewDO\nOmt6nmsiItK3ROZ7BNdf/we2bHmRysop+a6KiEifEpkgKCk5kXPO+Wq+qyEi0udEZmpIRERyUxCI\niEScgkBEJOIUBCIiEacgEBGJOAWBiEjEhRoEZjbDzNabWY2Zzc+x/X+Z2ergscHM9oRZHxEROVpo\n3yMwsziwCJgO1AIrzWypu7/Xso+7/13G/vOASWHVR0REcgvzjOAioMbdN7t7M/AEMKuD/ecAj4dY\nHxERySHMIBgJbM1Yrw3KjmJmZwBjgN+3s/1GM6s2s+q6uroer6iISJT1lYvFs4Gn3T2Za6O7P+ju\nVe5eVVFR0ctVExEZ2MIMgm3AqIz1yqAsl9loWkhEJC/CDIKVwFgzG2NmRaQ7+6XZO5nZOcAQ4NUQ\n6yIiIu0ILQjcPQHMBZYD64Cn3P1dM7vbzGZm7DobeMLdPay6iIhI+0K9DbW7LwOWZZXdnrV+Z5h1\nEBGRjvWVi8UiIpInCgIRkYhTEIiIRJyCQEQk4hQEIiIRpyAQEYk4BYGISMQpCEREIk5BICIScQoC\nEZGIUxCIiEScgkBEJOIUBCIiEacgEBGJOAWBiEjEKQhERCJOQSAiEnEKAhGRiFMQiIhEnIJARCTi\nFAQiIhGnIBARiTgFgYhIxCkIREQiTkEgIhJxCgIRkYhTEIiIRJyCQEQk4kINAjObYWbrzazGzOa3\ns8+/N7P3zOxdM/tlmPUREZGjFYR1YDOLA4uA6UAtsNLMlrr7exn7jAVuA6a6+24zOyWs+oiISG5h\nnhFcBNS4+2Z3bwaeAGZl7fNdYJG77wZw9x0h1kdERHIIMwhGAlsz1muDskxnA2eb2Z/M7DUzm5Hr\nQGZ2o5lVm1l1XV1dSNUVEYmmfF8sLgDGApcCc4CfmdlJ2Tu5+4PuXuXuVRUVFb1cRRGRgS3MINgG\njMpYrwzKMtUCS939sLt/AGwgHQwiItJLwgyClcBYMxtjZkXAbGBp1j7/l/TZAGY2jPRU0eYQ6yQi\nIllCCwJ3TwBzgeXAOuApd3/XzO42s5nBbsuBejN7D3gR+M/uXh9WnURE5Gjm7vmuwzGpqqry6urq\nfFdDRKRfMbNV7l6Va1u+LxaLiEieKQhERCJOQSAiEnEKAhGRiFMQiIhEnIJARCTiFAQiIhGnIBAR\niTgFgYhIxCkIREQiTkEgIhJxCgIRkYjrchCY2SVm9tfBcoWZjQmvWiIi0lu6FARmdgfwD6R/aB6g\nEPhFWJUSEZHe09UzgquBmUADgLt/DAwOq1IiItJ7uhoEzZ7+4QIHMLPy8KokIiK9qatB8JSZ/RQ4\nycy+C6wAfhZetUREpLcUdGUnd19oZtOBfcA44HZ3fz7UmomISK/oNAjMLA6scPcvAur8RUQGmE6n\nhtw9CaTM7MReqI+IiPSyLk0NAQeAtWb2PMEnhwDc/fuh1EpERHpNV4Pg18FDREQGmK5eLF5iZkXA\n2UHRenc/HF61RESkt3QpCMzsUmAJsAUwYJSZXe/uL4VXNRER6Q1dnRq6F7jc3dcDmNnZwOPAZ8Oq\nmIiI9I6ufqGssCUEANx9A+n7DYmISD/X1TOCajN7iCM3mrsOqA6nSiIi0pu6GgQ3ATcDLR8XfRn4\n51BqJCIivaqrU0MFwE/c/Wvu/jXgfiDe2YvMbIaZrTezGjObn2P7t82szsxWB4+/Obbqi4jI8epq\nELwAlGasl5K+8Vy7gltTLAKuBCYAc8xsQo5dn3T384PHQ12sj4iI9JCuBkGJux9oWQmWyzp5zUVA\njbtvdvdm4AlgVveqKSIiYelqEDSY2QUtK2ZWBRzq5DUjga0Z67VBWbavm9kaM3vazEblOpCZ3Whm\n1WZWXVdX18Uqi4hIV3Q1CP4W+JWZvWxmL5Me3c/tgX//N8Bodz+P9J1Nl+Tayd0fdPcqd6+qqKjo\ngX9WRERadBgEZnahmZ3q7iuBc4AngcPAvwEfdHLsbUDmCL8yKGvl7vXu3hSsPoS+oCYi0us6OyP4\nKdAcLE8B/ivpC8C7gQc7ee1KYKyZjQnuUzQbWJq5g5mNyFidCazrYr1FRKSHdPY9gri77wqWrwUe\ndPdngGfMbHVHL3T3hJnNBZaT/qjpw+7+rpndDVS7+1Lg+2Y2E0gAu4BvH0dbRESkGzoNAjMrcPcE\ncBlw4zG8FndfBizLKrs9Y/k24LauV1dERHpaZ53548AfzWwn6U8JvQxgZp8C9oZcNxER6QUdBoG7\n/w8zewEYATzn7h5sigHzwq6ciIiEryvTO6/lKNsQTnVERKS3dfV7BCIiMkApCEREIk5BICIScQoC\nEZGIUxCIiEScgkBEJOIUBCIiEacgEBGJOAWBiEjEKQhERCJOQSAiEnEKAhGRiFMQiIhEnIJARCTi\nFAQiIhGnIBARiTgFgYhIxCkIREQiTkEgIhJxCgIRkYhTEIiIRJyCQEQk4hQEIiIRpyAQEYk4BYGI\nSMQpCEREIi7UIDCzGWa23sxqzGx+B/t93czczKrCrI+IiBwttCAwsziwCLgSmADMMbMJOfYbDNwC\nvB5WXUREpH1hnhFcBNS4+2Z3bwaeAGbl2O+/Az8EGkOsi4iItCPMIBgJbM1Yrw3KWpnZBcAod/9/\nHR3IzG40s2ozq66rq+v5moqIRFjeLhabWQy4D/j7zvZ19wfdvcrdqyoqKsKvnIhIhIQZBNuAURnr\nlUFZi8HAp4E/mNkWYDKwVBeMRUR6V5hBsBIYa2ZjzKwImA0sbdno7nvdfZi7j3b30cBrwEx3rw6x\nTiIikiW0IHD3BDAXWA6sA55y93fN7G4zmxnWvysiIsemIMyDu/syYFlW2e3t7HtpmHUREZHc9M1i\nEZGIUxCIiEScgkBEJOIUBCIiEacgEBGJOAWBiEjEKQhERCJOQSAiEnEKAhGRiFMQiIhEnIJARCTi\nFAQiIhEX6k3nRESiwN1JOiQzn1NH1lMOiZSTdCflkHRIuJNKZb3GIZnq+FijBhUwaVhpj9ZfQSAi\nfZq7kwKSKUi5kwg6x1TQaSY8Xd5uJ5qrvJPOtuV4iVRLx517v8yOvrcYMGlYzx5TQSAScZ7ZuXZj\nhNpZZ5tqGf0G2450rh0cK6tcjkh6z78hCgKRELlndXrtdX7H1LH6kamFro52O9hP/Wz/EkYwKghE\nsiRSTmPSaUqmaEq2LHuwnF2Wal1uTnmbUbBGs9ESMygwI2YQN4ibEY8Fz+2tG8TMKAjKW48RrBcE\n2+MG8Vj6+YSieI/XXUEgA4p7ukM+0nG37cCPlLXfyavz7nsM2nSG8azOMbtzzSyPGRTEWjro3J1r\nh8fL6rxbjpfuxI90/maW77ep2xQE0qekPLsTT+XsrNsblTclXVMd3dDRiDW7/EhHmDX67WbHmrPj\nzhgVt4yaJTwKAulRiVRHnXaqk1F6ejQ/0BzVWXY6yswYtXbSaXZlVNxyvHiuUXEs/WWivjSaPXz4\nMLW1tTQ2Nua7Kv1SSUkJlZWVFBYWdvk1CgI5SjLl7G1OcTCRarfDbq+TTwyAfjxmUBI3iuNGcTyW\nsWyUxGPB89FlRbHcHXdf6mT7g9raWgYPHszo0aP13h0jd6e+vp7a2lrGjBnT5dcpCCIqkXL2NCXZ\n3Zxkd1OK3U3J1se+5lS/nl4pjEFxdocdM0oKYhnL7XfyBeq886qxsVEh0E1mxtChQ6mrqzum1ykI\nBrDmpLOnOdmmk9/dlGJPU5J9h1P5rl67imMZnXOBURyLZSy3lAdlMaO4IOjEg23xmDqQ/k4h0H3d\nee8UBP1cYzLFnqwR/e6mJHuaUhxI9H5nb9DJVEruUXjLPkVx04VBkV6mIOgHDiVSbUb0u5uSrSP9\ngyFNyg8ujDGo8Ein3dHc+JHyGIUxjeak/4vH40ycOJFEIsH48eNZsmQJZWVlx3XM6upqHnnkEe6/\n//6c2z/++GO+//3v8/TTTx/Xv9MdCoI+wN05mPAjnX1zss0ovzGkD7afUBRjSFGcIcVxhhTHguc4\nJxXHKdT0iuTZP721M9Tjz+/ghj2lpaWsXr0agOuuu44HHniAW2+9tXW7u+PuxGJdv4FzVVUVVVVV\n7W4/7bTT8hICoCDodYdTzo5DCT45mOAvwfPuplQoH5s04MSiIx38kUeME4viFKizF+nU5z73Odas\nWcOWLVu44ooruPjii1m1ahXLli1j/fr13HHHHTQ1NXHWWWfx85//nEGDBrFy5UpuueUWGhoaKC4u\n5oUXXmDVqlUsXLiQ3/72t/zxj3/klltuAdJn0C+99BL19fV8+ctf5p133qGxsZGbbrqJ6upqCgoK\nuO+++/jiF7/I4sWLWbp0KQcPHmTTpk1cffXV3HPPPcfdRgVBiJqSKT45lEx3+gcTfHIoQX1jskc/\nkRMzOKmo7Yi+5XFCUYy4pmlEui2RSPC73/2OGTNmALBx40aWLFnC5MmT2blzJwsWLGDFihWUl5fz\nwx/+kPvuu4/58+dz7bXX8uSTT3LhhReyb98+Skvb3jZ64cKFLFq0iKlTp3LgwAFKSkrabF+0aBFm\nxtq1a3n//fe5/PLL2bBhAwCrV6/mrbfeori4mHHjxjFv3jxGjRp1XO0MNQjMbAbwEyAOPOTu/5S1\n/T8BNwNJ4ABwo7u/F2adwnIokWozyv/LofRIvycUGJyUNaIfUpSewjmhKKaLqyI97NChQ5x//vlA\n+ozghhtu4OOPP+aMM85g8uTJALz22mu89957TJ06FYDm5mamTJnC+vXrGTFiBBdeeCEAJ5xwwlHH\nnzp1KrfeeivXXXcdX/va16isrGyz/ZVXXmHevHkAnHPOOZxxxhmtQXDZZZdx4oknAjBhwgQ+/PDD\nvhsEZhYHFgHTgVpgpZktzerof+nuDwT7zwTuA2aEVaee0nA41TrCb3ne23x8nX5hjCMdfVHLXH16\nlD+4MKYLsBI5Hc3hhy3zGkGm8vLy1mV3Z/r06Tz++ONt9lm7dm2nx58/fz5XXXUVy5YtY+rUqSxf\nvvyos4L2FBcXty7H43ESiUSXXteRMM8ILgJq3H0zgJk9AcwCWoPA3fdl7F9OH70jbmMyxUf7D7Nl\n/2E+3H+Y+qbkcR1vSHGM4aUFnFpWwPDSAipKCygvMHX2Iv3I5MmTufnmm6mpqeFTn/oUDQ0NbNu2\njXHjxrF9+3ZWrlzJhRdeyP79+4+aGtq0aRMTJ05k4sSJrFy5kvfff7/1DATSZyGPPfYY06ZNY8OG\nDXz00UeMGzeON998M5S2hBkEI4GtGeu1wMXZO5nZzcCtQBEwLdeBzOxG4EaA008/vccrmi2Rcj5u\nSLBlfzNb9h9m+8FEtxLKgKElcYaXFjC8rIBTSws4pSxOSVw/FS3S31VUVLB48WLmzJlDU1MTAAsW\nLODss8/mySefZN68eRw6dIjS0lJWrFjR5rU//vGPefHFF4nFYpx77rlceeWVbN++vXX79773PW66\n6SYmTpxIQUEBixcvbnMm0NPMQ/i1GwAz+wYww93/Jlj/FnCxu89tZ/9vAle4+/UdHbeqqsqrq6t7\nvL7JlLNmVyMb9zSzteEwx/rF2xgwrDTOqS2dflkBFSUFFMU1yhc5FuvWrWP8+PH5rka/lus9NLNV\n7p7z86thnhFsAzKvYFQGZe15Avg/IdanXR/sa+b52gZ2dXHKx6B1hH9qWQHDy+JUlBTo45gi0i+F\nGQQrgbFmNoZ0AMwGvpm5g5mNdfeNwepVwEZ60d7mJC/UNrBhb3On+w4riTN6cCGjBxcxalABxZre\nEZEBIrQgcPeEmc0FlpP++OjD7v6umd0NVLv7UmCumf0VcBjYDXQ4LdSDdeONHYd4efvBdm+bPKgw\nFnT8hZwxuJDBhT3/83AiIn1BqN8jcPdlwLKsstszlm8J899vz+s7DvGHjw/m3Hb+0BKqTilhaHFc\nn+IRkUiI3DeLt+xv5o85QmBEWQGXjypnRFnXf9VHRGQgiFQQ7GtOsnTL/jYfBS2OG9NGlnPeycU6\nAxCRSIrUFc/lWw8cddvmWaMH85mhJQoBEWkVj8c5//zz+fSnP81XvvIV9uzZ06PHX7x4MXPnpj9J\nf+edd7Jw4cIePf6xikwQ1Dcm2LTvcJuyS04t48wTivJUIxHpq1puMfHOO+9w8skns2jRonxXKVSR\nmRp6a2djm/URZQVMPbW0nb1FpC+4665wztTvuKPrX6SdMmUKa9asaV3/0Y9+xFNPPUVTUxNXX301\nd911FwCPPPIICxcuxMw477zzePTRR/nNb37DggULaG5uZujQoTz22GMMHz68x9tzvCIRBMmUs3ZX\nU5uyqgpNB4lIx5LJJC+88AI33HADAM899xwbN27kjTfewN2ZOXMmL730EkOHDmXBggX8+c9/Ztiw\nYezatQuASy65hNdeew0z46GHHuKee+7h3nvvzWeTcopEEOxsTNKU8StfpXFj3Enh3bdDRHrGsYzc\ne1LLbai3bdvG+PHjmT59OpAOgueee45JkyYBcODAATZu3Mjbb7/NNddcw7Bh6TumnnzyyQDU1tZy\n7bXXsn37dpqbmxkzZkxe2tOZSFwjyL5b6Gnluh2EiLSv5RrBhx9+iLu3XiNwd2677TZWr17N6tWr\nqampaT1byGXevHnMnTuXtWvX8tOf/pTGxsZ2982nSATBzsa29+seWhKJEyEROU5lZWXcf//93Hvv\nvSQSCa644goefvhhDhw4AMC2bdvYsWMH06ZN41e/+hX19fUArVNDe/fuZeTIkQAsWbIkP43ogkj0\niPWNbc8IhpbodhEi0jWTJk3ivPPO4/HHH+db3/oW69atY8qUKQAMGjSIX/ziF5x77rn84Ac/4Atf\n+ALxeJxJkyaxePFi7rzzTq655hqGDBnCtGnT+OCDD/LcmtxCuw11WLpzG+qH1u1mZ0YYfOvsExlZ\nrm8Qi/RFug318etLt6HuM04tKyBm6TODpMPQYp0RiIi0iEQQfPmMwQCk3NnXnKKkIBKXRkREuiRS\nPWLMjJN0NiDS5/W3Keu+pDvvXaSCQET6vpKSEurr6xUG3eDu1NfXU1JSckyvi8TUkIj0H5WVldTW\n1lJXV5fvqvRLJSUlVFZWHtNrFAQi0qcUFhb22W/gDlSaGhIRiTgFgYhIxCkIREQirt99s9jM6oAP\nu/nyYcDOHqxOXxaltkK02hultoLa21POcPeKXBv6XRAcDzOrbu8r1gNNlNoK0WpvlNoKam9v0NSQ\niEjEKQhERCIuakHwYL4r0Iui1FaIVnuj1FZQe0MXqWsEIiJytKidEYiISBYFgYhIxEUiCMxshpmt\nN7MaM5uf7/p0l5k9bGY7zOydjLKTzex5M9sYPA8Jys3M7g/avMbMLsh4zfXB/hvN7Pp8tKUzZjbK\nzF40s/fM7F0zuyUoH6jtLTGzN8zs7aC9dwXlY8zs9aBdT5pZUVBeHKzXBNtHZxzrtqB8vZldkZ8W\ndc7M4mb2lpn9NlgfyG3dYmZrzWy1mVUHZX3nb9ndB/QDiAObgDOBIuBtYEK+69XNtnweuAB4J6Ps\nHmB+sDwf+GGw/CXgd4ABk4HXg/KTgc3B85BgeUi+25ajrSOAC4LlwcAGYMIAbq8Bg4LlQuD1oB1P\nAbOD8geAm4Ll7wEPBMuzgSeD5QnB33gxMCb424/nu33ttPlW4JfAb4P1gdzWLcCwrLI+87cchTOC\ni4Aad9/s7s3AE8CsPNepW9z9JWBXVvEsYEmwvAT4akb5I572GnCSmY0ArgCed/dd7r4beB6YEX7t\nj427b3f3N4Pl/cA6YCQDt73u7geC1cLg4cA04OmgPLu9Le/D08BlZmZB+RPu3uTuHwA1pP8f6FPM\nrBK4CngoWDcGaFs70Gf+lqMQBCOBrRnrtUHZQDHc3bcHy38BhgfL7bW7370fwVTAJNKj5AHb3mCq\nZDWwg/T/5JuAPe6eCHbJrHtru4Lte4Gh9J/2/hj4L0AqWB/KwG0rpEP9OTNbZWY3BmV95m9Zv0cw\ngLi7m9mA+jywmQ0CngH+1t33pQeCaQOtve6eBM43s5OAfwXOyXOVQmFmXwZ2uPsqM7s03/XpJZe4\n+zYzOwV43szez9yY77/lKJwRbANGZaxXBmUDxSfBaSPB846gvL1295v3w8wKSYfAY+7+66B4wLa3\nhbvvAV4EppCeFmgZsGXWvbVdwfYTgXr6R3unAjPNbAvpqdppwE8YmG0FwN23Bc87SIf8RfShv+Uo\nBMFKYGzwiYQi0heblua5Tj1pKdDy6YHrgWczyv9j8AmEycDe4DR0OXC5mQ0JPqVweVDWpwRzwP8C\nrHP3+zI2DdT2VgRnAphZKTCd9HWRF4FvBLtlt7flffgG8HtPX1FcCswOPmkzBhgLvNE7regad7/N\n3SvdfTTp/x9/7+7XMQDbCmBm5WY2uGWZ9N/gO/Slv+V8X03vjQfpq/AbSM+5/iDf9TmOdjwObAcO\nk54fvIH0XOkLwEZgBXBysK8Bi4I2rwWqMo7zHdIX1mqAv853u9pp6yWk51XXAKuDx5cGcHvPA94K\n2vsOcHtQfibpzq0G+BVQHJSXBOs1wfYzM471g+B9WA9cme+2ddLuSznyqaEB2dagXW8Hj3db+qC+\n9LesW0yIiERcFKaGRESkAwoCEZGIUxCIiEScgkBEJOIUBCIiEacgEGmHmX3VzNzMzgnWR1vGnV/b\neU2n+4j0NQoCkfbNAV4JnkUGLAWBSA7BPY4uIf2lvdk5tn/bzJ41sz8E94a/I2Nz3Mx+ZunfFXgu\n+KYwZvZdM1tp6d8ceMbMynqnNSIdUxCI5DYL+Dd33wDUm9lnc+xzEfB10t8KvsbMqoLyscAidz8X\n2BPsA/Brd7/Q3T9D+vYRN4TaApEuUhCI5DaH9A3RCJ5zTQ897+717n4I+DXpMwiAD9x9dbC8Chgd\nLH/azF42s7XAdcC5odRc5BjpNtQiWczsZNJ3xJwY3Bo4Tvq+R4uyds2+P0vLelNGWRIoDZYXA191\n97fN7Nuk77Mjknc6IxA52jeAR939DHcf7e6jgA9oewtggOnB786Wkv51qT91ctzBwPbg9trX9Xit\nRbpJQSBytDmk7xmf6RngtqyyN4LyNcAz7l7dyXH/G+lfWfsT8H4n+4r0Gt19VKQbgqmdKnefm++6\niBwvnRGIiESczghERCJOZ8VNNOUAAAAgSURBVAQiIhGnIBARiTgFgYhIxCkIREQiTkEgIhJx/x/h\nJjuXW6OZeAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHuzHqTqrbOY",
        "colab_type": "code",
        "outputId": "087cdfb4-2d4f-4dc6-a26a-59f77a63b9dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "x_res, y_res = rus.fit_resample(x_train, y_train)\n",
        "clf = Lasso(alpha=0.4, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='random')\n",
        "clf.fit(x_res, y_res)\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "currPrec = precision_score(y_test, y_pred.round(), average='binary')\n",
        "currRec = recall_score(y_test, y_pred.round())\n",
        "\n",
        "print(currPrec)\n",
        "print(currRec)\n",
        "print(confusion_matrix(y_test, y_pred.round()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2047703180212014\n",
            "0.6935966487133453\n",
            "[[12171  4501]\n",
            " [  512  1159]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmL5rZMXp707",
        "colab_type": "code",
        "outputId": "f257c6a7-ab80-4a8f-d77f-506a40c580b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#model\n",
        "MOD = RandomForestClassifier() \n",
        "#Implemente RandomSearchCV\n",
        "m_params = { \n",
        "            \"RF\": {\n",
        "                    \"n_estimators\" : np.linspace(2, 500, 500, dtype = \"int\"),  \n",
        "                    \"max_depth\": [5, 20, 30, None], \n",
        "                    \"min_samples_split\": np.linspace(2, 50, 50, dtype = \"int\"),  \n",
        "                    \"max_features\": [\"sqrt\", \"log2\",10, 20, None],\n",
        "                    \"oob_score\": [True],\n",
        "                    \"bootstrap\": [True]\n",
        "                    },\n",
        "            }\n",
        "scoreFunction = {\"recall\": \"recall\", \"precision\": \"precision\"}\n",
        "random_search = RandomizedSearchCV(MOD,\n",
        "                                    param_distributions = m_params[\"RF\"], \n",
        "                                    n_iter = 20,\n",
        "                                    scoring = scoreFunction,               \n",
        "                                    refit = \"recall\",\n",
        "                                    return_train_score = True,\n",
        "                                    random_state = 42,\n",
        "                                    cv = 5,\n",
        "                                    verbose = 1) \n",
        "\n",
        "#trains and optimizes the model\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "#recover the best model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTPL-IlFDnnD",
        "colab_type": "code",
        "outputId": "2ee902a7-d139-48ff-d48f-fe22603c8ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "x_res, y_res = rus.fit_resample(x_train, y_train)\n",
        "clf = MLPClassifier(solver='sgd', activation=\"tanh\", alpha=0.1, hidden_layer_sizes=(300,42,18,8,4,2), \n",
        "                    random_state=1, verbose=1, max_iter=1000, learning_rate=\"adaptive\")\n",
        "clf.fit(normalized_x, y_train)\n",
        "\n",
        "y_pred = clf.predict(normalized_x_test)\n",
        "\n",
        "currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "print(currPrec)\n",
        "print(currRec)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.48466697\n",
            "Iteration 2, loss = 0.34536264\n",
            "Iteration 3, loss = 0.33782932\n",
            "Iteration 4, loss = 0.33675047\n",
            "Iteration 5, loss = 0.33644604\n",
            "Iteration 6, loss = 0.33625590\n",
            "Iteration 7, loss = 0.33608848\n",
            "Iteration 8, loss = 0.33592234\n",
            "Iteration 9, loss = 0.33575862\n",
            "Iteration 10, loss = 0.33559446\n",
            "Iteration 11, loss = 0.33543261\n",
            "Iteration 12, loss = 0.33526908\n",
            "Iteration 13, loss = 0.33510594\n",
            "Iteration 14, loss = 0.33494366\n",
            "Iteration 15, loss = 0.33478496\n",
            "Iteration 16, loss = 0.33462493\n",
            "Iteration 17, loss = 0.33446269\n",
            "Iteration 18, loss = 0.33430584\n",
            "Iteration 19, loss = 0.33414563\n",
            "Iteration 20, loss = 0.33398526\n",
            "Iteration 21, loss = 0.33382787\n",
            "Iteration 22, loss = 0.33366995\n",
            "Iteration 23, loss = 0.33351195\n",
            "Iteration 24, loss = 0.33335627\n",
            "Iteration 25, loss = 0.33320021\n",
            "Iteration 26, loss = 0.33304187\n",
            "Iteration 27, loss = 0.33288731\n",
            "Iteration 28, loss = 0.33273134\n",
            "Iteration 29, loss = 0.33257393\n",
            "Iteration 30, loss = 0.33241817\n",
            "Iteration 31, loss = 0.33226371\n",
            "Iteration 32, loss = 0.33210926\n",
            "Iteration 33, loss = 0.33195205\n",
            "Iteration 34, loss = 0.33179820\n",
            "Iteration 35, loss = 0.33164250\n",
            "Iteration 36, loss = 0.33148674\n",
            "Iteration 37, loss = 0.33133175\n",
            "Iteration 38, loss = 0.33117506\n",
            "Iteration 39, loss = 0.33101842\n",
            "Iteration 40, loss = 0.33086178\n",
            "Iteration 41, loss = 0.33070587\n",
            "Iteration 42, loss = 0.33054359\n",
            "Iteration 43, loss = 0.33038414\n",
            "Iteration 44, loss = 0.33022351\n",
            "Iteration 45, loss = 0.33006117\n",
            "Iteration 46, loss = 0.32989302\n",
            "Iteration 47, loss = 0.32972405\n",
            "Iteration 48, loss = 0.32955295\n",
            "Iteration 49, loss = 0.32937557\n",
            "Iteration 50, loss = 0.32919125\n",
            "Iteration 51, loss = 0.32900480\n",
            "Iteration 52, loss = 0.32880868\n",
            "Iteration 53, loss = 0.32860024\n",
            "Iteration 54, loss = 0.32838054\n",
            "Iteration 55, loss = 0.32814499\n",
            "Iteration 56, loss = 0.32789247\n",
            "Iteration 57, loss = 0.32761122\n",
            "Iteration 58, loss = 0.32729806\n",
            "Iteration 59, loss = 0.32694344\n",
            "Iteration 60, loss = 0.32653090\n",
            "Iteration 61, loss = 0.32605843\n",
            "Iteration 62, loss = 0.32547050\n",
            "Iteration 63, loss = 0.32478015\n",
            "Iteration 64, loss = 0.32388088\n",
            "Iteration 65, loss = 0.32272750\n",
            "Iteration 66, loss = 0.32121349\n",
            "Iteration 67, loss = 0.31912267\n",
            "Iteration 68, loss = 0.31616078\n",
            "Iteration 69, loss = 0.31187874\n",
            "Iteration 70, loss = 0.30568503\n",
            "Iteration 71, loss = 0.29728932\n",
            "Iteration 72, loss = 0.28737227\n",
            "Iteration 73, loss = 0.27747913\n",
            "Iteration 74, loss = 0.26983117\n",
            "Iteration 75, loss = 0.26419882\n",
            "Iteration 76, loss = 0.25956815\n",
            "Iteration 77, loss = 0.25536407\n",
            "Iteration 78, loss = 0.25115322\n",
            "Iteration 79, loss = 0.24763318\n",
            "Iteration 80, loss = 0.24451202\n",
            "Iteration 81, loss = 0.24205838\n",
            "Iteration 82, loss = 0.24025660\n",
            "Iteration 83, loss = 0.23819782\n",
            "Iteration 84, loss = 0.23696870\n",
            "Iteration 85, loss = 0.23550569\n",
            "Iteration 86, loss = 0.23460397\n",
            "Iteration 87, loss = 0.23395277\n",
            "Iteration 88, loss = 0.23283079\n",
            "Iteration 89, loss = 0.23208228\n",
            "Iteration 90, loss = 0.23154094\n",
            "Iteration 91, loss = 0.23063704\n",
            "Iteration 92, loss = 0.23051135\n",
            "Iteration 93, loss = 0.22968872\n",
            "Iteration 94, loss = 0.22931367\n",
            "Iteration 95, loss = 0.22879563\n",
            "Iteration 96, loss = 0.22861937\n",
            "Iteration 97, loss = 0.22832596\n",
            "Iteration 98, loss = 0.22783888\n",
            "Iteration 99, loss = 0.22752280\n",
            "Iteration 100, loss = 0.22736864\n",
            "Iteration 101, loss = 0.22705055\n",
            "Iteration 102, loss = 0.22695736\n",
            "Iteration 103, loss = 0.22666634\n",
            "Iteration 104, loss = 0.22655234\n",
            "Iteration 105, loss = 0.22646233\n",
            "Iteration 106, loss = 0.22591904\n",
            "Iteration 107, loss = 0.22628397\n",
            "Iteration 108, loss = 0.22582635\n",
            "Iteration 109, loss = 0.22557060\n",
            "Iteration 110, loss = 0.22530234\n",
            "Iteration 111, loss = 0.22548694\n",
            "Iteration 112, loss = 0.22526876\n",
            "Iteration 113, loss = 0.22481241\n",
            "Iteration 114, loss = 0.22505526\n",
            "Iteration 115, loss = 0.22456428\n",
            "Iteration 116, loss = 0.22454300\n",
            "Iteration 117, loss = 0.22428336\n",
            "Iteration 118, loss = 0.22460934\n",
            "Iteration 119, loss = 0.22404460\n",
            "Iteration 120, loss = 0.22399033\n",
            "Iteration 121, loss = 0.22391469\n",
            "Iteration 122, loss = 0.22362767\n",
            "Iteration 123, loss = 0.22368467\n",
            "Iteration 124, loss = 0.22374133\n",
            "Iteration 125, loss = 0.22347859\n",
            "Iteration 126, loss = 0.22330969\n",
            "Iteration 127, loss = 0.22326163\n",
            "Iteration 128, loss = 0.22315528\n",
            "Iteration 129, loss = 0.22314674\n",
            "Iteration 130, loss = 0.22296113\n",
            "Iteration 131, loss = 0.22264356\n",
            "Iteration 132, loss = 0.22269293\n",
            "Iteration 133, loss = 0.22235490\n",
            "Iteration 134, loss = 0.22225687\n",
            "Iteration 135, loss = 0.22234564\n",
            "Iteration 136, loss = 0.22244005\n",
            "Iteration 137, loss = 0.22205586\n",
            "Iteration 138, loss = 0.22197565\n",
            "Iteration 139, loss = 0.22190680\n",
            "Iteration 140, loss = 0.22187817\n",
            "Iteration 141, loss = 0.22168385\n",
            "Iteration 142, loss = 0.22147491\n",
            "Iteration 143, loss = 0.22174175\n",
            "Iteration 144, loss = 0.22139639\n",
            "Iteration 145, loss = 0.22151352\n",
            "Iteration 146, loss = 0.22100597\n",
            "Iteration 147, loss = 0.22096551\n",
            "Iteration 148, loss = 0.22114874\n",
            "Iteration 149, loss = 0.22093054\n",
            "Iteration 150, loss = 0.22092423\n",
            "Iteration 151, loss = 0.22097282\n",
            "Iteration 152, loss = 0.22074355\n",
            "Iteration 153, loss = 0.22028977\n",
            "Iteration 154, loss = 0.22039124\n",
            "Iteration 155, loss = 0.22021756\n",
            "Iteration 156, loss = 0.22007825\n",
            "Iteration 157, loss = 0.22030329\n",
            "Iteration 158, loss = 0.21985507\n",
            "Iteration 159, loss = 0.21976531\n",
            "Iteration 160, loss = 0.21968474\n",
            "Iteration 161, loss = 0.21981846\n",
            "Iteration 162, loss = 0.21957194\n",
            "Iteration 163, loss = 0.21943846\n",
            "Iteration 164, loss = 0.21938116\n",
            "Iteration 165, loss = 0.21932488\n",
            "Iteration 166, loss = 0.21923516\n",
            "Iteration 167, loss = 0.21938578\n",
            "Iteration 168, loss = 0.21927216\n",
            "Iteration 169, loss = 0.21903458\n",
            "Iteration 170, loss = 0.21919918\n",
            "Iteration 171, loss = 0.21874628\n",
            "Iteration 172, loss = 0.21883422\n",
            "Iteration 173, loss = 0.21852181\n",
            "Iteration 174, loss = 0.21841906\n",
            "Iteration 175, loss = 0.21835653\n",
            "Iteration 176, loss = 0.21845642\n",
            "Iteration 177, loss = 0.21845101\n",
            "Iteration 178, loss = 0.21800156\n",
            "Iteration 179, loss = 0.21830164\n",
            "Iteration 180, loss = 0.21790592\n",
            "Iteration 181, loss = 0.21799344\n",
            "Iteration 182, loss = 0.21792480\n",
            "Iteration 183, loss = 0.21793303\n",
            "Iteration 184, loss = 0.21781137\n",
            "Iteration 185, loss = 0.21773676\n",
            "Iteration 186, loss = 0.21755967\n",
            "Iteration 187, loss = 0.21756114\n",
            "Iteration 188, loss = 0.21721097\n",
            "Iteration 189, loss = 0.21721229\n",
            "Iteration 190, loss = 0.21713749\n",
            "Iteration 191, loss = 0.21716058\n",
            "Iteration 192, loss = 0.21745068\n",
            "Iteration 193, loss = 0.21704957\n",
            "Iteration 194, loss = 0.21696248\n",
            "Iteration 195, loss = 0.21690778\n",
            "Iteration 196, loss = 0.21678754\n",
            "Iteration 197, loss = 0.21654927\n",
            "Iteration 198, loss = 0.21670583\n",
            "Iteration 199, loss = 0.21638950\n",
            "Iteration 200, loss = 0.21666687\n",
            "Iteration 201, loss = 0.21611978\n",
            "Iteration 202, loss = 0.21642134\n",
            "Iteration 203, loss = 0.21613997\n",
            "Iteration 204, loss = 0.21601829\n",
            "Iteration 205, loss = 0.21605391\n",
            "Iteration 206, loss = 0.21598972\n",
            "Iteration 207, loss = 0.21579640\n",
            "Iteration 208, loss = 0.21573321\n",
            "Iteration 209, loss = 0.21556224\n",
            "Iteration 210, loss = 0.21573844\n",
            "Iteration 211, loss = 0.21548392\n",
            "Iteration 212, loss = 0.21541369\n",
            "Iteration 213, loss = 0.21524118\n",
            "Iteration 214, loss = 0.21535676\n",
            "Iteration 215, loss = 0.21523483\n",
            "Iteration 216, loss = 0.21519175\n",
            "Iteration 217, loss = 0.21503753\n",
            "Iteration 218, loss = 0.21498315\n",
            "Iteration 219, loss = 0.21473685\n",
            "Iteration 220, loss = 0.21486928\n",
            "Iteration 221, loss = 0.21481834\n",
            "Iteration 222, loss = 0.21495866\n",
            "Iteration 223, loss = 0.21457538\n",
            "Iteration 224, loss = 0.21471247\n",
            "Iteration 225, loss = 0.21450224\n",
            "Iteration 226, loss = 0.21453196\n",
            "Iteration 227, loss = 0.21419699\n",
            "Iteration 228, loss = 0.21434995\n",
            "Iteration 229, loss = 0.21427038\n",
            "Iteration 230, loss = 0.21427620\n",
            "Iteration 231, loss = 0.21437434\n",
            "Iteration 232, loss = 0.21400672\n",
            "Iteration 233, loss = 0.21399895\n",
            "Iteration 234, loss = 0.21363648\n",
            "Iteration 235, loss = 0.21385032\n",
            "Iteration 236, loss = 0.21349399\n",
            "Iteration 237, loss = 0.21364057\n",
            "Iteration 238, loss = 0.21349461\n",
            "Iteration 239, loss = 0.21358458\n",
            "Iteration 240, loss = 0.21335581\n",
            "Iteration 241, loss = 0.21329845\n",
            "Iteration 242, loss = 0.21336872\n",
            "Iteration 243, loss = 0.21318927\n",
            "Iteration 244, loss = 0.21307465\n",
            "Iteration 245, loss = 0.21303054\n",
            "Iteration 246, loss = 0.21284060\n",
            "Iteration 247, loss = 0.21265939\n",
            "Iteration 248, loss = 0.21299844\n",
            "Iteration 249, loss = 0.21298462\n",
            "Iteration 250, loss = 0.21286737\n",
            "Iteration 251, loss = 0.21247388\n",
            "Iteration 252, loss = 0.21256977\n",
            "Iteration 253, loss = 0.21241407\n",
            "Iteration 254, loss = 0.21244013\n",
            "Iteration 255, loss = 0.21236453\n",
            "Iteration 256, loss = 0.21240212\n",
            "Iteration 257, loss = 0.21221534\n",
            "Iteration 258, loss = 0.21215893\n",
            "Iteration 259, loss = 0.21217529\n",
            "Iteration 260, loss = 0.21230095\n",
            "Iteration 261, loss = 0.21207324\n",
            "Iteration 262, loss = 0.21183588\n",
            "Iteration 263, loss = 0.21191163\n",
            "Iteration 264, loss = 0.21182268\n",
            "Iteration 265, loss = 0.21201106\n",
            "Iteration 266, loss = 0.21183289\n",
            "Iteration 267, loss = 0.21176081\n",
            "Iteration 268, loss = 0.21155763\n",
            "Iteration 269, loss = 0.21180493\n",
            "Iteration 270, loss = 0.21150929\n",
            "Iteration 271, loss = 0.21127381\n",
            "Iteration 272, loss = 0.21103005\n",
            "Iteration 273, loss = 0.21127572\n",
            "Iteration 274, loss = 0.21145748\n",
            "Iteration 275, loss = 0.21133078\n",
            "Iteration 276, loss = 0.21118716\n",
            "Iteration 277, loss = 0.21140438\n",
            "Iteration 278, loss = 0.21112675\n",
            "Iteration 279, loss = 0.21080142\n",
            "Iteration 280, loss = 0.21104305\n",
            "Iteration 281, loss = 0.21086661\n",
            "Iteration 282, loss = 0.21071052\n",
            "Iteration 283, loss = 0.21060295\n",
            "Iteration 284, loss = 0.21083333\n",
            "Iteration 285, loss = 0.21046676\n",
            "Iteration 286, loss = 0.21055997\n",
            "Iteration 287, loss = 0.21047600\n",
            "Iteration 288, loss = 0.21036969\n",
            "Iteration 289, loss = 0.21035909\n",
            "Iteration 290, loss = 0.21018877\n",
            "Iteration 291, loss = 0.21017937\n",
            "Iteration 292, loss = 0.21007335\n",
            "Iteration 293, loss = 0.21017532\n",
            "Iteration 294, loss = 0.21024474\n",
            "Iteration 295, loss = 0.21007374\n",
            "Iteration 296, loss = 0.20974881\n",
            "Iteration 297, loss = 0.20981483\n",
            "Iteration 298, loss = 0.20987700\n",
            "Iteration 299, loss = 0.20964809\n",
            "Iteration 300, loss = 0.20995732\n",
            "Iteration 301, loss = 0.20972714\n",
            "Iteration 302, loss = 0.20978604\n",
            "Iteration 303, loss = 0.20944856\n",
            "Iteration 304, loss = 0.20955472\n",
            "Iteration 305, loss = 0.20948453\n",
            "Iteration 306, loss = 0.20968848\n",
            "Iteration 307, loss = 0.20918631\n",
            "Iteration 308, loss = 0.20947436\n",
            "Iteration 309, loss = 0.20934923\n",
            "Iteration 310, loss = 0.20915081\n",
            "Iteration 311, loss = 0.20937177\n",
            "Iteration 312, loss = 0.20900458\n",
            "Iteration 313, loss = 0.20918634\n",
            "Iteration 314, loss = 0.20881750\n",
            "Iteration 315, loss = 0.20933555\n",
            "Iteration 316, loss = 0.20906084\n",
            "Iteration 317, loss = 0.20893166\n",
            "Iteration 318, loss = 0.20888989\n",
            "Iteration 319, loss = 0.20877466\n",
            "Iteration 320, loss = 0.20884985\n",
            "Iteration 321, loss = 0.20877692\n",
            "Iteration 322, loss = 0.20853730\n",
            "Iteration 323, loss = 0.20846637\n",
            "Iteration 324, loss = 0.20846001\n",
            "Iteration 325, loss = 0.20830797\n",
            "Iteration 326, loss = 0.20855721\n",
            "Iteration 327, loss = 0.20818852\n",
            "Iteration 328, loss = 0.20846445\n",
            "Iteration 329, loss = 0.20836129\n",
            "Iteration 330, loss = 0.20827722\n",
            "Iteration 331, loss = 0.20830321\n",
            "Iteration 332, loss = 0.20813138\n",
            "Iteration 333, loss = 0.20785708\n",
            "Iteration 334, loss = 0.20795527\n",
            "Iteration 335, loss = 0.20800128\n",
            "Iteration 336, loss = 0.20780799\n",
            "Iteration 337, loss = 0.20811966\n",
            "Iteration 338, loss = 0.20783200\n",
            "Iteration 339, loss = 0.20789467\n",
            "Iteration 340, loss = 0.20786948\n",
            "Iteration 341, loss = 0.20755355\n",
            "Iteration 342, loss = 0.20762100\n",
            "Iteration 343, loss = 0.20754901\n",
            "Iteration 344, loss = 0.20740210\n",
            "Iteration 345, loss = 0.20734523\n",
            "Iteration 346, loss = 0.20740931\n",
            "Iteration 347, loss = 0.20743104\n",
            "Iteration 348, loss = 0.20759032\n",
            "Iteration 349, loss = 0.20709567\n",
            "Iteration 350, loss = 0.20741250\n",
            "Iteration 351, loss = 0.20727292\n",
            "Iteration 352, loss = 0.20717756\n",
            "Iteration 353, loss = 0.20729886\n",
            "Iteration 354, loss = 0.20738792\n",
            "Iteration 355, loss = 0.20725162\n",
            "Iteration 356, loss = 0.20713892\n",
            "Iteration 357, loss = 0.20707207\n",
            "Iteration 358, loss = 0.20686102\n",
            "Iteration 359, loss = 0.20702290\n",
            "Iteration 360, loss = 0.20694138\n",
            "Iteration 361, loss = 0.20660641\n",
            "Iteration 362, loss = 0.20662989\n",
            "Iteration 363, loss = 0.20680842\n",
            "Iteration 364, loss = 0.20668367\n",
            "Iteration 365, loss = 0.20654214\n",
            "Iteration 366, loss = 0.20670558\n",
            "Iteration 367, loss = 0.20639817\n",
            "Iteration 368, loss = 0.20636136\n",
            "Iteration 369, loss = 0.20642515\n",
            "Iteration 370, loss = 0.20643381\n",
            "Iteration 371, loss = 0.20639708\n",
            "Iteration 372, loss = 0.20658894\n",
            "Iteration 373, loss = 0.20651956\n",
            "Iteration 374, loss = 0.20624665\n",
            "Iteration 375, loss = 0.20625883\n",
            "Iteration 376, loss = 0.20618489\n",
            "Iteration 377, loss = 0.20617367\n",
            "Iteration 378, loss = 0.20602301\n",
            "Iteration 379, loss = 0.20604428\n",
            "Iteration 380, loss = 0.20611868\n",
            "Iteration 381, loss = 0.20611490\n",
            "Iteration 382, loss = 0.20581196\n",
            "Iteration 383, loss = 0.20585516\n",
            "Iteration 384, loss = 0.20609615\n",
            "Iteration 385, loss = 0.20566134\n",
            "Iteration 386, loss = 0.20580803\n",
            "Iteration 387, loss = 0.20561545\n",
            "Iteration 388, loss = 0.20572881\n",
            "Iteration 389, loss = 0.20565290\n",
            "Iteration 390, loss = 0.20548339\n",
            "Iteration 391, loss = 0.20551070\n",
            "Iteration 392, loss = 0.20552273\n",
            "Iteration 393, loss = 0.20575775\n",
            "Iteration 394, loss = 0.20547064\n",
            "Iteration 395, loss = 0.20569578\n",
            "Iteration 396, loss = 0.20532556\n",
            "Iteration 397, loss = 0.20535562\n",
            "Iteration 398, loss = 0.20531316\n",
            "Iteration 399, loss = 0.20528840\n",
            "Iteration 400, loss = 0.20528923\n",
            "Iteration 401, loss = 0.20527843\n",
            "Iteration 402, loss = 0.20499721\n",
            "Iteration 403, loss = 0.20499760\n",
            "Iteration 404, loss = 0.20488785\n",
            "Iteration 405, loss = 0.20506378\n",
            "Iteration 406, loss = 0.20506189\n",
            "Iteration 407, loss = 0.20506851\n",
            "Iteration 408, loss = 0.20491013\n",
            "Iteration 409, loss = 0.20474048\n",
            "Iteration 410, loss = 0.20487692\n",
            "Iteration 411, loss = 0.20475509\n",
            "Iteration 412, loss = 0.20482756\n",
            "Iteration 413, loss = 0.20478918\n",
            "Iteration 414, loss = 0.20484399\n",
            "Iteration 415, loss = 0.20455336\n",
            "Iteration 416, loss = 0.20467893\n",
            "Iteration 417, loss = 0.20479174\n",
            "Iteration 418, loss = 0.20450519\n",
            "Iteration 419, loss = 0.20440158\n",
            "Iteration 420, loss = 0.20448169\n",
            "Iteration 421, loss = 0.20437568\n",
            "Iteration 422, loss = 0.20442902\n",
            "Iteration 423, loss = 0.20438190\n",
            "Iteration 424, loss = 0.20419814\n",
            "Iteration 425, loss = 0.20436425\n",
            "Iteration 426, loss = 0.20415239\n",
            "Iteration 427, loss = 0.20421325\n",
            "Iteration 428, loss = 0.20419931\n",
            "Iteration 429, loss = 0.20404147\n",
            "Iteration 430, loss = 0.20410440\n",
            "Iteration 431, loss = 0.20405566\n",
            "Iteration 432, loss = 0.20409188\n",
            "Iteration 433, loss = 0.20415996\n",
            "Iteration 434, loss = 0.20406868\n",
            "Iteration 435, loss = 0.20398053\n",
            "Iteration 436, loss = 0.20400223\n",
            "Iteration 437, loss = 0.20410901\n",
            "Iteration 438, loss = 0.20370199\n",
            "Iteration 439, loss = 0.20379671\n",
            "Iteration 440, loss = 0.20383599\n",
            "Iteration 441, loss = 0.20390939\n",
            "Iteration 442, loss = 0.20351870\n",
            "Iteration 443, loss = 0.20370620\n",
            "Iteration 444, loss = 0.20347226\n",
            "Iteration 445, loss = 0.20348574\n",
            "Iteration 446, loss = 0.20358480\n",
            "Iteration 447, loss = 0.20360859\n",
            "Iteration 448, loss = 0.20355906\n",
            "Iteration 449, loss = 0.20338706\n",
            "Iteration 450, loss = 0.20377764\n",
            "Iteration 451, loss = 0.20329440\n",
            "Iteration 452, loss = 0.20340289\n",
            "Iteration 453, loss = 0.20347510\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 454, loss = 0.20265116\n",
            "Iteration 455, loss = 0.20287340\n",
            "Iteration 456, loss = 0.20270878\n",
            "Iteration 457, loss = 0.20278780\n",
            "Iteration 458, loss = 0.20268749\n",
            "Iteration 459, loss = 0.20264411\n",
            "Iteration 460, loss = 0.20270803\n",
            "Iteration 461, loss = 0.20263249\n",
            "Iteration 462, loss = 0.20271129\n",
            "Iteration 463, loss = 0.20262032\n",
            "Iteration 464, loss = 0.20264390\n",
            "Iteration 465, loss = 0.20266475\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 466, loss = 0.20249450\n",
            "Iteration 467, loss = 0.20253061\n",
            "Iteration 468, loss = 0.20249141\n",
            "Iteration 469, loss = 0.20248959\n",
            "Iteration 470, loss = 0.20247135\n",
            "Iteration 471, loss = 0.20248617\n",
            "Iteration 472, loss = 0.20251014\n",
            "Iteration 473, loss = 0.20249617\n",
            "Iteration 474, loss = 0.20242445\n",
            "Iteration 475, loss = 0.20247635\n",
            "Iteration 476, loss = 0.20247966\n",
            "Iteration 477, loss = 0.20249677\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
            "Iteration 478, loss = 0.20244549\n",
            "Iteration 479, loss = 0.20242701\n",
            "Iteration 480, loss = 0.20243571\n",
            "Iteration 481, loss = 0.20243648\n",
            "Iteration 482, loss = 0.20242455\n",
            "Iteration 483, loss = 0.20243320\n",
            "Iteration 484, loss = 0.20242976\n",
            "Iteration 485, loss = 0.20242996\n",
            "Iteration 486, loss = 0.20242766\n",
            "Iteration 487, loss = 0.20243000\n",
            "Iteration 488, loss = 0.20242551\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
            "Iteration 489, loss = 0.20242633\n",
            "Iteration 490, loss = 0.20242044\n",
            "Iteration 491, loss = 0.20241847\n",
            "Iteration 492, loss = 0.20241790\n",
            "Iteration 493, loss = 0.20241830\n",
            "Iteration 494, loss = 0.20241697\n",
            "Iteration 495, loss = 0.20241818\n",
            "Iteration 496, loss = 0.20241821\n",
            "Iteration 497, loss = 0.20241764\n",
            "Iteration 498, loss = 0.20241668\n",
            "Iteration 499, loss = 0.20241714\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
            "Iteration 500, loss = 0.20241427\n",
            "Iteration 501, loss = 0.20241444\n",
            "Iteration 502, loss = 0.20241414\n",
            "Iteration 503, loss = 0.20241407\n",
            "Iteration 504, loss = 0.20241433\n",
            "Iteration 505, loss = 0.20241409\n",
            "Iteration 506, loss = 0.20241432\n",
            "Iteration 507, loss = 0.20241403\n",
            "Iteration 508, loss = 0.20241415\n",
            "Iteration 509, loss = 0.20241420\n",
            "Iteration 510, loss = 0.20241418\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
            "0.6473577235772358\n",
            "0.3812088569718731\n",
            "[[16325   347]\n",
            " [ 1034   637]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1B6KuyRQEvx",
        "colab_type": "code",
        "outputId": "48d5914f-6be5-4d71-d2b6-ca9ce51a388e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Precision: 34\n",
        "#recall: 78\n",
        "#lasso = Lasso(alpha=0.4, fit_intercept=True, normalize=False, precompute=False, \n",
        "           # copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, \n",
        "           # positive=False, random_state=None, selection='random')\n",
        "#Precision: 34\n",
        "#recall: 78\n",
        "lr = LogisticRegression(penalty='elasticnet', solver='saga' , l1_ratio=0.5, \n",
        "                         random_state=0, tol=0.0001, C=8, fit_intercept=True, \n",
        "                         class_weight='balanced', max_iter=5012, verbose=1, n_jobs=-1)\n",
        "#Precision: 91\n",
        "#recall: 77\n",
        "gbclf = GradientBoostingClassifier(loss='deviance', learning_rate=0.032,  \n",
        "                                   n_estimators=300, subsample=0.4, max_depth=100, \n",
        "                                   min_impurity_decrease=0.1 , verbose=1, \n",
        "                                   max_features='sqrt', max_leaf_nodes=None, ccp_alpha=0)\n",
        "#Precision: 98\n",
        "#recall: 63\n",
        "gbclfP = GradientBoostingClassifier(loss='deviance', learning_rate=0.01, n_estimators=250, \n",
        "                                   subsample=0.7, max_depth=20, min_impurity_decrease=0. , \n",
        "                                   verbose=1, max_features='sqrt', max_leaf_nodes=None)\n",
        "#Precision: 33\n",
        "#recall: 80\n",
        "sv = SVC(C=2, kernel=\"poly\", degree=3, coef0=0.6)\n",
        "\n",
        "estimators = [\n",
        "              #(\"lasso\", lasso),\n",
        "              (\"lr\", lr),\n",
        "              (\"gbclf\", gbclf),\n",
        "              (\"gbclfP\", gbclfP),\n",
        "              (\"sv\", sv)\n",
        "]\n",
        "\n",
        "\n",
        "clf = StackingClassifier(\n",
        "  estimators=estimators, final_estimator=GradientBoostingClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "currPrec = precision_score(y_test, y_pred, average='binary')\n",
        "currRec = recall_score(y_test, y_pred)\n",
        "\n",
        "print(currPrec)\n",
        "print(currRec)\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  4.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5265           0.0362           50.68s\n",
            "         2           0.4984           0.0288           45.20s\n",
            "         3           0.4614           0.0243           44.29s\n",
            "         4           0.4420           0.0191           44.29s\n",
            "         5           0.4266           0.0159           44.03s\n",
            "         6           0.4044           0.0150           43.39s\n",
            "         7           0.3845           0.0126           43.07s\n",
            "         8           0.3727           0.0096           43.43s\n",
            "         9           0.3534           0.0116           42.72s\n",
            "        10           0.3419           0.0102           42.75s\n",
            "        20           0.2550           0.0052           40.06s\n",
            "        30           0.1996           0.0028           37.61s\n",
            "        40           0.1601           0.0022           35.08s\n",
            "        50           0.1354           0.0013           32.77s\n",
            "        60           0.1128           0.0018           30.09s\n",
            "        70           0.0972           0.0011           27.60s\n",
            "        80           0.0930           0.0006           25.15s\n",
            "        90           0.0820           0.0003           22.94s\n",
            "       100           0.0796           0.0003           20.79s\n",
            "       200           0.0596           0.0000            7.13s\n",
            "       300           0.0564           0.0000            0.00s\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5590           0.0110            1.23m\n",
            "         2           0.5453           0.0100            1.79m\n",
            "         3           0.5375           0.0087            2.06m\n",
            "         4           0.5215           0.0087            2.31m\n",
            "         5           0.5064           0.0086            2.49m\n",
            "         6           0.4940           0.0085            2.60m\n",
            "         7           0.4876           0.0083            2.64m\n",
            "         8           0.4796           0.0065            2.71m\n",
            "         9           0.4674           0.0061            2.77m\n",
            "        10           0.4574           0.0073            2.82m\n",
            "        20           0.3871           0.0040            2.70m\n",
            "        30           0.3338           0.0032            2.51m\n",
            "        40           0.2941           0.0024            2.39m\n",
            "        50           0.2576           0.0020            2.26m\n",
            "        60           0.2305           0.0015            2.15m\n",
            "        70           0.2069           0.0015            2.05m\n",
            "        80           0.1844           0.0010            1.95m\n",
            "        90           0.1677           0.0010            1.84m\n",
            "       100           0.1510           0.0009            1.73m\n",
            "       200           0.0615           0.0003           34.78s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.3min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.3min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.3min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.3min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5244           0.0336           44.14s\n",
            "         2           0.4782           0.0315           38.21s\n",
            "         3           0.4588           0.0195           37.90s\n",
            "         4           0.4436           0.0164           37.55s\n",
            "         5           0.4228           0.0165           36.51s\n",
            "         6           0.4083           0.0137           35.68s\n",
            "         7           0.3784           0.0138           34.57s\n",
            "         8           0.3639           0.0121           33.80s\n",
            "         9           0.3607           0.0106           33.76s\n",
            "        10           0.3484           0.0091           33.30s\n",
            "        20           0.2517           0.0049           29.43s\n",
            "        30           0.1977           0.0036           27.59s\n",
            "        40           0.1625           0.0024           25.98s\n",
            "        50           0.1343           0.0014           24.24s\n",
            "        60           0.1143           0.0007           22.01s\n",
            "        70           0.1031           0.0007           20.20s\n",
            "        80           0.0949           0.0007           18.48s\n",
            "        90           0.0844           0.0002           16.89s\n",
            "       100           0.0790           0.0002           15.34s\n",
            "       200           0.0608           0.0000            5.41s\n",
            "       300           0.0585           0.0000            0.00s\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5268           0.0392           28.78s\n",
            "         2           0.4804           0.0246           32.34s\n",
            "         3           0.4631           0.0221           30.97s\n",
            "         4           0.4348           0.0189           30.55s\n",
            "         5           0.4199           0.0155           30.92s\n",
            "         6           0.4111           0.0135           30.87s\n",
            "         7           0.3881           0.0110           31.29s\n",
            "         8           0.3750           0.0109           31.86s\n",
            "         9           0.3570           0.0114           31.69s\n",
            "        10           0.3477           0.0085           31.75s\n",
            "        20           0.2533           0.0063           29.37s\n",
            "        30           0.1999           0.0039           27.54s\n",
            "        40           0.1594           0.0025           25.83s\n",
            "        50           0.1375           0.0016           23.90s\n",
            "        60           0.1145           0.0010           22.22s\n",
            "        70           0.1002           0.0009           20.35s\n",
            "        80           0.0956           0.0002           18.65s\n",
            "        90           0.0852           0.0005           17.17s\n",
            "       100           0.0761           0.0004           15.59s\n",
            "       200           0.0621           0.0000            5.32s\n",
            "       300           0.0569          -0.0000            0.00s\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5295           0.0304           41.63s\n",
            "         2           0.4951           0.0241           39.80s\n",
            "         3           0.4672           0.0240           37.18s\n",
            "         4           0.4444           0.0212           35.07s\n",
            "         5           0.4225           0.0180           33.38s\n",
            "         6           0.4009           0.0134           33.44s\n",
            "         7           0.3866           0.0116           33.63s\n",
            "         8           0.3716           0.0130           33.20s\n",
            "         9           0.3565           0.0102           33.16s\n",
            "        10           0.3447           0.0094           33.56s\n",
            "        20           0.2553           0.0046           31.53s\n",
            "        30           0.1963           0.0034           29.36s\n",
            "        40           0.1599           0.0018           27.09s\n",
            "        50           0.1320           0.0016           25.20s\n",
            "        60           0.1139           0.0011           23.10s\n",
            "        70           0.1014           0.0013           21.12s\n",
            "        80           0.0878           0.0002           19.21s\n",
            "        90           0.0835           0.0003           17.41s\n",
            "       100           0.0790           0.0002           15.77s\n",
            "       200           0.0601           0.0001            5.50s\n",
            "       300           0.0567          -0.0000            0.00s\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5314           0.0368           34.91s\n",
            "         2           0.4869           0.0258           34.96s\n",
            "         3           0.4554           0.0187           37.29s\n",
            "         4           0.4444           0.0182           37.42s\n",
            "         5           0.4205           0.0164           36.82s\n",
            "         6           0.4088           0.0141           36.84s\n",
            "         7           0.3845           0.0144           36.35s\n",
            "         8           0.3735           0.0119           36.18s\n",
            "         9           0.3535           0.0111           35.92s\n",
            "        10           0.3412           0.0093           36.36s\n",
            "        20           0.2576           0.0057           34.88s\n",
            "        30           0.1992           0.0021           31.28s\n",
            "        40           0.1580           0.0014           28.26s\n",
            "        50           0.1324           0.0014           26.02s\n",
            "        60           0.1159           0.0011           24.02s\n",
            "        70           0.1033           0.0010           21.93s\n",
            "        80           0.0941           0.0005           19.97s\n",
            "        90           0.0856           0.0002           18.13s\n",
            "       100           0.0790           0.0002           16.51s\n",
            "       200           0.0605           0.0002            5.64s\n",
            "       300           0.0598           0.0000            0.00s\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5131           0.0378           33.27s\n",
            "         2           0.4879           0.0316           31.76s\n",
            "         3           0.4615           0.0204           31.18s\n",
            "         4           0.4423           0.0191           32.09s\n",
            "         5           0.4212           0.0180           32.15s\n",
            "         6           0.4022           0.0128           32.80s\n",
            "         7           0.3889           0.0129           32.26s\n",
            "         8           0.3732           0.0097           32.13s\n",
            "         9           0.3524           0.0114           31.81s\n",
            "        10           0.3391           0.0091           31.83s\n",
            "        20           0.2550           0.0051           29.78s\n",
            "        30           0.1986           0.0037           27.57s\n",
            "        40           0.1637           0.0016           25.71s\n",
            "        50           0.1332           0.0014           23.98s\n",
            "        60           0.1108           0.0015           22.06s\n",
            "        70           0.0957           0.0012           20.42s\n",
            "        80           0.0882           0.0006           18.66s\n",
            "        90           0.0811           0.0005           16.95s\n",
            "       100           0.0753           0.0007           15.45s\n",
            "       200           0.0633           0.0000            5.31s\n",
            "       300           0.0593          -0.0000            0.00s\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5643           0.0112            1.02m\n",
            "         2           0.5492           0.0115            1.05m\n",
            "         3           0.5309           0.0106            1.11m\n",
            "         4           0.5230           0.0087            1.20m\n",
            "         5           0.5058           0.0091            1.26m\n",
            "         6           0.4972           0.0075            1.33m\n",
            "         7           0.4891           0.0073            1.38m\n",
            "         8           0.4719           0.0068            1.43m\n",
            "         9           0.4609           0.0076            1.47m\n",
            "        10           0.4530           0.0070            1.51m\n",
            "        20           0.3856           0.0042            1.67m\n",
            "        30           0.3320           0.0031            1.68m\n",
            "        40           0.2942           0.0024            1.66m\n",
            "        50           0.2571           0.0021            1.61m\n",
            "        60           0.2300           0.0016            1.56m\n",
            "        70           0.2050           0.0017            1.48m\n",
            "        80           0.1855           0.0013            1.42m\n",
            "        90           0.1679           0.0013            1.35m\n",
            "       100           0.1528           0.0010            1.27m\n",
            "       200           0.0610           0.0003           25.28s\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5622           0.0109           52.05s\n",
            "         2           0.5466           0.0109            1.04m\n",
            "         3           0.5312           0.0102            1.08m\n",
            "         4           0.5204           0.0096            1.16m\n",
            "         5           0.5090           0.0088            1.25m\n",
            "         6           0.4929           0.0073            1.31m\n",
            "         7           0.4899           0.0073            1.38m\n",
            "         8           0.4793           0.0063            1.41m\n",
            "         9           0.4669           0.0061            1.42m\n",
            "        10           0.4589           0.0055            1.45m\n",
            "        20           0.3844           0.0043            1.56m\n",
            "        30           0.3345           0.0031            1.56m\n",
            "        40           0.2910           0.0023            1.55m\n",
            "        50           0.2583           0.0019            1.50m\n",
            "        60           0.2313           0.0014            1.45m\n",
            "        70           0.2072           0.0016            1.40m\n",
            "        80           0.1879           0.0010            1.34m\n",
            "        90           0.1664           0.0011            1.28m\n",
            "       100           0.1503           0.0010            1.20m\n",
            "       200           0.0604           0.0003           24.73s\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5608           0.0123           45.64s\n",
            "         2           0.5507           0.0109           53.03s\n",
            "         3           0.5396           0.0090            1.06m\n",
            "         4           0.5168           0.0090            1.13m\n",
            "         5           0.5129           0.0077            1.20m\n",
            "         6           0.5017           0.0069            1.27m\n",
            "         7           0.4869           0.0083            1.31m\n",
            "         8           0.4767           0.0066            1.33m\n",
            "         9           0.4671           0.0062            1.38m\n",
            "        10           0.4570           0.0060            1.40m\n",
            "        20           0.3855           0.0039            1.57m\n",
            "        30           0.3337           0.0031            1.58m\n",
            "        40           0.2915           0.0026            1.56m\n",
            "        50           0.2564           0.0018            1.51m\n",
            "        60           0.2295           0.0016            1.46m\n",
            "        70           0.2056           0.0013            1.40m\n",
            "        80           0.1853           0.0010            1.34m\n",
            "        90           0.1666           0.0012            1.27m\n",
            "       100           0.1508           0.0011            1.20m\n",
            "       200           0.0612           0.0003           24.64s\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5622           0.0105           56.10s\n",
            "         2           0.5483           0.0110            1.10m\n",
            "         3           0.5334           0.0086            1.20m\n",
            "         4           0.5221           0.0085            1.31m\n",
            "         5           0.5092           0.0096            1.38m\n",
            "         6           0.4962           0.0077            1.43m\n",
            "         7           0.4852           0.0077            1.44m\n",
            "         8           0.4753           0.0068            1.45m\n",
            "         9           0.4665           0.0066            1.47m\n",
            "        10           0.4605           0.0065            1.49m\n",
            "        20           0.3818           0.0040            1.58m\n",
            "        30           0.3326           0.0033            1.60m\n",
            "        40           0.2909           0.0023            1.58m\n",
            "        50           0.2587           0.0017            1.55m\n",
            "        60           0.2304           0.0018            1.49m\n",
            "        70           0.2077           0.0014            1.43m\n",
            "        80           0.1852           0.0013            1.36m\n",
            "        90           0.1671           0.0011            1.29m\n",
            "       100           0.1509           0.0009            1.21m\n",
            "       200           0.0604           0.0003           25.09s\n",
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           0.5610           0.0119           51.38s\n",
            "         2           0.5515           0.0105           59.32s\n",
            "         3           0.5331           0.0102            1.07m\n",
            "         4           0.5222           0.0088            1.17m\n",
            "         5           0.5047           0.0094            1.22m\n",
            "         6           0.4992           0.0077            1.26m\n",
            "         7           0.4892           0.0073            1.31m\n",
            "         8           0.4786           0.0068            1.35m\n",
            "         9           0.4632           0.0071            1.38m\n",
            "        10           0.4575           0.0064            1.44m\n",
            "        20           0.3840           0.0044            1.59m\n",
            "        30           0.3318           0.0029            1.67m\n",
            "        40           0.2904           0.0025            1.65m\n",
            "        50           0.2582           0.0021            1.60m\n",
            "        60           0.2291           0.0017            1.54m\n",
            "        70           0.2058           0.0014            1.47m\n",
            "        80           0.1843           0.0011            1.40m\n",
            "        90           0.1660           0.0011            1.32m\n",
            "       100           0.1508           0.0009            1.24m\n",
            "       200           0.0605           0.0003           25.18s\n",
            "0.9075931232091691\n",
            "0.758228605625374\n",
            "[[16543   129]\n",
            " [  404  1267]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5ZtepGWG_j0",
        "colab_type": "code",
        "outputId": "c4a75bc5-a7d2-4cd3-be9c-55cc2cf87b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(73370, 42)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnonXktCTrOr",
        "colab_type": "code",
        "outputId": "2e9510ed-7ea2-43ac-be4c-f13bb724fcf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "n_pcs= pca.components_.shape[0]\n",
        "\n",
        "# get the index of the most important feature on EACH component\n",
        "# LIST COMPREHENSION HERE\n",
        "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
        "\n",
        "\n",
        "initial_feature_names = list(x.columns) \n",
        "# get the names\n",
        "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
        "\n",
        "# LIST COMPREHENSION HERE AGAIN\n",
        "dic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\n",
        "\n",
        "# build the dataframe\n",
        "df = pd.DataFrame(dic.items())\n",
        "\n",
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     0                         1\n",
            "0  PC0            glucose_apache\n",
            "1  PC1          d1_platelets_min\n",
            "2  PC2  d1_sysbp_noninvasive_max\n",
            "3  PC3          d1_heartrate_max\n",
            "4  PC4                d1_bun_max\n",
            "5  PC5                d1_bun_max\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL0j7ybvdMMi",
        "colab_type": "code",
        "outputId": "68c3fb94-e06b-4d37-d976-62901553dcbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pca.n_components_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    }
  ]
}